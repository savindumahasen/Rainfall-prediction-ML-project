{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Rainfall-predictions-ML-project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x113a872ede0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import the  pyspark libraries adn create the pyspark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Rainfall-predictions-ML-project').getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "| id|day|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|rainfall|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "|  0|  1|  1017.4|   21.2|       20.6|   19.9|    19.4|    87.0| 88.0|     1.1|         60.0|     17.2|       1|\n",
      "|  1|  2|  1019.5|   16.2|       16.9|   15.8|    15.4|    95.0| 91.0|     0.0|         50.0|     21.9|       1|\n",
      "|  2|  3|  1024.1|   19.4|       16.1|   14.6|     9.3|    75.0| 47.0|     8.3|         70.0|     18.1|       1|\n",
      "|  3|  4|  1013.4|   18.1|       17.8|   16.9|    16.8|    95.0| 95.0|     0.0|         60.0|     35.6|       1|\n",
      "|  4|  5|  1021.8|   21.3|       18.4|   15.2|     9.6|    52.0| 45.0|     3.6|         40.0|     24.8|       0|\n",
      "|  5|  6|  1022.7|   20.6|       18.6|   16.5|    12.5|    79.0| 81.0|     0.0|         20.0|     15.7|       1|\n",
      "|  6|  7|  1022.8|   19.5|       18.4|   15.3|    11.3|    56.0| 46.0|     7.6|         20.0|     28.4|       0|\n",
      "|  7|  8|  1019.7|   15.8|       13.6|   12.7|    11.8|    96.0|100.0|     0.0|         50.0|     52.8|       1|\n",
      "|  8|  9|  1017.4|   17.6|       16.5|   15.6|    12.5|    86.0|100.0|     0.0|         50.0|     37.5|       1|\n",
      "|  9| 10|  1025.4|   16.5|       14.4|   12.0|     8.6|    77.0| 84.0|     1.0|         50.0|     38.3|       0|\n",
      "| 10| 11|  1016.8|   16.3|       15.3|   14.1|    14.0|    97.0|100.0|     0.0|         40.0|     24.4|       1|\n",
      "| 11| 12|  1012.5|   16.2|       15.2|   14.0|    12.4|    98.0|100.0|     0.0|         50.0|     23.5|       1|\n",
      "| 12| 13|  1020.4|   15.0|       15.5|   13.2|    12.0|    77.0| 86.0|     0.0|         50.0|     32.4|       1|\n",
      "| 13| 14|  1012.5|   13.5|       12.9|   11.6|    11.8|    87.0| 92.0|     0.0|         50.0|     37.0|       1|\n",
      "| 14| 15|  1018.4|   17.8|       16.5|   15.1|    12.1|    77.0| 85.0|     1.2|         40.0|     20.9|       1|\n",
      "| 15| 16|  1024.3|   15.3|       12.9|   10.0|    11.2|    79.0| 91.0|     0.3|         20.0|     20.0|       1|\n",
      "| 16| 17|  1022.5|   16.3|       13.1|   11.4|     2.0|    79.0| 70.0|     6.8|         20.0|     30.3|       1|\n",
      "| 17| 18|  1034.6|   17.5|       16.2|   14.1|    11.8|    68.0| 60.0|     2.5|         50.0|     13.4|       0|\n",
      "| 18| 19|  1024.1|   16.8|       15.8|   14.0|    12.9|    76.0| 77.0|     0.0|         20.0|     20.5|       1|\n",
      "| 19| 20|  1020.2|   16.4|       14.2|   12.7|    11.6|    75.0| 93.0|     0.1|         40.0|     21.9|       1|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read the train set\n",
    "df_train_set=spark.read.csv('train.csv',header=True,inferSchema=True)\n",
    "df_train_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- maxtemp: double (nullable = true)\n",
      " |-- temparature: double (nullable = true)\n",
      " |-- mintemp: double (nullable = true)\n",
      " |-- dewpoint: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- cloud: double (nullable = true)\n",
      " |-- sunshine: double (nullable = true)\n",
      " |-- winddirection: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- rainfall: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check the data types of each column in dataset\n",
    "df_train_set.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'day',\n",
       " 'pressure',\n",
       " 'maxtemp',\n",
       " 'temparature',\n",
       " 'mintemp',\n",
       " 'dewpoint',\n",
       " 'humidity',\n",
       " 'cloud',\n",
       " 'sunshine',\n",
       " 'winddirection',\n",
       " 'windspeed',\n",
       " 'rainfall']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the column names\n",
    "df_train_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "| id|day|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|rainfall|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "|  0|  0|       0|      0|          0|      0|       0|       0|    0|       0|            0|        0|       0|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## chech the null values\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "df_train_set.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_train_set.columns]\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|rainfall|\n",
      "+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "|  1017.4|   21.2|       20.6|   19.9|    19.4|    87.0| 88.0|     1.1|         60.0|     17.2|       1|\n",
      "|  1019.5|   16.2|       16.9|   15.8|    15.4|    95.0| 91.0|     0.0|         50.0|     21.9|       1|\n",
      "|  1024.1|   19.4|       16.1|   14.6|     9.3|    75.0| 47.0|     8.3|         70.0|     18.1|       1|\n",
      "|  1013.4|   18.1|       17.8|   16.9|    16.8|    95.0| 95.0|     0.0|         60.0|     35.6|       1|\n",
      "|  1021.8|   21.3|       18.4|   15.2|     9.6|    52.0| 45.0|     3.6|         40.0|     24.8|       0|\n",
      "|  1022.7|   20.6|       18.6|   16.5|    12.5|    79.0| 81.0|     0.0|         20.0|     15.7|       1|\n",
      "|  1022.8|   19.5|       18.4|   15.3|    11.3|    56.0| 46.0|     7.6|         20.0|     28.4|       0|\n",
      "|  1019.7|   15.8|       13.6|   12.7|    11.8|    96.0|100.0|     0.0|         50.0|     52.8|       1|\n",
      "|  1017.4|   17.6|       16.5|   15.6|    12.5|    86.0|100.0|     0.0|         50.0|     37.5|       1|\n",
      "|  1025.4|   16.5|       14.4|   12.0|     8.6|    77.0| 84.0|     1.0|         50.0|     38.3|       0|\n",
      "|  1016.8|   16.3|       15.3|   14.1|    14.0|    97.0|100.0|     0.0|         40.0|     24.4|       1|\n",
      "|  1012.5|   16.2|       15.2|   14.0|    12.4|    98.0|100.0|     0.0|         50.0|     23.5|       1|\n",
      "|  1020.4|   15.0|       15.5|   13.2|    12.0|    77.0| 86.0|     0.0|         50.0|     32.4|       1|\n",
      "|  1012.5|   13.5|       12.9|   11.6|    11.8|    87.0| 92.0|     0.0|         50.0|     37.0|       1|\n",
      "|  1018.4|   17.8|       16.5|   15.1|    12.1|    77.0| 85.0|     1.2|         40.0|     20.9|       1|\n",
      "|  1024.3|   15.3|       12.9|   10.0|    11.2|    79.0| 91.0|     0.3|         20.0|     20.0|       1|\n",
      "|  1022.5|   16.3|       13.1|   11.4|     2.0|    79.0| 70.0|     6.8|         20.0|     30.3|       1|\n",
      "|  1034.6|   17.5|       16.2|   14.1|    11.8|    68.0| 60.0|     2.5|         50.0|     13.4|       0|\n",
      "|  1024.1|   16.8|       15.8|   14.0|    12.9|    76.0| 77.0|     0.0|         20.0|     20.5|       1|\n",
      "|  1020.2|   16.4|       14.2|   12.7|    11.6|    75.0| 93.0|     0.1|         40.0|     21.9|       1|\n",
      "+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_set=df_train_set.select(['pressure','maxtemp','temparature','mintemp','dewpoint','humidity','cloud','sunshine','winddirection','windspeed','rainfall'])\n",
    "df_train_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperate the independent features and target lables\n",
    "#independent_features=df_train_set.select(['id','day','pressure','maxtemp','temparature','mintemp','dewpoint','humidity','cloud','sunshine','winddirection','windspeed'])\n",
    "#target_feature=df_train_set.select(['rainfall'])\n",
    "#independent_features.show(),target_feature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+--------------------+\n",
      "|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|rainfall|independent_features|\n",
      "+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+--------------------+\n",
      "|  1017.4|   21.2|       20.6|   19.9|    19.4|    87.0| 88.0|     1.1|         60.0|     17.2|       1|[1017.4,21.2,20.6...|\n",
      "|  1019.5|   16.2|       16.9|   15.8|    15.4|    95.0| 91.0|     0.0|         50.0|     21.9|       1|[1019.5,16.2,16.9...|\n",
      "|  1024.1|   19.4|       16.1|   14.6|     9.3|    75.0| 47.0|     8.3|         70.0|     18.1|       1|[1024.1,19.4,16.1...|\n",
      "|  1013.4|   18.1|       17.8|   16.9|    16.8|    95.0| 95.0|     0.0|         60.0|     35.6|       1|[1013.4,18.1,17.8...|\n",
      "|  1021.8|   21.3|       18.4|   15.2|     9.6|    52.0| 45.0|     3.6|         40.0|     24.8|       0|[1021.8,21.3,18.4...|\n",
      "|  1022.7|   20.6|       18.6|   16.5|    12.5|    79.0| 81.0|     0.0|         20.0|     15.7|       1|[1022.7,20.6,18.6...|\n",
      "|  1022.8|   19.5|       18.4|   15.3|    11.3|    56.0| 46.0|     7.6|         20.0|     28.4|       0|[1022.8,19.5,18.4...|\n",
      "|  1019.7|   15.8|       13.6|   12.7|    11.8|    96.0|100.0|     0.0|         50.0|     52.8|       1|[1019.7,15.8,13.6...|\n",
      "|  1017.4|   17.6|       16.5|   15.6|    12.5|    86.0|100.0|     0.0|         50.0|     37.5|       1|[1017.4,17.6,16.5...|\n",
      "|  1025.4|   16.5|       14.4|   12.0|     8.6|    77.0| 84.0|     1.0|         50.0|     38.3|       0|[1025.4,16.5,14.4...|\n",
      "|  1016.8|   16.3|       15.3|   14.1|    14.0|    97.0|100.0|     0.0|         40.0|     24.4|       1|[1016.8,16.3,15.3...|\n",
      "|  1012.5|   16.2|       15.2|   14.0|    12.4|    98.0|100.0|     0.0|         50.0|     23.5|       1|[1012.5,16.2,15.2...|\n",
      "|  1020.4|   15.0|       15.5|   13.2|    12.0|    77.0| 86.0|     0.0|         50.0|     32.4|       1|[1020.4,15.0,15.5...|\n",
      "|  1012.5|   13.5|       12.9|   11.6|    11.8|    87.0| 92.0|     0.0|         50.0|     37.0|       1|[1012.5,13.5,12.9...|\n",
      "|  1018.4|   17.8|       16.5|   15.1|    12.1|    77.0| 85.0|     1.2|         40.0|     20.9|       1|[1018.4,17.8,16.5...|\n",
      "|  1024.3|   15.3|       12.9|   10.0|    11.2|    79.0| 91.0|     0.3|         20.0|     20.0|       1|[1024.3,15.3,12.9...|\n",
      "|  1022.5|   16.3|       13.1|   11.4|     2.0|    79.0| 70.0|     6.8|         20.0|     30.3|       1|[1022.5,16.3,13.1...|\n",
      "|  1034.6|   17.5|       16.2|   14.1|    11.8|    68.0| 60.0|     2.5|         50.0|     13.4|       0|[1034.6,17.5,16.2...|\n",
      "|  1024.1|   16.8|       15.8|   14.0|    12.9|    76.0| 77.0|     0.0|         20.0|     20.5|       1|[1024.1,16.8,15.8...|\n",
      "|  1020.2|   16.4|       14.2|   12.7|    11.6|    75.0| 93.0|     0.1|         40.0|     21.9|       1|[1020.2,16.4,14.2...|\n",
      "+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_assembler = VectorAssembler(inputCols=['pressure','maxtemp','temparature','mintemp','dewpoint','humidity','cloud','sunshine','winddirection','windspeed'],outputCol='independent_features')\n",
    "output_features =feature_assembler.transform(df_train_set)\n",
    "output_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|independent_features|\n",
      "+--------------------+\n",
      "|[1017.4,21.2,20.6...|\n",
      "|[1019.5,16.2,16.9...|\n",
      "|[1024.1,19.4,16.1...|\n",
      "|[1013.4,18.1,17.8...|\n",
      "|[1021.8,21.3,18.4...|\n",
      "|[1022.7,20.6,18.6...|\n",
      "|[1022.8,19.5,18.4...|\n",
      "|[1019.7,15.8,13.6...|\n",
      "|[1017.4,17.6,16.5...|\n",
      "|[1025.4,16.5,14.4...|\n",
      "|[1016.8,16.3,15.3...|\n",
      "|[1012.5,16.2,15.2...|\n",
      "|[1020.4,15.0,15.5...|\n",
      "|[1012.5,13.5,12.9...|\n",
      "|[1018.4,17.8,16.5...|\n",
      "|[1024.3,15.3,12.9...|\n",
      "|[1022.5,16.3,13.1...|\n",
      "|[1034.6,17.5,16.2...|\n",
      "|[1024.1,16.8,15.8...|\n",
      "|[1020.2,16.4,14.2...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## select the  indepedent features from dataset\n",
    "output_independent_features=output_features.select('independent_features')\n",
    "output_independent_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale the dataset usinf standard scaling\n",
    "#from pyspark.ml.feature import StandardScaler\n",
    "#scaler=StandardScaler(inputCol='independent_features', outputCol='scaled_features',withStd=True, withMean=False)\n",
    "#scaler_model=scaler.fit(output_independent_features)\n",
    "#scaled_train_data = scaler_model.transform(output_independent_features)\n",
    "#scaled_train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_train_preprocessed_data=scaled_train_data.select('scaled_features')\n",
    "#scaled_train_preprocessed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explotary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_train_preprocessed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to pandas dataframe\n",
    "#pandas_df=scaled_train_preprocessed_data.toPandas()\n",
    "#pandas_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pressure</th>\n",
       "      <th>maxtemp</th>\n",
       "      <th>temparature</th>\n",
       "      <th>mintemp</th>\n",
       "      <th>dewpoint</th>\n",
       "      <th>humidity</th>\n",
       "      <th>cloud</th>\n",
       "      <th>sunshine</th>\n",
       "      <th>winddirection</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1017.4</td>\n",
       "      <td>21.2</td>\n",
       "      <td>20.6</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.4</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1019.5</td>\n",
       "      <td>16.2</td>\n",
       "      <td>16.9</td>\n",
       "      <td>15.8</td>\n",
       "      <td>15.4</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024.1</td>\n",
       "      <td>19.4</td>\n",
       "      <td>16.1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1013.4</td>\n",
       "      <td>18.1</td>\n",
       "      <td>17.8</td>\n",
       "      <td>16.9</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021.8</td>\n",
       "      <td>21.3</td>\n",
       "      <td>18.4</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pressure  maxtemp  temparature  mintemp  dewpoint  humidity  cloud  \\\n",
       "0    1017.4     21.2         20.6     19.9      19.4      87.0   88.0   \n",
       "1    1019.5     16.2         16.9     15.8      15.4      95.0   91.0   \n",
       "2    1024.1     19.4         16.1     14.6       9.3      75.0   47.0   \n",
       "3    1013.4     18.1         17.8     16.9      16.8      95.0   95.0   \n",
       "4    1021.8     21.3         18.4     15.2       9.6      52.0   45.0   \n",
       "\n",
       "   sunshine  winddirection  windspeed  rainfall  \n",
       "0       1.1           60.0       17.2         1  \n",
       "1       0.0           50.0       21.9         1  \n",
       "2       8.3           70.0       18.1         1  \n",
       "3       0.0           60.0       35.6         1  \n",
       "4       3.6           40.0       24.8         0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_set_pandas = df_train_set.toPandas()\n",
    "df_train_set_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pressure</th>\n",
       "      <th>maxtemp</th>\n",
       "      <th>temparature</th>\n",
       "      <th>mintemp</th>\n",
       "      <th>dewpoint</th>\n",
       "      <th>humidity</th>\n",
       "      <th>cloud</th>\n",
       "      <th>sunshine</th>\n",
       "      <th>winddirection</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1017.4</td>\n",
       "      <td>21.2</td>\n",
       "      <td>20.6</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.4</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1019.5</td>\n",
       "      <td>16.2</td>\n",
       "      <td>16.9</td>\n",
       "      <td>15.8</td>\n",
       "      <td>15.4</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024.1</td>\n",
       "      <td>19.4</td>\n",
       "      <td>16.1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1013.4</td>\n",
       "      <td>18.1</td>\n",
       "      <td>17.8</td>\n",
       "      <td>16.9</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021.8</td>\n",
       "      <td>21.3</td>\n",
       "      <td>18.4</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pressure  maxtemp  temparature  mintemp  dewpoint  humidity  cloud  \\\n",
       "0    1017.4     21.2         20.6     19.9      19.4      87.0   88.0   \n",
       "1    1019.5     16.2         16.9     15.8      15.4      95.0   91.0   \n",
       "2    1024.1     19.4         16.1     14.6       9.3      75.0   47.0   \n",
       "3    1013.4     18.1         17.8     16.9      16.8      95.0   95.0   \n",
       "4    1021.8     21.3         18.4     15.2       9.6      52.0   45.0   \n",
       "\n",
       "   sunshine  winddirection  windspeed  \n",
       "0       1.1           60.0       17.2  \n",
       "1       0.0           50.0       21.9  \n",
       "2       8.3           70.0       18.1  \n",
       "3       0.0           60.0       35.6  \n",
       "4       3.6           40.0       24.8  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_set_pandas_new=df_train_set_pandas.drop('rainfall',axis=True)\n",
    "df_train_set_pandas_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "2185    1\n",
       "2186    1\n",
       "2187    1\n",
       "2188    1\n",
       "2189    1\n",
       "Name: rainfall, Length: 2190, dtype: int32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_set_pandas_target_column=df_train_set_pandas['rainfall']\n",
    "df_train_set_pandas_target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity',\n",
       "       'cloud', 'sunshine', 'winddirection', 'windspeed', 'rainfall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_set_pandas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for df_train_set_pandas_columns in df_train_set_pandas.columns:\n",
    "    #print(type(df_train_set_pandas_columns))\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.bar(df_train_set_pandas[df_train_set_pandas_columns], df_train_set_pandas[\"rainfall\"],color=\"skyblue\")\n",
    "    plt.xlabel(df_train_set_pandas_columns)\n",
    "    plt.ylabel(\"rainfall\")\n",
    "    plt.title(\"rainfall vs \"+df_train_set_pandas_columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2190 entries, 0 to 2189\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   pressure       2190 non-null   float64\n",
      " 1   maxtemp        2190 non-null   float64\n",
      " 2   temparature    2190 non-null   float64\n",
      " 3   mintemp        2190 non-null   float64\n",
      " 4   dewpoint       2190 non-null   float64\n",
      " 5   humidity       2190 non-null   float64\n",
      " 6   cloud          2190 non-null   float64\n",
      " 7   sunshine       2190 non-null   float64\n",
      " 8   winddirection  2190 non-null   float64\n",
      " 9   windspeed      2190 non-null   float64\n",
      " 10  rainfall       2190 non-null   int32  \n",
      "dtypes: float64(10), int32(1)\n",
      "memory usage: 179.8 KB\n"
     ]
    }
   ],
   "source": [
    "## check the information of dataset\n",
    "pandas_df_info=df_train_set_pandas.info()\n",
    "pandas_df_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pressure</th>\n",
       "      <th>maxtemp</th>\n",
       "      <th>temparature</th>\n",
       "      <th>mintemp</th>\n",
       "      <th>dewpoint</th>\n",
       "      <th>humidity</th>\n",
       "      <th>cloud</th>\n",
       "      <th>sunshine</th>\n",
       "      <th>winddirection</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "      <td>2190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1013.602146</td>\n",
       "      <td>26.365799</td>\n",
       "      <td>23.953059</td>\n",
       "      <td>22.170091</td>\n",
       "      <td>20.454566</td>\n",
       "      <td>82.036530</td>\n",
       "      <td>75.721918</td>\n",
       "      <td>3.744429</td>\n",
       "      <td>104.863151</td>\n",
       "      <td>21.804703</td>\n",
       "      <td>0.753425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.655366</td>\n",
       "      <td>5.654330</td>\n",
       "      <td>5.222410</td>\n",
       "      <td>5.059120</td>\n",
       "      <td>5.288406</td>\n",
       "      <td>7.800654</td>\n",
       "      <td>18.026498</td>\n",
       "      <td>3.626327</td>\n",
       "      <td>80.002416</td>\n",
       "      <td>9.898659</td>\n",
       "      <td>0.431116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1008.600000</td>\n",
       "      <td>21.300000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>14.125000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1013.000000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>23.850000</td>\n",
       "      <td>22.150000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1017.775000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1034.600000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>26.700000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pressure      maxtemp  temparature      mintemp     dewpoint  \\\n",
       "count  2190.000000  2190.000000  2190.000000  2190.000000  2190.000000   \n",
       "mean   1013.602146    26.365799    23.953059    22.170091    20.454566   \n",
       "std       5.655366     5.654330     5.222410     5.059120     5.288406   \n",
       "min     999.000000    10.400000     7.400000     4.000000    -0.300000   \n",
       "25%    1008.600000    21.300000    19.300000    17.700000    16.800000   \n",
       "50%    1013.000000    27.800000    25.500000    23.850000    22.150000   \n",
       "75%    1017.775000    31.200000    28.400000    26.400000    25.000000   \n",
       "max    1034.600000    36.000000    31.500000    29.800000    26.700000   \n",
       "\n",
       "          humidity        cloud     sunshine  winddirection    windspeed  \\\n",
       "count  2190.000000  2190.000000  2190.000000    2190.000000  2190.000000   \n",
       "mean     82.036530    75.721918     3.744429     104.863151    21.804703   \n",
       "std       7.800654    18.026498     3.626327      80.002416     9.898659   \n",
       "min      39.000000     2.000000     0.000000      10.000000     4.400000   \n",
       "25%      77.000000    69.000000     0.400000      40.000000    14.125000   \n",
       "50%      82.000000    83.000000     2.400000      70.000000    20.500000   \n",
       "75%      88.000000    88.000000     6.800000     200.000000    27.900000   \n",
       "max      98.000000   100.000000    12.100000     300.000000    59.500000   \n",
       "\n",
       "          rainfall  \n",
       "count  2190.000000  \n",
       "mean      0.753425  \n",
       "std       0.431116  \n",
       "min       0.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the summary stactics of dataset\n",
    "pandas_df_stat=df_train_set_pandas.describe()\n",
    "pandas_df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               pressure   maxtemp  temparature   mintemp  dewpoint  humidity  \\\n",
      "pressure       1.000000 -0.800499    -0.816531 -0.814453 -0.817008 -0.119949   \n",
      "maxtemp       -0.800499  1.000000     0.982932  0.965529  0.906703 -0.072615   \n",
      "temparature   -0.816531  0.982932     1.000000  0.987150  0.933617 -0.025016   \n",
      "mintemp       -0.814453  0.965529     0.987150  1.000000  0.941342  0.009891   \n",
      "dewpoint      -0.817008  0.906703     0.933617  0.941342  1.000000  0.153390   \n",
      "humidity      -0.119949 -0.072615    -0.025016  0.009891  0.153390  1.000000   \n",
      "cloud          0.098600 -0.289047    -0.249355 -0.219399 -0.088446  0.584854   \n",
      "sunshine      -0.257163  0.452387     0.414019  0.379497  0.249676 -0.541592   \n",
      "winddirection -0.643293  0.662235     0.668963  0.663828  0.643073 -0.012430   \n",
      "windspeed      0.266012 -0.354168    -0.342262 -0.328871 -0.312179  0.062285   \n",
      "rainfall      -0.049886 -0.079304    -0.049660 -0.026841  0.081965  0.454213   \n",
      "\n",
      "                  cloud  sunshine  winddirection  windspeed  rainfall  \n",
      "pressure       0.098600 -0.257163      -0.643293   0.266012 -0.049886  \n",
      "maxtemp       -0.289047  0.452387       0.662235  -0.354168 -0.079304  \n",
      "temparature   -0.249355  0.414019       0.668963  -0.342262 -0.049660  \n",
      "mintemp       -0.219399  0.379497       0.663828  -0.328871 -0.026841  \n",
      "dewpoint      -0.088446  0.249676       0.643073  -0.312179  0.081965  \n",
      "humidity       0.584854 -0.541592      -0.012430   0.062285  0.454213  \n",
      "cloud          1.000000 -0.805128      -0.127087   0.184698  0.641191  \n",
      "sunshine      -0.805128  1.000000       0.272235  -0.241752 -0.555287  \n",
      "winddirection -0.127087  0.272235       1.000000  -0.192417 -0.006939  \n",
      "windspeed      0.184698 -0.241752      -0.192417   1.000000  0.111625  \n",
      "rainfall       0.641191 -0.555287      -0.006939   0.111625  1.000000  \n"
     ]
    }
   ],
   "source": [
    "correlation=df_train_set_pandas.corr()\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the trainset into train and test set (validation set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(df_train_set_pandas_new,df_train_set_pandas_target_column, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1752, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1752,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf=RandomForestClassifier()\n",
    "rf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_predictions=rf_clf.predict(X_test)\n",
    "rf_clf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the randomforest classifier model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447488584474886"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "rf_clf_accuracy=accuracy_score(y_test,rf_clf_predictions)\n",
    "rf_clf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1552511415525114"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rf_clf_mean_sqaured_error=mean_squared_error(y_test,rf_clf_predictions)\n",
    "rf_clf_mean_sqaured_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 72,  47],\n",
       "       [ 21, 298]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn.metrics import confusion_matrix\n",
    "rf_clf_confusion_matrix=confusion_matrix(y_test,rf_clf_predictions)\n",
    "rf_clf_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.61      0.68       119\n",
      "           1       0.86      0.93      0.90       319\n",
      "\n",
      "    accuracy                           0.84       438\n",
      "   macro avg       0.82      0.77      0.79       438\n",
      "weighted avg       0.84      0.84      0.84       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "rf_clf_classification_report=classification_report(y_test,rf_clf_predictions)\n",
    "print(rf_clf_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf=DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make the predictions in DecisionTree Classifier using X_test\n",
    "dt_clf_predictions=dt_clf.predict(X_test)\n",
    "dt_clf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the decisiontree classifier model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7876712328767124"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "dt_clf_accuracy_score=accuracy_score(y_test,dt_clf_predictions)\n",
    "dt_clf_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21232876712328766"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "dt_clf_mean_squared_error=mean_squared_error(y_test,dt_clf_predictions)\n",
    "dt_clf_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.60      0.60       119\n",
      "           1       0.85      0.86      0.85       319\n",
      "\n",
      "    accuracy                           0.79       438\n",
      "   macro avg       0.73      0.73      0.73       438\n",
      "weighted avg       0.79      0.79      0.79       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "dt_clf_classification_report=classification_report(y_test,dt_clf_predictions)\n",
    "print(dt_clf_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 71,  48],\n",
       "       [ 45, 274]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "dt_clf_confusion_matrix=confusion_matrix(y_test,dt_clf_predictions)\n",
    "dt_clf_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_predictions=lr_clf.predict(X_test)\n",
    "lr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1506849315068493"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluvate the logistic regression model performance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lr_clf_mean_squared_error=mean_squared_error(y_test,lr_predictions)\n",
    "lr_clf_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493150684931506"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "lr_clf_accuracy_score=accuracy_score(y_test,lr_predictions)\n",
    "lr_clf_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.61      0.69       119\n",
      "           1       0.86      0.94      0.90       319\n",
      "\n",
      "    accuracy                           0.85       438\n",
      "   macro avg       0.83      0.77      0.79       438\n",
      "weighted avg       0.84      0.85      0.84       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "lr_clf_classification_report=classification_report(y_test,lr_predictions)\n",
    "print(lr_clf_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 72,  47],\n",
       "       [ 19, 300]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "lr_clf_confusion_matrix=confusion_matrix(y_test,lr_predictions)\n",
    "lr_clf_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_clf=SVC()\n",
    "svc_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_predictions=svc_clf.predict(X_test)\n",
    "svc_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2602739726027397"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluvate the SVC model performance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "svc_mean_sqaured_error=mean_squared_error(y_test,svc_predictions)\n",
    "svc_mean_sqaured_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7397260273972602"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "svc_accuracy_score=accuracy_score(y_test,svc_predictions)\n",
    "svc_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.04      0.08       119\n",
      "           1       0.74      1.00      0.85       319\n",
      "\n",
      "    accuracy                           0.74       438\n",
      "   macro avg       0.87      0.52      0.46       438\n",
      "weighted avg       0.81      0.74      0.64       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "svc_classification_report=classification_report(y_test,svc_predictions)\n",
    "print(svc_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5, 114],\n",
       "       [  0, 319]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "svc_confusion_matrix=confusion_matrix(y_test,svc_predictions)\n",
    "svc_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning and Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_clf_param_grid={'n_estimators':[100,200,300,400],'criterion':['gini','entropy','log_loss'],\n",
    "                   'max_depth':[100,200,300,400],'min_samples_split':[2,4,6,8,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [100, 200, 300, 400],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 6, 8, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [100, 200, 300, 400],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 6, 8, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: RandomForestClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=100, min_samples_split=8, n_estimators=400)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=100, min_samples_split=8, n_estimators=400)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': [100, 200, 300, 400],\n",
       "                         'min_samples_split': [2, 4, 6, 8, 10],\n",
       "                         'n_estimators': [100, 200, 300, 400]})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv_rf_clf=GridSearchCV(estimator=rf_clf,param_grid=rf_clf_param_grid)\n",
    "grid_search_cv_rf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 100,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv_rf_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make the predictions using grid_search_cv_rf_clf\n",
    "grid_search_cv_rf_clf_predictions=grid_search_cv_rf_clf.predict(X_test)\n",
    "grid_search_cv_rf_clf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluvate perofrmance of random forest classifier model after doing the hyper parameter tuning and grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8561643835616438"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv_rf_clf_accuracy_score=grid_search_cv_rf_clf.score(X_test,y_test)\n",
    "grid_search_cv_rf_clf_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14383561643835616"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv_rf_clf_mean_squared_error=mean_squared_error(y_test,grid_search_cv_rf_clf_predictions)\n",
    "grid_search_cv_rf_clf_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.62      0.70       119\n",
      "           1       0.87      0.94      0.91       319\n",
      "\n",
      "    accuracy                           0.86       438\n",
      "   macro avg       0.84      0.78      0.80       438\n",
      "weighted avg       0.85      0.86      0.85       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_search_cv_rf_clf_classification_report=classification_report(y_test,grid_search_cv_rf_clf_predictions)\n",
    "print(grid_search_cv_rf_clf_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 74,  45],\n",
       "       [ 18, 301]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv_rf_clf_confusion_matrix=confusion_matrix(y_test,grid_search_cv_rf_clf_predictions)\n",
    "grid_search_cv_rf_clf_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_clf_param_grid={'criterion':[\"gini\",\"entropy\",\"log_loss\"],'splitter':['best','random'],'max_depth':[100,200,300,400],\n",
    "                   'min_samples_split':[2,4,6,8,10],'min_samples_leaf':[2,4,6,8,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [100, 200, 300, 400],\n",
       "                         &#x27;min_samples_leaf&#x27;: [2, 4, 6, 8, 10],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 6, 8, 10],\n",
       "                         &#x27;splitter&#x27;: [&#x27;best&#x27;, &#x27;random&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;, &#x27;log_loss&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [100, 200, 300, 400],\n",
       "                         &#x27;min_samples_leaf&#x27;: [2, 4, 6, 8, 10],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 4, 6, 8, 10],\n",
       "                         &#x27;splitter&#x27;: [&#x27;best&#x27;, &#x27;random&#x27;]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: DecisionTreeClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;log_loss&#x27;, max_depth=300, min_samples_leaf=10,\n",
       "                       min_samples_split=4, splitter=&#x27;random&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;log_loss&#x27;, max_depth=300, min_samples_leaf=10,\n",
       "                       min_samples_split=4, splitter=&#x27;random&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy', 'log_loss'],\n",
       "                         'max_depth': [100, 200, 300, 400],\n",
       "                         'min_samples_leaf': [2, 4, 6, 8, 10],\n",
       "                         'min_samples_split': [2, 4, 6, 8, 10],\n",
       "                         'splitter': ['best', 'random']})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf_grid_search_cv=GridSearchCV(estimator=dt_clf,param_grid=dt_clf_param_grid)\n",
    "dt_clf_grid_search_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'log_loss',\n",
       " 'max_depth': 300,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 4,\n",
       " 'splitter': 'random'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Identify the decision tree best paramters after completing the hyper parameter tuning and grid search cv\n",
    "dt_clf_grid_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make the predictions\n",
    "dt_clf_grid_search_cv_predictions=dt_clf_grid_search_cv.predict(X_test)\n",
    "dt_clf_grid_search_cv_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8059360730593608"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluvate the decision tree model after doing the hyper parameter tuning and grid search cv\n",
    "dt_clf_grid_search_cv_accuracy_score=accuracy_score(y_test,dt_clf_grid_search_cv_predictions)\n",
    "dt_clf_grid_search_cv_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19406392694063926"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf_grid_search_cv_mean_sqaured_error=mean_squared_error(y_test,dt_clf_grid_search_cv_predictions)\n",
    "dt_clf_grid_search_cv_mean_sqaured_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.52      0.59       119\n",
      "           1       0.84      0.91      0.87       319\n",
      "\n",
      "    accuracy                           0.81       438\n",
      "   macro avg       0.76      0.72      0.73       438\n",
      "weighted avg       0.80      0.81      0.80       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_clf_grid_search_cv_classification_report=classification_report(y_test,dt_clf_grid_search_cv_predictions)\n",
    "print(dt_clf_grid_search_cv_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 62,  57],\n",
       "       [ 28, 291]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf_grid_search_cv_confusion_matrix=confusion_matrix(y_test,dt_clf_grid_search_cv_predictions)\n",
    "dt_clf_grid_search_cv_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'penalty':['l1','elasticnet','l2'],'dual':[False],'C':[100.0,200.0,300.0,400.0,500.0,600.0,700.0,800.0,900.0,1000.0],\n",
    "            'class_weight':['dict','balanced'],'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': ['l1', 'elasticnet', 'l2'],\n",
       " 'dual': [False],\n",
       " 'C': [100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0, 800.0, 900.0, 1000.0],\n",
       " 'class_weight': ['dict', 'balanced'],\n",
       " 'solver': ['lbfgs',\n",
       "  'liblinear',\n",
       "  'newton-cg',\n",
       "  'newton-cholesky',\n",
       "  'sag',\n",
       "  'saga']}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "1400 fits failed out of a total of 1800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'class_weight' parameter of LogisticRegression must be an instance of 'dict', a str among {'balanced'} or None. Got 'dict' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1203, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84188685        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84417257 0.84074725 0.84245991 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84188685        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.844744   0.84074725 0.84246154 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84303134 0.84074725 0.84246154 0.84302971 0.85044526 0.85158812\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84188685        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84188685 0.84074725 0.84189011 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84360602 0.84074725 0.84189011 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85044689\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84132194 0.84074725 0.84303134 0.84302971 0.85044526 0.85044689\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84417257 0.84074725 0.84245991 0.84302971 0.85044526 0.85158974\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84303134 0.84074725 0.84189011 0.84302971 0.85044526 0.85216117\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85044851\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84245991 0.84074725 0.84189011 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84302971 0.84074725 0.84189011 0.84302971 0.85044526 0.85158812]\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0,\n",
       "                               800.0, 900.0, 1000.0],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;dict&#x27;, &#x27;balanced&#x27;], &#x27;dual&#x27;: [False],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;elasticnet&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                    &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0,\n",
       "                               800.0, 900.0, 1000.0],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;dict&#x27;, &#x27;balanced&#x27;], &#x27;dual&#x27;: [False],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;elasticnet&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                    &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: LogisticRegression</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=800.0, class_weight=&#x27;balanced&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=800.0, class_weight=&#x27;balanced&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={'C': [100.0, 200.0, 300.0, 400.0, 500.0, 600.0, 700.0,\n",
       "                               800.0, 900.0, 1000.0],\n",
       "                         'class_weight': ['dict', 'balanced'], 'dual': [False],\n",
       "                         'penalty': ['l1', 'elasticnet', 'l2'],\n",
       "                         'solver': ['lbfgs', 'liblinear', 'newton-cg',\n",
       "                                    'newton-cholesky', 'sag', 'saga']})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_clf_grid_search_cv=GridSearchCV(estimator=lr_clf,param_grid=param_grid)\n",
    "lr_clf_grid_search_cv.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 800.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'penalty': 'l2',\n",
       " 'solver': 'saga'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_grid_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make the predictions using logistic regression\n",
    "lr_clf_grid_search_cv_predictions=lr_clf_grid_search_cv.predict(X_test)\n",
    "lr_clf_grid_search_cv_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluvate the  logistic regression model after doing the hyper parameter tuning and gridf search  cv\n",
    "\n",
    "lr_clf_grid_search_cv_mean_squared_error=mean_squared_error(lr_clf_grid_search_cv_predictions,y_test)\n",
    "lr_clf_grid_search_cv_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_grid_search_cv_accuracy_score=accuracy_score(lr_clf_grid_search_cv_predictions,y_test)\n",
    "lr_clf_grid_search_cv_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 81,  35],\n",
       "       [ 38, 284]], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_grid_search_cv_confusion_matrix=confusion_matrix(lr_clf_grid_search_cv_predictions,y_test)\n",
    "lr_clf_grid_search_cv_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       116\n",
      "           1       0.89      0.88      0.89       322\n",
      "\n",
      "    accuracy                           0.83       438\n",
      "   macro avg       0.79      0.79      0.79       438\n",
      "weighted avg       0.83      0.83      0.83       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_clf_grid_search_cv_classification_report=classification_report(lr_clf_grid_search_cv_predictions,y_test)\n",
    "print(lr_clf_grid_search_cv_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8561643835616438, 0.8059360730593608, 0.8333333333333334)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the summary of  accuracies after doing the   hyper parameter tuning \n",
    "grid_search_cv_rf_clf_accuracy_score,dt_clf_grid_search_cv_accuracy_score,lr_clf_grid_search_cv_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the accuracies of  model before doing  the hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8447488584474886, 0.7876712328767124, 0.8493150684931506)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_accuracy,dt_clf_accuracy_score,lr_clf_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After comppleting the hyper parameter tuning and grid search  cv   random forest classifier  can be recognized as the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier Ensemble Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import  voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;rf_classifier&#x27;, RandomForestClassifier()),\n",
       "                             (&#x27;decision_tree&#x27;, DecisionTreeClassifier()),\n",
       "                             (&#x27;logistic_regression0&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>VotingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.VotingClassifier.html\">?<span>Documentation for VotingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>VotingClassifier(estimators=[(&#x27;rf_classifier&#x27;, RandomForestClassifier()),\n",
       "                             (&#x27;decision_tree&#x27;, DecisionTreeClassifier()),\n",
       "                             (&#x27;logistic_regression0&#x27;, LogisticRegression())])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>rf_classifier</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>decision_tree</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>logistic_regression0</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('rf_classifier', RandomForestClassifier()),\n",
       "                             ('decision_tree', DecisionTreeClassifier()),\n",
       "                             ('logistic_regression0', LogisticRegression())])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "## Making the final model using   voiting classifier\n",
    "final_model=VotingClassifier(estimators=[('rf_classifier',rf_clf),('decision_tree',dt_clf),('logistic_regression0',lr_clf)], voting='hard')\n",
    "\n",
    "## train the  final model using  all the train dataet\n",
    "final_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make the  predictions using final model\n",
    "final_model_predictions=final_model.predict(X_test)\n",
    "final_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493150684931506"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluvate the final model  performance  using metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "final_model_accuracy_score=accuracy_score(y_test,final_model_predictions)\n",
    "final_model_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1506849315068493"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics  import mean_squared_error\n",
    "final_model_mean_squared_error=mean_squared_error(y_test,final_model_predictions)\n",
    "final_model_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.61      0.69       119\n",
      "           1       0.87      0.94      0.90       319\n",
      "\n",
      "    accuracy                           0.85       438\n",
      "   macro avg       0.83      0.78      0.79       438\n",
      "weighted avg       0.84      0.85      0.84       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "final_model_classification_report=classification_report(y_test,final_model_predictions)\n",
    "print(final_model_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73,  46],\n",
       "       [ 20, 299]], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "final_model_confusion_matrix=confusion_matrix(y_test,final_model_predictions)\n",
    "final_model_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ensemble method use for after doing the hyper parameter tuning and grid search  cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "1400 fits failed out of a total of 1800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'class_weight' parameter of LogisticRegression must be an instance of 'dict', a str among {'balanced'} or None. Got 'dict' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 63, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1203, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84074725        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84417257 0.84074725 0.84245991 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.844744   0.84074725 0.84246154 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84303134 0.84074725 0.84246154 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84188685 0.84074725 0.84189011 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85044851\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84360602 0.84074725 0.84189011 0.84302971 0.85044526 0.85158812\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84132194 0.84074725 0.84303134 0.84302971 0.84987546 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84417257 0.84074725 0.84245991 0.84302971 0.85044526 0.85044689\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84303134 0.84074725 0.84189011 0.84302971 0.85044526 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85101832\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84245991 0.84074725 0.84189011 0.84302971 0.85044526 0.85158812\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.84131705        nan        nan        nan 0.85158812\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84302971 0.84074725 0.84189011 0.84302971 0.85044526 0.85101832]\n",
      "  warnings.warn(\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;rf_clf_after_hyper_parameter&#x27;,\n",
       "                              GridSearchCV(estimator=RandomForestClassifier(),\n",
       "                                           param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;,\n",
       "                                                                     &#x27;entropy&#x27;,\n",
       "                                                                     &#x27;log_loss&#x27;],\n",
       "                                                       &#x27;max_depth&#x27;: [100, 200,\n",
       "                                                                     300, 400],\n",
       "                                                       &#x27;min_samples_split&#x27;: [2,\n",
       "                                                                             4,\n",
       "                                                                             6,\n",
       "                                                                             8,\n",
       "                                                                             10],\n",
       "                                                       &#x27;n_estimators&#x27;: [100,\n",
       "                                                                        200,\n",
       "                                                                        300,\n",
       "                                                                        400]})),\n",
       "                             (&#x27;dt_clf_after_hyper_parameter&#x27;,\n",
       "                              GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "                                           param_gr...\n",
       "                                                                             10],\n",
       "                                                       &#x27;splitter&#x27;: [&#x27;best&#x27;,\n",
       "                                                                    &#x27;random&#x27;]})),\n",
       "                             (&#x27;lr_after_hyper_parameter&#x27;,\n",
       "                              GridSearchCV(estimator=LogisticRegression(),\n",
       "                                           param_grid={&#x27;C&#x27;: [100.0, 200.0,\n",
       "                                                             300.0, 400.0,\n",
       "                                                             500.0, 600.0,\n",
       "                                                             700.0, 800.0,\n",
       "                                                             900.0, 1000.0],\n",
       "                                                       &#x27;class_weight&#x27;: [&#x27;dict&#x27;,\n",
       "                                                                        &#x27;balanced&#x27;],\n",
       "                                                       &#x27;dual&#x27;: [False],\n",
       "                                                       &#x27;penalty&#x27;: [&#x27;l1&#x27;,\n",
       "                                                                   &#x27;elasticnet&#x27;,\n",
       "                                                                   &#x27;l2&#x27;],\n",
       "                                                       &#x27;solver&#x27;: [&#x27;lbfgs&#x27;,\n",
       "                                                                  &#x27;liblinear&#x27;,\n",
       "                                                                  &#x27;newton-cg&#x27;,\n",
       "                                                                  &#x27;newton-cholesky&#x27;,\n",
       "                                                                  &#x27;sag&#x27;,\n",
       "                                                                  &#x27;saga&#x27;]}))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>VotingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.VotingClassifier.html\">?<span>Documentation for VotingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>VotingClassifier(estimators=[(&#x27;rf_clf_after_hyper_parameter&#x27;,\n",
       "                              GridSearchCV(estimator=RandomForestClassifier(),\n",
       "                                           param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;,\n",
       "                                                                     &#x27;entropy&#x27;,\n",
       "                                                                     &#x27;log_loss&#x27;],\n",
       "                                                       &#x27;max_depth&#x27;: [100, 200,\n",
       "                                                                     300, 400],\n",
       "                                                       &#x27;min_samples_split&#x27;: [2,\n",
       "                                                                             4,\n",
       "                                                                             6,\n",
       "                                                                             8,\n",
       "                                                                             10],\n",
       "                                                       &#x27;n_estimators&#x27;: [100,\n",
       "                                                                        200,\n",
       "                                                                        300,\n",
       "                                                                        400]})),\n",
       "                             (&#x27;dt_clf_after_hyper_parameter&#x27;,\n",
       "                              GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "                                           param_gr...\n",
       "                                                                             10],\n",
       "                                                       &#x27;splitter&#x27;: [&#x27;best&#x27;,\n",
       "                                                                    &#x27;random&#x27;]})),\n",
       "                             (&#x27;lr_after_hyper_parameter&#x27;,\n",
       "                              GridSearchCV(estimator=LogisticRegression(),\n",
       "                                           param_grid={&#x27;C&#x27;: [100.0, 200.0,\n",
       "                                                             300.0, 400.0,\n",
       "                                                             500.0, 600.0,\n",
       "                                                             700.0, 800.0,\n",
       "                                                             900.0, 1000.0],\n",
       "                                                       &#x27;class_weight&#x27;: [&#x27;dict&#x27;,\n",
       "                                                                        &#x27;balanced&#x27;],\n",
       "                                                       &#x27;dual&#x27;: [False],\n",
       "                                                       &#x27;penalty&#x27;: [&#x27;l1&#x27;,\n",
       "                                                                   &#x27;elasticnet&#x27;,\n",
       "                                                                   &#x27;l2&#x27;],\n",
       "                                                       &#x27;solver&#x27;: [&#x27;lbfgs&#x27;,\n",
       "                                                                  &#x27;liblinear&#x27;,\n",
       "                                                                  &#x27;newton-cg&#x27;,\n",
       "                                                                  &#x27;newton-cholesky&#x27;,\n",
       "                                                                  &#x27;sag&#x27;,\n",
       "                                                                  &#x27;saga&#x27;]}))])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>rf_clf_after_hyper_parameter</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: RandomForestClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=100, min_samples_split=8, n_estimators=400)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=100, min_samples_split=8, n_estimators=400)</pre></div> </div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>dt_clf_after_hyper_parameter</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: DecisionTreeClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;log_loss&#x27;, max_depth=300, min_samples_leaf=10,\n",
       "                       min_samples_split=4, splitter=&#x27;random&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;log_loss&#x27;, max_depth=300, min_samples_leaf=10,\n",
       "                       min_samples_split=4, splitter=&#x27;random&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>lr_after_hyper_parameter</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: LogisticRegression</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=800.0, class_weight=&#x27;balanced&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=800.0, class_weight=&#x27;balanced&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('rf_clf_after_hyper_parameter',\n",
       "                              GridSearchCV(estimator=RandomForestClassifier(),\n",
       "                                           param_grid={'criterion': ['gini',\n",
       "                                                                     'entropy',\n",
       "                                                                     'log_loss'],\n",
       "                                                       'max_depth': [100, 200,\n",
       "                                                                     300, 400],\n",
       "                                                       'min_samples_split': [2,\n",
       "                                                                             4,\n",
       "                                                                             6,\n",
       "                                                                             8,\n",
       "                                                                             10],\n",
       "                                                       'n_estimators': [100,\n",
       "                                                                        200,\n",
       "                                                                        300,\n",
       "                                                                        400]})),\n",
       "                             ('dt_clf_after_hyper_parameter',\n",
       "                              GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "                                           param_gr...\n",
       "                                                                             10],\n",
       "                                                       'splitter': ['best',\n",
       "                                                                    'random']})),\n",
       "                             ('lr_after_hyper_parameter',\n",
       "                              GridSearchCV(estimator=LogisticRegression(),\n",
       "                                           param_grid={'C': [100.0, 200.0,\n",
       "                                                             300.0, 400.0,\n",
       "                                                             500.0, 600.0,\n",
       "                                                             700.0, 800.0,\n",
       "                                                             900.0, 1000.0],\n",
       "                                                       'class_weight': ['dict',\n",
       "                                                                        'balanced'],\n",
       "                                                       'dual': [False],\n",
       "                                                       'penalty': ['l1',\n",
       "                                                                   'elasticnet',\n",
       "                                                                   'l2'],\n",
       "                                                       'solver': ['lbfgs',\n",
       "                                                                  'liblinear',\n",
       "                                                                  'newton-cg',\n",
       "                                                                  'newton-cholesky',\n",
       "                                                                  'sag',\n",
       "                                                                  'saga']}))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "final_model_after_grid_search_cv=VotingClassifier(estimators=[('rf_clf_after_hyper_parameter',grid_search_cv_rf_clf),('dt_clf_after_hyper_parameter',dt_clf_grid_search_cv),\n",
    "                                                ('lr_after_hyper_parameter',lr_clf_grid_search_cv)], voting='hard')\n",
    "final_model_after_grid_search_cv.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make the  predictions \n",
    "final_model_after_grid_search_cv_predictions=final_model_after_grid_search_cv.predict(X_test)\n",
    "final_model_after_grid_search_cv_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluvate the model performace after doing the  hypaer parameter tuning and grid search  cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493150684931506"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import  accuracy_score\n",
    "final_model_after_grid_search_cv_accuracy_score=accuracy_score(y_test,final_model_after_grid_search_cv_predictions)\n",
    "final_model_after_grid_search_cv_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1506849315068493"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics  import  mean_squared_error\n",
    "final_model_after_grid_search_cv_mean_sqaured_error=mean_squared_error(y_test,final_model_after_grid_search_cv_predictions)\n",
    "final_model_after_grid_search_cv_mean_sqaured_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.62      0.69       119\n",
      "           1       0.87      0.93      0.90       319\n",
      "\n",
      "    accuracy                           0.85       438\n",
      "   macro avg       0.82      0.78      0.80       438\n",
      "weighted avg       0.84      0.85      0.84       438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics  import  classification_report\n",
    "final_model_after_grid_search_cv_classification_report=classification_report(y_test,final_model_after_grid_search_cv_predictions)\n",
    "print(final_model_after_grid_search_cv_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 74,  45],\n",
       "       [ 21, 298]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "final_model_after_grid_search_cv_confusion_matrix=confusion_matrix(y_test,final_model_after_grid_search_cv_predictions)\n",
    "final_model_after_grid_search_cv_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summary of  models accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After using  voting classifier to  hyper parameter tuned  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493150684931506"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_after_grid_search_cv_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Before  using  the voting classifier and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8447488584474886, 0.7876712328767124, 0.8493150684931506)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_accuracy,dt_clf_accuracy_score,lr_clf_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After applying to the hyper parameter tuning and grid search  cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8561643835616438, 0.8059360730593608, 0.8333333333333334)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv_rf_clf_accuracy_score,dt_clf_grid_search_cv_accuracy_score,lr_clf_grid_search_cv_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When we are considering the above models accuracies,  before and after applying hyper parameter tuning and grid search  cv, random forest classifier model\n",
    "# accuracy is  good for final predictions among other models like logistic regression and decision tree classifier.But this randomforest model accuracy also  not sufficient for making the final predictions. Using the ensemble  technique like voting classifier also we cannot  get the sufficient accuracy for this random forest model and  other model used. So then , As a soultion I moved into the deep learning concept.In this, concept I am going to use \n",
    "# ANN (Artificial Neural Network) architecture to  get better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning (ANN - Artificail Neural Network Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\THIS PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, input_shape=(10,)),  # First Dense Layer\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.01),# LeakyReLU Activation\n",
    "    #tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.8),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.10),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy','mse']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.2397 - loss: 1.6092 - mse: 0.5821\n",
      "Epoch 2/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7597 - loss: 5.2713 - mse: 0.2403\n",
      "Epoch 3/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7597 - loss: 7.1829 - mse: 0.2403\n",
      "Epoch 4/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.7597 - loss: 7.3147 - mse: 0.2403\n",
      "Epoch 5/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.7597 - loss: 6.3883 - mse: 0.2403\n",
      "Epoch 6/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.7597 - loss: 4.8068 - mse: 0.2403\n",
      "Epoch 7/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7597 - loss: 2.7576 - mse: 0.2403\n",
      "Epoch 8/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.7597 - loss: 0.5930 - mse: 0.1946\n",
      "Epoch 9/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.2403 - loss: 5.7555 - mse: 0.7589\n",
      "Epoch 10/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.2403 - loss: 5.2947 - mse: 0.7582\n",
      "Epoch 11/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.2403 - loss: 1.1443 - mse: 0.4496\n",
      "Epoch 12/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.7597 - loss: 1.7354 - mse: 0.2399\n",
      "Epoch 13/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7597 - loss: 3.0971 - mse: 0.2403\n",
      "Epoch 14/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.7597 - loss: 3.9242 - mse: 0.2403\n",
      "Epoch 15/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.7597 - loss: 4.2907 - mse: 0.2403\n",
      "Epoch 16/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.7597 - loss: 4.2603 - mse: 0.2403\n",
      "Epoch 17/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - accuracy: 0.7597 - loss: 3.8858 - mse: 0.2403\n",
      "Epoch 18/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7597 - loss: 3.2121 - mse: 0.2403\n",
      "Epoch 19/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.7597 - loss: 2.2774 - mse: 0.2402\n",
      "Epoch 20/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.7597 - loss: 1.1266 - mse: 0.2354\n",
      "Epoch 21/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.2403 - loss: 1.0969 - mse: 0.4320\n",
      "Epoch 22/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.2403 - loss: 2.4736 - mse: 0.7011\n",
      "Epoch 23/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.2403 - loss: 1.2770 - mse: 0.4953\n",
      "Epoch 24/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7597 - loss: 0.7703 - mse: 0.2191\n",
      "Epoch 25/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7597 - loss: 1.4246 - mse: 0.2388\n",
      "Epoch 26/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.7597 - loss: 1.7773 - mse: 0.2399\n",
      "Epoch 27/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.7597 - loss: 1.8151 - mse: 0.2400\n",
      "Epoch 28/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7597 - loss: 1.5693 - mse: 0.2395\n",
      "Epoch 29/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7597 - loss: 1.0781 - mse: 0.2344\n",
      "Epoch 30/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7597 - loss: 0.5604 - mse: 0.1855\n",
      "Epoch 31/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.2403 - loss: 1.3567 - mse: 0.5196\n",
      "Epoch 32/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.2403 - loss: 1.2318 - mse: 0.4807\n",
      "Epoch 33/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.7597 - loss: 0.5564 - mse: 0.1842\n",
      "Epoch 34/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7597 - loss: 0.9058 - mse: 0.2285\n",
      "Epoch 35/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7597 - loss: 1.1363 - mse: 0.2357\n",
      "Epoch 36/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.7597 - loss: 1.1006 - mse: 0.2350\n",
      "Epoch 37/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7597 - loss: 0.8337 - mse: 0.2243\n",
      "Epoch 38/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.7597 - loss: 0.5506 - mse: 0.1823\n",
      "Epoch 39/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.2403 - loss: 0.9289 - mse: 0.3628\n",
      "Epoch 40/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.2403 - loss: 0.8321 - mse: 0.3182\n",
      "Epoch 41/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.7597 - loss: 0.5555 - mse: 0.1838\n",
      "Epoch 42/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.7597 - loss: 0.7599 - mse: 0.2184\n",
      "Epoch 43/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.7597 - loss: 0.8469 - mse: 0.2252\n",
      "Epoch 44/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.7597 - loss: 0.7259 - mse: 0.2149\n",
      "Epoch 45/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.7597 - loss: 0.5417 - mse: 0.1791\n",
      "Epoch 46/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.4989 - loss: 0.7113 - mse: 0.2590\n",
      "Epoch 47/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.5639 - loss: 0.6946 - mse: 0.2508\n",
      "Epoch 48/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.7597 - loss: 0.5363 - mse: 0.1770\n",
      "Epoch 49/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.7597 - loss: 0.6540 - mse: 0.2051\n",
      "Epoch 50/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7597 - loss: 0.6875 - mse: 0.2100\n",
      "Epoch 51/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.7597 - loss: 0.5825 - mse: 0.1908\n",
      "Epoch 52/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7671 - loss: 0.5471 - mse: 0.1802\n",
      "Epoch 53/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.6376 - loss: 0.6595 - mse: 0.2334\n",
      "Epoch 54/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7700 - loss: 0.5485 - mse: 0.1808\n",
      "Epoch 55/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7597 - loss: 0.5596 - mse: 0.1845\n",
      "Epoch 56/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.7597 - loss: 0.6164 - mse: 0.1979\n",
      "Epoch 57/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7597 - loss: 0.5729 - mse: 0.1880\n",
      "Epoch 58/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.7620 - loss: 0.5284 - mse: 0.1730\n",
      "Epoch 59/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.7449 - loss: 0.5939 - mse: 0.2016\n",
      "Epoch 60/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7745 - loss: 0.5485 - mse: 0.1808\n",
      "Epoch 61/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7597 - loss: 0.5357 - mse: 0.1765\n",
      "Epoch 62/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7597 - loss: 0.5735 - mse: 0.1878\n",
      "Epoch 63/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7597 - loss: 0.5492 - mse: 0.1809\n",
      "Epoch 64/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.7660 - loss: 0.5256 - mse: 0.1718\n",
      "Epoch 65/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.7745 - loss: 0.5617 - mse: 0.1867\n",
      "Epoch 66/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7728 - loss: 0.5330 - mse: 0.1744\n",
      "Epoch 67/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.7608 - loss: 0.5302 - mse: 0.1743\n",
      "Epoch 68/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7597 - loss: 0.5497 - mse: 0.1808\n",
      "Epoch 69/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.7614 - loss: 0.5299 - mse: 0.1741\n",
      "Epoch 70/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7723 - loss: 0.5259 - mse: 0.1716\n",
      "Epoch 71/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.7791 - loss: 0.5417 - mse: 0.1780\n",
      "Epoch 72/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7694 - loss: 0.5212 - mse: 0.1699\n",
      "Epoch 73/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7614 - loss: 0.5282 - mse: 0.1734\n",
      "Epoch 74/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7614 - loss: 0.5327 - mse: 0.1750\n",
      "Epoch 75/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7660 - loss: 0.5185 - mse: 0.1693\n",
      "Epoch 76/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7774 - loss: 0.5260 - mse: 0.1715\n",
      "Epoch 77/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.7780 - loss: 0.5248 - mse: 0.1710\n",
      "Epoch 78/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7683 - loss: 0.5163 - mse: 0.1683\n",
      "Epoch 79/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7637 - loss: 0.5238 - mse: 0.1715\n",
      "Epoch 80/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7660 - loss: 0.5184 - mse: 0.1693\n",
      "Epoch 81/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7751 - loss: 0.5152 - mse: 0.1674\n",
      "Epoch 82/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7803 - loss: 0.5206 - mse: 0.1693\n",
      "Epoch 83/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7751 - loss: 0.5131 - mse: 0.1667\n",
      "Epoch 84/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.7677 - loss: 0.5156 - mse: 0.1681\n",
      "Epoch 85/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7677 - loss: 0.5155 - mse: 0.1681\n",
      "Epoch 86/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7745 - loss: 0.5107 - mse: 0.1658\n",
      "Epoch 87/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7803 - loss: 0.5141 - mse: 0.1667\n",
      "Epoch 88/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7768 - loss: 0.5104 - mse: 0.1654\n",
      "Epoch 89/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7723 - loss: 0.5098 - mse: 0.1657\n",
      "Epoch 90/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.7705 - loss: 0.5109 - mse: 0.1662\n",
      "Epoch 91/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7757 - loss: 0.5072 - mse: 0.1644\n",
      "Epoch 92/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7797 - loss: 0.5087 - mse: 0.1646\n",
      "Epoch 93/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7791 - loss: 0.5069 - mse: 0.1639\n",
      "Epoch 94/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7751 - loss: 0.5056 - mse: 0.1638\n",
      "Epoch 95/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7740 - loss: 0.5063 - mse: 0.1642\n",
      "Epoch 96/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7768 - loss: 0.5037 - mse: 0.1629\n",
      "Epoch 97/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.7820 - loss: 0.5042 - mse: 0.1628\n",
      "Epoch 98/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.7820 - loss: 0.5029 - mse: 0.1623\n",
      "Epoch 99/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.7780 - loss: 0.5017 - mse: 0.1621\n",
      "Epoch 100/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7768 - loss: 0.5018 - mse: 0.1622\n",
      "Epoch 101/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.7791 - loss: 0.4999 - mse: 0.1612\n",
      "Epoch 102/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7871 - loss: 0.4999 - mse: 0.1610\n",
      "Epoch 103/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7860 - loss: 0.4987 - mse: 0.1605\n",
      "Epoch 104/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.7803 - loss: 0.4978 - mse: 0.1604\n",
      "Epoch 105/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.7803 - loss: 0.4974 - mse: 0.1602\n",
      "Epoch 106/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.7854 - loss: 0.4959 - mse: 0.1594\n",
      "Epoch 107/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7894 - loss: 0.4957 - mse: 0.1592\n",
      "Epoch 108/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.7888 - loss: 0.4944 - mse: 0.1587\n",
      "Epoch 109/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7854 - loss: 0.4938 - mse: 0.1586\n",
      "Epoch 110/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.7865 - loss: 0.4929 - mse: 0.1582\n",
      "Epoch 111/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.7917 - loss: 0.4919 - mse: 0.1576\n",
      "Epoch 112/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.7939 - loss: 0.4913 - mse: 0.1573\n",
      "Epoch 113/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.7922 - loss: 0.4901 - mse: 0.1569\n",
      "Epoch 114/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7905 - loss: 0.4896 - mse: 0.1567\n",
      "Epoch 115/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.7934 - loss: 0.4885 - mse: 0.1562\n",
      "Epoch 116/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.7951 - loss: 0.4877 - mse: 0.1558\n",
      "Epoch 117/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.7945 - loss: 0.4867 - mse: 0.1554\n",
      "Epoch 118/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.7939 - loss: 0.4858 - mse: 0.1551\n",
      "Epoch 119/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7945 - loss: 0.4850 - mse: 0.1547\n",
      "Epoch 120/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7945 - loss: 0.4840 - mse: 0.1542\n",
      "Epoch 121/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.7985 - loss: 0.4832 - mse: 0.1539\n",
      "Epoch 122/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.7974 - loss: 0.4822 - mse: 0.1535\n",
      "Epoch 123/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.7951 - loss: 0.4815 - mse: 0.1532\n",
      "Epoch 124/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7974 - loss: 0.4805 - mse: 0.1528\n",
      "Epoch 125/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8008 - loss: 0.4797 - mse: 0.1523\n",
      "Epoch 126/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8008 - loss: 0.4788 - mse: 0.1520\n",
      "Epoch 127/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8002 - loss: 0.4779 - mse: 0.1516\n",
      "Epoch 128/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - accuracy: 0.7997 - loss: 0.4770 - mse: 0.1512\n",
      "Epoch 129/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.8008 - loss: 0.4761 - mse: 0.1508\n",
      "Epoch 130/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8019 - loss: 0.4753 - mse: 0.1504\n",
      "Epoch 131/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.8014 - loss: 0.4744 - mse: 0.1501\n",
      "Epoch 132/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 0.8019 - loss: 0.4735 - mse: 0.1497\n",
      "Epoch 133/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step - accuracy: 0.8042 - loss: 0.4726 - mse: 0.1493\n",
      "Epoch 134/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.8054 - loss: 0.4718 - mse: 0.1489\n",
      "Epoch 135/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8059 - loss: 0.4709 - mse: 0.1485\n",
      "Epoch 136/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459ms/step - accuracy: 0.8048 - loss: 0.4700 - mse: 0.1482\n",
      "Epoch 137/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8054 - loss: 0.4691 - mse: 0.1478\n",
      "Epoch 138/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8076 - loss: 0.4682 - mse: 0.1474\n",
      "Epoch 139/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.8082 - loss: 0.4673 - mse: 0.1471\n",
      "Epoch 140/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.8076 - loss: 0.4664 - mse: 0.1467\n",
      "Epoch 141/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8076 - loss: 0.4655 - mse: 0.1464\n",
      "Epoch 142/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8088 - loss: 0.4645 - mse: 0.1460\n",
      "Epoch 143/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8094 - loss: 0.4635 - mse: 0.1456\n",
      "Epoch 144/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8088 - loss: 0.4626 - mse: 0.1453\n",
      "Epoch 145/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8082 - loss: 0.4617 - mse: 0.1449\n",
      "Epoch 146/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8094 - loss: 0.4609 - mse: 0.1446\n",
      "Epoch 147/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8099 - loss: 0.4601 - mse: 0.1442\n",
      "Epoch 148/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8099 - loss: 0.4592 - mse: 0.1439\n",
      "Epoch 149/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8099 - loss: 0.4583 - mse: 0.1436\n",
      "Epoch 150/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8105 - loss: 0.4574 - mse: 0.1432\n",
      "Epoch 151/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8111 - loss: 0.4565 - mse: 0.1428\n",
      "Epoch 152/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8111 - loss: 0.4556 - mse: 0.1425\n",
      "Epoch 153/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8105 - loss: 0.4547 - mse: 0.1421\n",
      "Epoch 154/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8134 - loss: 0.4539 - mse: 0.1418\n",
      "Epoch 155/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8151 - loss: 0.4530 - mse: 0.1414\n",
      "Epoch 156/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8162 - loss: 0.4522 - mse: 0.1411\n",
      "Epoch 157/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8168 - loss: 0.4513 - mse: 0.1407\n",
      "Epoch 158/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8185 - loss: 0.4505 - mse: 0.1404\n",
      "Epoch 159/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8185 - loss: 0.4497 - mse: 0.1401\n",
      "Epoch 160/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8185 - loss: 0.4488 - mse: 0.1398\n",
      "Epoch 161/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8185 - loss: 0.4480 - mse: 0.1395\n",
      "Epoch 162/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8225 - loss: 0.4472 - mse: 0.1391\n",
      "Epoch 163/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8236 - loss: 0.4464 - mse: 0.1388\n",
      "Epoch 164/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8236 - loss: 0.4456 - mse: 0.1386\n",
      "Epoch 165/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8231 - loss: 0.4448 - mse: 0.1383\n",
      "Epoch 166/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8253 - loss: 0.4441 - mse: 0.1380\n",
      "Epoch 167/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8265 - loss: 0.4433 - mse: 0.1377\n",
      "Epoch 168/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8259 - loss: 0.4426 - mse: 0.1374\n",
      "Epoch 169/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8271 - loss: 0.4418 - mse: 0.1371\n",
      "Epoch 170/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8276 - loss: 0.4411 - mse: 0.1368\n",
      "Epoch 171/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8293 - loss: 0.4403 - mse: 0.1365\n",
      "Epoch 172/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8299 - loss: 0.4396 - mse: 0.1362\n",
      "Epoch 173/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8288 - loss: 0.4389 - mse: 0.1360\n",
      "Epoch 174/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8276 - loss: 0.4383 - mse: 0.1357\n",
      "Epoch 175/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8276 - loss: 0.4376 - mse: 0.1354\n",
      "Epoch 176/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8276 - loss: 0.4369 - mse: 0.1352\n",
      "Epoch 177/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8271 - loss: 0.4362 - mse: 0.1349\n",
      "Epoch 178/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8282 - loss: 0.4356 - mse: 0.1347\n",
      "Epoch 179/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8276 - loss: 0.4350 - mse: 0.1344\n",
      "Epoch 180/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8288 - loss: 0.4343 - mse: 0.1342\n",
      "Epoch 181/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8293 - loss: 0.4337 - mse: 0.1340\n",
      "Epoch 182/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8299 - loss: 0.4331 - mse: 0.1337\n",
      "Epoch 183/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8305 - loss: 0.4326 - mse: 0.1335\n",
      "Epoch 184/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8316 - loss: 0.4320 - mse: 0.1333\n",
      "Epoch 185/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8311 - loss: 0.4314 - mse: 0.1330\n",
      "Epoch 186/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8311 - loss: 0.4308 - mse: 0.1328\n",
      "Epoch 187/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8311 - loss: 0.4303 - mse: 0.1326\n",
      "Epoch 188/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8305 - loss: 0.4297 - mse: 0.1324\n",
      "Epoch 189/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8311 - loss: 0.4292 - mse: 0.1322\n",
      "Epoch 190/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8316 - loss: 0.4286 - mse: 0.1320\n",
      "Epoch 191/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8316 - loss: 0.4280 - mse: 0.1318\n",
      "Epoch 192/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8316 - loss: 0.4275 - mse: 0.1316\n",
      "Epoch 193/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8311 - loss: 0.4271 - mse: 0.1314\n",
      "Epoch 194/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8316 - loss: 0.4266 - mse: 0.1312\n",
      "Epoch 195/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8322 - loss: 0.4261 - mse: 0.1311\n",
      "Epoch 196/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 535ms/step - accuracy: 0.8305 - loss: 0.4256 - mse: 0.1308\n",
      "Epoch 197/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - accuracy: 0.8311 - loss: 0.4251 - mse: 0.1306\n",
      "Epoch 198/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.8311 - loss: 0.4247 - mse: 0.1305\n",
      "Epoch 199/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8311 - loss: 0.4242 - mse: 0.1303\n",
      "Epoch 200/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8316 - loss: 0.4238 - mse: 0.1301\n",
      "Epoch 201/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8322 - loss: 0.4233 - mse: 0.1299\n",
      "Epoch 202/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8328 - loss: 0.4229 - mse: 0.1298\n",
      "Epoch 203/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8328 - loss: 0.4224 - mse: 0.1296\n",
      "Epoch 204/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8339 - loss: 0.4220 - mse: 0.1294\n",
      "Epoch 205/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8345 - loss: 0.4216 - mse: 0.1293\n",
      "Epoch 206/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8356 - loss: 0.4212 - mse: 0.1291\n",
      "Epoch 207/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8356 - loss: 0.4208 - mse: 0.1290\n",
      "Epoch 208/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8379 - loss: 0.4204 - mse: 0.1288\n",
      "Epoch 209/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8385 - loss: 0.4200 - mse: 0.1287\n",
      "Epoch 210/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8390 - loss: 0.4197 - mse: 0.1285\n",
      "Epoch 211/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8390 - loss: 0.4193 - mse: 0.1283\n",
      "Epoch 212/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8385 - loss: 0.4189 - mse: 0.1282\n",
      "Epoch 213/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8390 - loss: 0.4185 - mse: 0.1281\n",
      "Epoch 214/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8396 - loss: 0.4182 - mse: 0.1279\n",
      "Epoch 215/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8390 - loss: 0.4178 - mse: 0.1278\n",
      "Epoch 216/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8390 - loss: 0.4175 - mse: 0.1276\n",
      "Epoch 217/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8402 - loss: 0.4172 - mse: 0.1275\n",
      "Epoch 218/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8385 - loss: 0.4168 - mse: 0.1274\n",
      "Epoch 219/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8390 - loss: 0.4165 - mse: 0.1273\n",
      "Epoch 220/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8390 - loss: 0.4162 - mse: 0.1272\n",
      "Epoch 221/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8396 - loss: 0.4159 - mse: 0.1270\n",
      "Epoch 222/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8390 - loss: 0.4155 - mse: 0.1269\n",
      "Epoch 223/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8396 - loss: 0.4152 - mse: 0.1268\n",
      "Epoch 224/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8396 - loss: 0.4149 - mse: 0.1267\n",
      "Epoch 225/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8402 - loss: 0.4146 - mse: 0.1265\n",
      "Epoch 226/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8408 - loss: 0.4143 - mse: 0.1264\n",
      "Epoch 227/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8408 - loss: 0.4140 - mse: 0.1263\n",
      "Epoch 228/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8408 - loss: 0.4138 - mse: 0.1262\n",
      "Epoch 229/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8408 - loss: 0.4135 - mse: 0.1261\n",
      "Epoch 230/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8408 - loss: 0.4132 - mse: 0.1260\n",
      "Epoch 231/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8408 - loss: 0.4129 - mse: 0.1259\n",
      "Epoch 232/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8413 - loss: 0.4126 - mse: 0.1258\n",
      "Epoch 233/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8413 - loss: 0.4124 - mse: 0.1257\n",
      "Epoch 234/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8413 - loss: 0.4121 - mse: 0.1256\n",
      "Epoch 235/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8425 - loss: 0.4118 - mse: 0.1255\n",
      "Epoch 236/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8419 - loss: 0.4116 - mse: 0.1254\n",
      "Epoch 237/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8425 - loss: 0.4113 - mse: 0.1253\n",
      "Epoch 238/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8430 - loss: 0.4111 - mse: 0.1252\n",
      "Epoch 239/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8425 - loss: 0.4108 - mse: 0.1251\n",
      "Epoch 240/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8425 - loss: 0.4106 - mse: 0.1250\n",
      "Epoch 241/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8425 - loss: 0.4103 - mse: 0.1249\n",
      "Epoch 242/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8425 - loss: 0.4101 - mse: 0.1248\n",
      "Epoch 243/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8425 - loss: 0.4099 - mse: 0.1247\n",
      "Epoch 244/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8430 - loss: 0.4096 - mse: 0.1246\n",
      "Epoch 245/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8430 - loss: 0.4094 - mse: 0.1245\n",
      "Epoch 246/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8425 - loss: 0.4092 - mse: 0.1244\n",
      "Epoch 247/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8430 - loss: 0.4090 - mse: 0.1244\n",
      "Epoch 248/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8430 - loss: 0.4087 - mse: 0.1243\n",
      "Epoch 249/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8425 - loss: 0.4085 - mse: 0.1242\n",
      "Epoch 250/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8425 - loss: 0.4083 - mse: 0.1241\n",
      "Epoch 251/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8430 - loss: 0.4081 - mse: 0.1240\n",
      "Epoch 252/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8442 - loss: 0.4079 - mse: 0.1239\n",
      "Epoch 253/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8436 - loss: 0.4077 - mse: 0.1239\n",
      "Epoch 254/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8430 - loss: 0.4074 - mse: 0.1238\n",
      "Epoch 255/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8442 - loss: 0.4072 - mse: 0.1237\n",
      "Epoch 256/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8430 - loss: 0.4070 - mse: 0.1236\n",
      "Epoch 257/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8430 - loss: 0.4068 - mse: 0.1235\n",
      "Epoch 258/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8430 - loss: 0.4066 - mse: 0.1235\n",
      "Epoch 259/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8430 - loss: 0.4064 - mse: 0.1234\n",
      "Epoch 260/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8436 - loss: 0.4062 - mse: 0.1233\n",
      "Epoch 261/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8442 - loss: 0.4060 - mse: 0.1233\n",
      "Epoch 262/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8442 - loss: 0.4058 - mse: 0.1232\n",
      "Epoch 263/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8442 - loss: 0.4056 - mse: 0.1231\n",
      "Epoch 264/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8436 - loss: 0.4054 - mse: 0.1230\n",
      "Epoch 265/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8436 - loss: 0.4052 - mse: 0.1230\n",
      "Epoch 266/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8442 - loss: 0.4050 - mse: 0.1229\n",
      "Epoch 267/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8442 - loss: 0.4048 - mse: 0.1228\n",
      "Epoch 268/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8442 - loss: 0.4046 - mse: 0.1228\n",
      "Epoch 269/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8442 - loss: 0.4045 - mse: 0.1227\n",
      "Epoch 270/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8442 - loss: 0.4043 - mse: 0.1226\n",
      "Epoch 271/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8442 - loss: 0.4041 - mse: 0.1226\n",
      "Epoch 272/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8442 - loss: 0.4039 - mse: 0.1225\n",
      "Epoch 273/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8436 - loss: 0.4037 - mse: 0.1224\n",
      "Epoch 274/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8447 - loss: 0.4036 - mse: 0.1224\n",
      "Epoch 275/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8447 - loss: 0.4034 - mse: 0.1223\n",
      "Epoch 276/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8447 - loss: 0.4032 - mse: 0.1222\n",
      "Epoch 277/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8459 - loss: 0.4030 - mse: 0.1222\n",
      "Epoch 278/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8453 - loss: 0.4029 - mse: 0.1221\n",
      "Epoch 279/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8459 - loss: 0.4027 - mse: 0.1221\n",
      "Epoch 280/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8470 - loss: 0.4025 - mse: 0.1220\n",
      "Epoch 281/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8465 - loss: 0.4024 - mse: 0.1219\n",
      "Epoch 282/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8459 - loss: 0.4022 - mse: 0.1219\n",
      "Epoch 283/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8470 - loss: 0.4020 - mse: 0.1218\n",
      "Epoch 284/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8470 - loss: 0.4019 - mse: 0.1218\n",
      "Epoch 285/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8470 - loss: 0.4017 - mse: 0.1217\n",
      "Epoch 286/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8470 - loss: 0.4016 - mse: 0.1217\n",
      "Epoch 287/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8465 - loss: 0.4014 - mse: 0.1216\n",
      "Epoch 288/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8470 - loss: 0.4012 - mse: 0.1215\n",
      "Epoch 289/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8476 - loss: 0.4011 - mse: 0.1215\n",
      "Epoch 290/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8465 - loss: 0.4009 - mse: 0.1214\n",
      "Epoch 291/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8459 - loss: 0.4008 - mse: 0.1214\n",
      "Epoch 292/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8470 - loss: 0.4006 - mse: 0.1213\n",
      "Epoch 293/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8465 - loss: 0.4005 - mse: 0.1213\n",
      "Epoch 294/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8465 - loss: 0.4003 - mse: 0.1212\n",
      "Epoch 295/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8470 - loss: 0.4002 - mse: 0.1212\n",
      "Epoch 296/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8470 - loss: 0.4000 - mse: 0.1211\n",
      "Epoch 297/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8465 - loss: 0.3999 - mse: 0.1210\n",
      "Epoch 298/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8470 - loss: 0.3997 - mse: 0.1210\n",
      "Epoch 299/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8470 - loss: 0.3996 - mse: 0.1209\n",
      "Epoch 300/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8476 - loss: 0.3994 - mse: 0.1209\n",
      "Epoch 301/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8476 - loss: 0.3993 - mse: 0.1208\n",
      "Epoch 302/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8470 - loss: 0.3992 - mse: 0.1208\n",
      "Epoch 303/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8476 - loss: 0.3990 - mse: 0.1207\n",
      "Epoch 304/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8476 - loss: 0.3989 - mse: 0.1207\n",
      "Epoch 305/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8476 - loss: 0.3987 - mse: 0.1206\n",
      "Epoch 306/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8482 - loss: 0.3986 - mse: 0.1206\n",
      "Epoch 307/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8482 - loss: 0.3985 - mse: 0.1205\n",
      "Epoch 308/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8482 - loss: 0.3983 - mse: 0.1205\n",
      "Epoch 309/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8482 - loss: 0.3982 - mse: 0.1205\n",
      "Epoch 310/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8482 - loss: 0.3980 - mse: 0.1204\n",
      "Epoch 311/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8482 - loss: 0.3979 - mse: 0.1204\n",
      "Epoch 312/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8476 - loss: 0.3978 - mse: 0.1203\n",
      "Epoch 313/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8476 - loss: 0.3976 - mse: 0.1203\n",
      "Epoch 314/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8482 - loss: 0.3975 - mse: 0.1202\n",
      "Epoch 315/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8476 - loss: 0.3974 - mse: 0.1202\n",
      "Epoch 316/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8476 - loss: 0.3973 - mse: 0.1201\n",
      "Epoch 317/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8476 - loss: 0.3971 - mse: 0.1201\n",
      "Epoch 318/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8482 - loss: 0.3970 - mse: 0.1200\n",
      "Epoch 319/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8482 - loss: 0.3969 - mse: 0.1200\n",
      "Epoch 320/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8476 - loss: 0.3967 - mse: 0.1200\n",
      "Epoch 321/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8482 - loss: 0.3966 - mse: 0.1199\n",
      "Epoch 322/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8482 - loss: 0.3965 - mse: 0.1199\n",
      "Epoch 323/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8482 - loss: 0.3964 - mse: 0.1198\n",
      "Epoch 324/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8482 - loss: 0.3962 - mse: 0.1198\n",
      "Epoch 325/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8482 - loss: 0.3961 - mse: 0.1197\n",
      "Epoch 326/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8482 - loss: 0.3960 - mse: 0.1197\n",
      "Epoch 327/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8482 - loss: 0.3959 - mse: 0.1197\n",
      "Epoch 328/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8482 - loss: 0.3958 - mse: 0.1196\n",
      "Epoch 329/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8482 - loss: 0.3956 - mse: 0.1196\n",
      "Epoch 330/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8482 - loss: 0.3955 - mse: 0.1195\n",
      "Epoch 331/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8482 - loss: 0.3954 - mse: 0.1195\n",
      "Epoch 332/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8476 - loss: 0.3953 - mse: 0.1194\n",
      "Epoch 333/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8482 - loss: 0.3952 - mse: 0.1194\n",
      "Epoch 334/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8476 - loss: 0.3950 - mse: 0.1194\n",
      "Epoch 335/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8476 - loss: 0.3949 - mse: 0.1193\n",
      "Epoch 336/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8476 - loss: 0.3948 - mse: 0.1193\n",
      "Epoch 337/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8482 - loss: 0.3947 - mse: 0.1192\n",
      "Epoch 338/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8487 - loss: 0.3946 - mse: 0.1192\n",
      "Epoch 339/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8493 - loss: 0.3945 - mse: 0.1192\n",
      "Epoch 340/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8482 - loss: 0.3943 - mse: 0.1191\n",
      "Epoch 341/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8482 - loss: 0.3942 - mse: 0.1191\n",
      "Epoch 342/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8487 - loss: 0.3941 - mse: 0.1190\n",
      "Epoch 343/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8487 - loss: 0.3940 - mse: 0.1190\n",
      "Epoch 344/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8487 - loss: 0.3939 - mse: 0.1189\n",
      "Epoch 345/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8487 - loss: 0.3937 - mse: 0.1189\n",
      "Epoch 346/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8487 - loss: 0.3936 - mse: 0.1189\n",
      "Epoch 347/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8487 - loss: 0.3935 - mse: 0.1188\n",
      "Epoch 348/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8487 - loss: 0.3934 - mse: 0.1188\n",
      "Epoch 349/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8487 - loss: 0.3933 - mse: 0.1188\n",
      "Epoch 350/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8487 - loss: 0.3931 - mse: 0.1187\n",
      "Epoch 351/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8487 - loss: 0.3930 - mse: 0.1187\n",
      "Epoch 352/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8487 - loss: 0.3929 - mse: 0.1186\n",
      "Epoch 353/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8487 - loss: 0.3928 - mse: 0.1186\n",
      "Epoch 354/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8487 - loss: 0.3927 - mse: 0.1186\n",
      "Epoch 355/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8493 - loss: 0.3926 - mse: 0.1185\n",
      "Epoch 356/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8487 - loss: 0.3924 - mse: 0.1185\n",
      "Epoch 357/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8493 - loss: 0.3923 - mse: 0.1184\n",
      "Epoch 358/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8493 - loss: 0.3922 - mse: 0.1184\n",
      "Epoch 359/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8493 - loss: 0.3921 - mse: 0.1183\n",
      "Epoch 360/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8493 - loss: 0.3919 - mse: 0.1183\n",
      "Epoch 361/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8493 - loss: 0.3918 - mse: 0.1183\n",
      "Epoch 362/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8493 - loss: 0.3916 - mse: 0.1182\n",
      "Epoch 363/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8493 - loss: 0.3915 - mse: 0.1182\n",
      "Epoch 364/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8487 - loss: 0.3912 - mse: 0.1181\n",
      "Epoch 365/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8493 - loss: 0.3910 - mse: 0.1180\n",
      "Epoch 366/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8493 - loss: 0.3909 - mse: 0.1180\n",
      "Epoch 367/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8493 - loss: 0.3907 - mse: 0.1179\n",
      "Epoch 368/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8493 - loss: 0.3906 - mse: 0.1179\n",
      "Epoch 369/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8493 - loss: 0.3905 - mse: 0.1179\n",
      "Epoch 370/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8493 - loss: 0.3904 - mse: 0.1178\n",
      "Epoch 371/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8493 - loss: 0.3902 - mse: 0.1178\n",
      "Epoch 372/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8493 - loss: 0.3902 - mse: 0.1178\n",
      "Epoch 373/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8499 - loss: 0.3900 - mse: 0.1177\n",
      "Epoch 374/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8493 - loss: 0.3899 - mse: 0.1177\n",
      "Epoch 375/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8493 - loss: 0.3898 - mse: 0.1177\n",
      "Epoch 376/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8499 - loss: 0.3897 - mse: 0.1176\n",
      "Epoch 377/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8493 - loss: 0.3895 - mse: 0.1176\n",
      "Epoch 378/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8493 - loss: 0.3894 - mse: 0.1176\n",
      "Epoch 379/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8499 - loss: 0.3893 - mse: 0.1175\n",
      "Epoch 380/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8499 - loss: 0.3892 - mse: 0.1175\n",
      "Epoch 381/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8499 - loss: 0.3890 - mse: 0.1174\n",
      "Epoch 382/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.3889 - mse: 0.1174\n",
      "Epoch 383/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8499 - loss: 0.3888 - mse: 0.1173\n",
      "Epoch 384/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8499 - loss: 0.3887 - mse: 0.1173\n",
      "Epoch 385/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8499 - loss: 0.3886 - mse: 0.1173\n",
      "Epoch 386/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8493 - loss: 0.3885 - mse: 0.1172\n",
      "Epoch 387/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8499 - loss: 0.3884 - mse: 0.1172\n",
      "Epoch 388/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8499 - loss: 0.3883 - mse: 0.1172\n",
      "Epoch 389/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8487 - loss: 0.3881 - mse: 0.1171\n",
      "Epoch 390/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8493 - loss: 0.3880 - mse: 0.1171\n",
      "Epoch 391/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8493 - loss: 0.3879 - mse: 0.1170\n",
      "Epoch 392/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8487 - loss: 0.3878 - mse: 0.1170\n",
      "Epoch 393/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8493 - loss: 0.3876 - mse: 0.1170\n",
      "Epoch 394/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8493 - loss: 0.3875 - mse: 0.1169\n",
      "Epoch 395/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8493 - loss: 0.3874 - mse: 0.1169\n",
      "Epoch 396/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8493 - loss: 0.3873 - mse: 0.1169\n",
      "Epoch 397/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8493 - loss: 0.3872 - mse: 0.1168\n",
      "Epoch 398/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8487 - loss: 0.3871 - mse: 0.1168\n",
      "Epoch 399/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8493 - loss: 0.3869 - mse: 0.1168\n",
      "Epoch 400/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8493 - loss: 0.3868 - mse: 0.1167\n",
      "Epoch 401/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8493 - loss: 0.3867 - mse: 0.1167\n",
      "Epoch 402/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8493 - loss: 0.3866 - mse: 0.1167\n",
      "Epoch 403/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8493 - loss: 0.3865 - mse: 0.1166\n",
      "Epoch 404/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8493 - loss: 0.3864 - mse: 0.1166\n",
      "Epoch 405/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8493 - loss: 0.3863 - mse: 0.1166\n",
      "Epoch 406/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8493 - loss: 0.3861 - mse: 0.1165\n",
      "Epoch 407/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8493 - loss: 0.3860 - mse: 0.1165\n",
      "Epoch 408/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8493 - loss: 0.3859 - mse: 0.1165\n",
      "Epoch 409/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8493 - loss: 0.3858 - mse: 0.1164\n",
      "Epoch 410/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8493 - loss: 0.3857 - mse: 0.1164\n",
      "Epoch 411/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8493 - loss: 0.3856 - mse: 0.1163\n",
      "Epoch 412/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8493 - loss: 0.3855 - mse: 0.1163\n",
      "Epoch 413/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8487 - loss: 0.3853 - mse: 0.1162\n",
      "Epoch 414/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8487 - loss: 0.3852 - mse: 0.1162\n",
      "Epoch 415/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.3851 - mse: 0.1162\n",
      "Epoch 416/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8493 - loss: 0.3850 - mse: 0.1161\n",
      "Epoch 417/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8493 - loss: 0.3849 - mse: 0.1161\n",
      "Epoch 418/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8499 - loss: 0.3848 - mse: 0.1161\n",
      "Epoch 419/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8493 - loss: 0.3847 - mse: 0.1160\n",
      "Epoch 420/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8493 - loss: 0.3846 - mse: 0.1160\n",
      "Epoch 421/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8499 - loss: 0.3845 - mse: 0.1160\n",
      "Epoch 422/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.3843 - mse: 0.1159\n",
      "Epoch 423/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8493 - loss: 0.3842 - mse: 0.1159\n",
      "Epoch 424/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8493 - loss: 0.3841 - mse: 0.1159\n",
      "Epoch 425/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8493 - loss: 0.3840 - mse: 0.1158\n",
      "Epoch 426/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8487 - loss: 0.3839 - mse: 0.1158\n",
      "Epoch 427/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8493 - loss: 0.3838 - mse: 0.1158\n",
      "Epoch 428/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8493 - loss: 0.3837 - mse: 0.1157\n",
      "Epoch 429/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8493 - loss: 0.3835 - mse: 0.1157\n",
      "Epoch 430/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8499 - loss: 0.3834 - mse: 0.1156\n",
      "Epoch 431/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8499 - loss: 0.3833 - mse: 0.1156\n",
      "Epoch 432/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8499 - loss: 0.3832 - mse: 0.1156\n",
      "Epoch 433/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8499 - loss: 0.3831 - mse: 0.1155\n",
      "Epoch 434/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8499 - loss: 0.3830 - mse: 0.1155\n",
      "Epoch 435/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8499 - loss: 0.3828 - mse: 0.1155\n",
      "Epoch 436/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8499 - loss: 0.3827 - mse: 0.1154\n",
      "Epoch 437/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8499 - loss: 0.3826 - mse: 0.1154\n",
      "Epoch 438/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8499 - loss: 0.3825 - mse: 0.1154\n",
      "Epoch 439/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8499 - loss: 0.3824 - mse: 0.1153\n",
      "Epoch 440/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8499 - loss: 0.3822 - mse: 0.1153\n",
      "Epoch 441/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8499 - loss: 0.3821 - mse: 0.1153\n",
      "Epoch 442/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8505 - loss: 0.3820 - mse: 0.1152\n",
      "Epoch 443/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8505 - loss: 0.3819 - mse: 0.1152\n",
      "Epoch 444/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8505 - loss: 0.3818 - mse: 0.1152\n",
      "Epoch 445/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8505 - loss: 0.3817 - mse: 0.1151\n",
      "Epoch 446/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8505 - loss: 0.3816 - mse: 0.1151\n",
      "Epoch 447/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8505 - loss: 0.3815 - mse: 0.1151\n",
      "Epoch 448/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8505 - loss: 0.3813 - mse: 0.1150\n",
      "Epoch 449/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8505 - loss: 0.3812 - mse: 0.1150\n",
      "Epoch 450/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8505 - loss: 0.3811 - mse: 0.1150\n",
      "Epoch 451/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8505 - loss: 0.3810 - mse: 0.1149\n",
      "Epoch 452/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8505 - loss: 0.3809 - mse: 0.1149\n",
      "Epoch 453/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8505 - loss: 0.3808 - mse: 0.1149\n",
      "Epoch 454/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8505 - loss: 0.3807 - mse: 0.1148\n",
      "Epoch 455/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8505 - loss: 0.3805 - mse: 0.1148\n",
      "Epoch 456/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8505 - loss: 0.3804 - mse: 0.1147\n",
      "Epoch 457/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8505 - loss: 0.3803 - mse: 0.1147\n",
      "Epoch 458/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8505 - loss: 0.3802 - mse: 0.1147\n",
      "Epoch 459/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8499 - loss: 0.3801 - mse: 0.1146\n",
      "Epoch 460/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8499 - loss: 0.3800 - mse: 0.1146\n",
      "Epoch 461/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.3798 - mse: 0.1146\n",
      "Epoch 462/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8499 - loss: 0.3797 - mse: 0.1145\n",
      "Epoch 463/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8499 - loss: 0.3796 - mse: 0.1145\n",
      "Epoch 464/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8499 - loss: 0.3795 - mse: 0.1145\n",
      "Epoch 465/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.3794 - mse: 0.1144\n",
      "Epoch 466/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8499 - loss: 0.3793 - mse: 0.1144\n",
      "Epoch 467/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8499 - loss: 0.3792 - mse: 0.1144\n",
      "Epoch 468/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8493 - loss: 0.3791 - mse: 0.1143\n",
      "Epoch 469/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8493 - loss: 0.3790 - mse: 0.1143\n",
      "Epoch 470/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8487 - loss: 0.3788 - mse: 0.1143\n",
      "Epoch 471/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8487 - loss: 0.3787 - mse: 0.1142\n",
      "Epoch 472/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8493 - loss: 0.3786 - mse: 0.1142\n",
      "Epoch 473/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8487 - loss: 0.3785 - mse: 0.1142\n",
      "Epoch 474/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8493 - loss: 0.3784 - mse: 0.1141\n",
      "Epoch 475/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8493 - loss: 0.3783 - mse: 0.1141\n",
      "Epoch 476/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8493 - loss: 0.3782 - mse: 0.1141\n",
      "Epoch 477/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8493 - loss: 0.3781 - mse: 0.1140\n",
      "Epoch 478/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8493 - loss: 0.3780 - mse: 0.1140\n",
      "Epoch 479/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8493 - loss: 0.3779 - mse: 0.1140\n",
      "Epoch 480/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8493 - loss: 0.3778 - mse: 0.1139\n",
      "Epoch 481/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8493 - loss: 0.3777 - mse: 0.1139\n",
      "Epoch 482/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8493 - loss: 0.3776 - mse: 0.1139\n",
      "Epoch 483/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8493 - loss: 0.3775 - mse: 0.1139\n",
      "Epoch 484/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8493 - loss: 0.3774 - mse: 0.1138\n",
      "Epoch 485/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8493 - loss: 0.3773 - mse: 0.1138\n",
      "Epoch 486/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8493 - loss: 0.3772 - mse: 0.1138\n",
      "Epoch 487/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8493 - loss: 0.3771 - mse: 0.1137\n",
      "Epoch 488/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8493 - loss: 0.3770 - mse: 0.1137\n",
      "Epoch 489/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8493 - loss: 0.3769 - mse: 0.1137\n",
      "Epoch 490/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8493 - loss: 0.3768 - mse: 0.1136\n",
      "Epoch 491/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8493 - loss: 0.3767 - mse: 0.1136\n",
      "Epoch 492/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8493 - loss: 0.3766 - mse: 0.1136\n",
      "Epoch 493/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8499 - loss: 0.3765 - mse: 0.1136\n",
      "Epoch 494/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8499 - loss: 0.3764 - mse: 0.1135\n",
      "Epoch 495/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8499 - loss: 0.3763 - mse: 0.1135\n",
      "Epoch 496/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8499 - loss: 0.3762 - mse: 0.1135\n",
      "Epoch 497/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8499 - loss: 0.3761 - mse: 0.1134\n",
      "Epoch 498/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8499 - loss: 0.3760 - mse: 0.1134\n",
      "Epoch 499/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8499 - loss: 0.3759 - mse: 0.1134\n",
      "Epoch 500/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.3758 - mse: 0.1133\n",
      "Epoch 501/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8499 - loss: 0.3757 - mse: 0.1133\n",
      "Epoch 502/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8499 - loss: 0.3756 - mse: 0.1133\n",
      "Epoch 503/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8499 - loss: 0.3755 - mse: 0.1133\n",
      "Epoch 504/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8499 - loss: 0.3754 - mse: 0.1132\n",
      "Epoch 505/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8499 - loss: 0.3753 - mse: 0.1132\n",
      "Epoch 506/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8499 - loss: 0.3752 - mse: 0.1132\n",
      "Epoch 507/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8499 - loss: 0.3751 - mse: 0.1131\n",
      "Epoch 508/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8499 - loss: 0.3751 - mse: 0.1131\n",
      "Epoch 509/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8499 - loss: 0.3750 - mse: 0.1131\n",
      "Epoch 510/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8499 - loss: 0.3749 - mse: 0.1131\n",
      "Epoch 511/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8499 - loss: 0.3748 - mse: 0.1130\n",
      "Epoch 512/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8499 - loss: 0.3747 - mse: 0.1130\n",
      "Epoch 513/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8499 - loss: 0.3746 - mse: 0.1130\n",
      "Epoch 514/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.8499 - loss: 0.3745 - mse: 0.1129\n",
      "Epoch 515/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8499 - loss: 0.3744 - mse: 0.1129\n",
      "Epoch 516/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8499 - loss: 0.3743 - mse: 0.1129\n",
      "Epoch 517/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8499 - loss: 0.3742 - mse: 0.1129\n",
      "Epoch 518/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8499 - loss: 0.3741 - mse: 0.1128\n",
      "Epoch 519/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8499 - loss: 0.3741 - mse: 0.1128\n",
      "Epoch 520/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8499 - loss: 0.3740 - mse: 0.1128\n",
      "Epoch 521/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8499 - loss: 0.3739 - mse: 0.1128\n",
      "Epoch 522/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8499 - loss: 0.3738 - mse: 0.1127\n",
      "Epoch 523/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8499 - loss: 0.3737 - mse: 0.1127\n",
      "Epoch 524/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8499 - loss: 0.3736 - mse: 0.1127\n",
      "Epoch 525/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.8499 - loss: 0.3735 - mse: 0.1127\n",
      "Epoch 526/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8499 - loss: 0.3734 - mse: 0.1126\n",
      "Epoch 527/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8499 - loss: 0.3734 - mse: 0.1126\n",
      "Epoch 528/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8499 - loss: 0.3733 - mse: 0.1126\n",
      "Epoch 529/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8505 - loss: 0.3732 - mse: 0.1125\n",
      "Epoch 530/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8505 - loss: 0.3731 - mse: 0.1125\n",
      "Epoch 531/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8505 - loss: 0.3730 - mse: 0.1125\n",
      "Epoch 532/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8505 - loss: 0.3729 - mse: 0.1125\n",
      "Epoch 533/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8505 - loss: 0.3728 - mse: 0.1124\n",
      "Epoch 534/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8505 - loss: 0.3728 - mse: 0.1124\n",
      "Epoch 535/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8505 - loss: 0.3727 - mse: 0.1124\n",
      "Epoch 536/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.8505 - loss: 0.3726 - mse: 0.1124\n",
      "Epoch 537/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8510 - loss: 0.3725 - mse: 0.1123\n",
      "Epoch 538/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8510 - loss: 0.3724 - mse: 0.1123\n",
      "Epoch 539/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8510 - loss: 0.3723 - mse: 0.1123\n",
      "Epoch 540/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8510 - loss: 0.3723 - mse: 0.1123\n",
      "Epoch 541/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8510 - loss: 0.3722 - mse: 0.1122\n",
      "Epoch 542/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8516 - loss: 0.3721 - mse: 0.1122\n",
      "Epoch 543/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8516 - loss: 0.3720 - mse: 0.1122\n",
      "Epoch 544/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8516 - loss: 0.3719 - mse: 0.1121\n",
      "Epoch 545/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8516 - loss: 0.3718 - mse: 0.1121\n",
      "Epoch 546/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8516 - loss: 0.3716 - mse: 0.1121\n",
      "Epoch 547/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8516 - loss: 0.3715 - mse: 0.1120\n",
      "Epoch 548/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8510 - loss: 0.3714 - mse: 0.1120\n",
      "Epoch 549/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8510 - loss: 0.3713 - mse: 0.1119\n",
      "Epoch 550/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8516 - loss: 0.3711 - mse: 0.1119\n",
      "Epoch 551/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8522 - loss: 0.3710 - mse: 0.1118\n",
      "Epoch 552/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8522 - loss: 0.3708 - mse: 0.1118\n",
      "Epoch 553/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8522 - loss: 0.3707 - mse: 0.1118\n",
      "Epoch 554/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8522 - loss: 0.3706 - mse: 0.1117\n",
      "Epoch 555/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8522 - loss: 0.3705 - mse: 0.1117\n",
      "Epoch 556/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8516 - loss: 0.3704 - mse: 0.1116\n",
      "Epoch 557/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8522 - loss: 0.3703 - mse: 0.1116\n",
      "Epoch 558/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8522 - loss: 0.3701 - mse: 0.1116\n",
      "Epoch 559/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8522 - loss: 0.3700 - mse: 0.1116\n",
      "Epoch 560/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8522 - loss: 0.3699 - mse: 0.1115\n",
      "Epoch 561/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8522 - loss: 0.3698 - mse: 0.1115\n",
      "Epoch 562/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8516 - loss: 0.3696 - mse: 0.1115\n",
      "Epoch 563/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8527 - loss: 0.3695 - mse: 0.1114\n",
      "Epoch 564/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8516 - loss: 0.3694 - mse: 0.1114\n",
      "Epoch 565/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8516 - loss: 0.3693 - mse: 0.1114\n",
      "Epoch 566/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8516 - loss: 0.3692 - mse: 0.1113\n",
      "Epoch 567/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8516 - loss: 0.3691 - mse: 0.1113\n",
      "Epoch 568/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8516 - loss: 0.3690 - mse: 0.1113\n",
      "Epoch 569/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8516 - loss: 0.3689 - mse: 0.1113\n",
      "Epoch 570/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8516 - loss: 0.3688 - mse: 0.1112\n",
      "Epoch 571/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8516 - loss: 0.3687 - mse: 0.1112\n",
      "Epoch 572/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8516 - loss: 0.3686 - mse: 0.1112\n",
      "Epoch 573/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8516 - loss: 0.3685 - mse: 0.1112\n",
      "Epoch 574/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8516 - loss: 0.3685 - mse: 0.1111\n",
      "Epoch 575/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8516 - loss: 0.3684 - mse: 0.1111\n",
      "Epoch 576/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8516 - loss: 0.3683 - mse: 0.1111\n",
      "Epoch 577/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8516 - loss: 0.3682 - mse: 0.1111\n",
      "Epoch 578/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8516 - loss: 0.3681 - mse: 0.1110\n",
      "Epoch 579/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8516 - loss: 0.3680 - mse: 0.1110\n",
      "Epoch 580/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8516 - loss: 0.3679 - mse: 0.1110\n",
      "Epoch 581/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8516 - loss: 0.3678 - mse: 0.1109\n",
      "Epoch 582/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8516 - loss: 0.3677 - mse: 0.1109\n",
      "Epoch 583/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8522 - loss: 0.3676 - mse: 0.1109\n",
      "Epoch 584/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8527 - loss: 0.3675 - mse: 0.1108\n",
      "Epoch 585/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8522 - loss: 0.3674 - mse: 0.1108\n",
      "Epoch 586/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8522 - loss: 0.3673 - mse: 0.1108\n",
      "Epoch 587/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8522 - loss: 0.3672 - mse: 0.1108\n",
      "Epoch 588/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8522 - loss: 0.3671 - mse: 0.1107\n",
      "Epoch 589/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8522 - loss: 0.3670 - mse: 0.1107\n",
      "Epoch 590/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8527 - loss: 0.3669 - mse: 0.1107\n",
      "Epoch 591/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8527 - loss: 0.3668 - mse: 0.1106\n",
      "Epoch 592/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8527 - loss: 0.3667 - mse: 0.1106\n",
      "Epoch 593/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8527 - loss: 0.3667 - mse: 0.1106\n",
      "Epoch 594/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8527 - loss: 0.3666 - mse: 0.1106\n",
      "Epoch 595/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8527 - loss: 0.3665 - mse: 0.1105\n",
      "Epoch 596/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8527 - loss: 0.3664 - mse: 0.1105\n",
      "Epoch 597/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8527 - loss: 0.3663 - mse: 0.1105\n",
      "Epoch 598/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8527 - loss: 0.3662 - mse: 0.1105\n",
      "Epoch 599/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8527 - loss: 0.3661 - mse: 0.1104\n",
      "Epoch 600/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.8527 - loss: 0.3660 - mse: 0.1104\n",
      "Epoch 601/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8527 - loss: 0.3659 - mse: 0.1104\n",
      "Epoch 602/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8527 - loss: 0.3659 - mse: 0.1104\n",
      "Epoch 603/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8527 - loss: 0.3658 - mse: 0.1103\n",
      "Epoch 604/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8527 - loss: 0.3657 - mse: 0.1103\n",
      "Epoch 605/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8527 - loss: 0.3656 - mse: 0.1103\n",
      "Epoch 606/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8527 - loss: 0.3655 - mse: 0.1103\n",
      "Epoch 607/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8527 - loss: 0.3654 - mse: 0.1102\n",
      "Epoch 608/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8527 - loss: 0.3653 - mse: 0.1102\n",
      "Epoch 609/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8527 - loss: 0.3652 - mse: 0.1102\n",
      "Epoch 610/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8527 - loss: 0.3652 - mse: 0.1102\n",
      "Epoch 611/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8527 - loss: 0.3651 - mse: 0.1101\n",
      "Epoch 612/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8527 - loss: 0.3650 - mse: 0.1101\n",
      "Epoch 613/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8527 - loss: 0.3649 - mse: 0.1101\n",
      "Epoch 614/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8527 - loss: 0.3648 - mse: 0.1101\n",
      "Epoch 615/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8527 - loss: 0.3647 - mse: 0.1100\n",
      "Epoch 616/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8527 - loss: 0.3647 - mse: 0.1100\n",
      "Epoch 617/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8527 - loss: 0.3646 - mse: 0.1100\n",
      "Epoch 618/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8527 - loss: 0.3645 - mse: 0.1100\n",
      "Epoch 619/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8527 - loss: 0.3644 - mse: 0.1099\n",
      "Epoch 620/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8527 - loss: 0.3643 - mse: 0.1099\n",
      "Epoch 621/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8533 - loss: 0.3642 - mse: 0.1099\n",
      "Epoch 622/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8533 - loss: 0.3642 - mse: 0.1099\n",
      "Epoch 623/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8533 - loss: 0.3641 - mse: 0.1098\n",
      "Epoch 624/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8533 - loss: 0.3640 - mse: 0.1098\n",
      "Epoch 625/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8533 - loss: 0.3639 - mse: 0.1098\n",
      "Epoch 626/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8533 - loss: 0.3638 - mse: 0.1098\n",
      "Epoch 627/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8533 - loss: 0.3638 - mse: 0.1097\n",
      "Epoch 628/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8533 - loss: 0.3637 - mse: 0.1097\n",
      "Epoch 629/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.8533 - loss: 0.3636 - mse: 0.1097\n",
      "Epoch 630/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8533 - loss: 0.3635 - mse: 0.1097\n",
      "Epoch 631/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8533 - loss: 0.3634 - mse: 0.1097\n",
      "Epoch 632/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8533 - loss: 0.3634 - mse: 0.1096\n",
      "Epoch 633/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8533 - loss: 0.3633 - mse: 0.1096\n",
      "Epoch 634/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8533 - loss: 0.3632 - mse: 0.1096\n",
      "Epoch 635/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8533 - loss: 0.3631 - mse: 0.1096\n",
      "Epoch 636/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8533 - loss: 0.3630 - mse: 0.1096\n",
      "Epoch 637/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8533 - loss: 0.3630 - mse: 0.1095\n",
      "Epoch 638/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8533 - loss: 0.3629 - mse: 0.1095\n",
      "Epoch 639/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8533 - loss: 0.3628 - mse: 0.1095\n",
      "Epoch 640/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8533 - loss: 0.3627 - mse: 0.1094\n",
      "Epoch 641/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8533 - loss: 0.3627 - mse: 0.1094\n",
      "Epoch 642/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8533 - loss: 0.3626 - mse: 0.1094\n",
      "Epoch 643/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8533 - loss: 0.3625 - mse: 0.1094\n",
      "Epoch 644/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8533 - loss: 0.3624 - mse: 0.1094\n",
      "Epoch 645/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8533 - loss: 0.3624 - mse: 0.1093\n",
      "Epoch 646/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8533 - loss: 0.3623 - mse: 0.1093\n",
      "Epoch 647/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8527 - loss: 0.3622 - mse: 0.1093\n",
      "Epoch 648/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8527 - loss: 0.3621 - mse: 0.1093\n",
      "Epoch 649/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8527 - loss: 0.3620 - mse: 0.1092\n",
      "Epoch 650/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8527 - loss: 0.3620 - mse: 0.1092\n",
      "Epoch 651/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8527 - loss: 0.3619 - mse: 0.1092\n",
      "Epoch 652/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8527 - loss: 0.3618 - mse: 0.1092\n",
      "Epoch 653/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8527 - loss: 0.3618 - mse: 0.1092\n",
      "Epoch 654/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8527 - loss: 0.3617 - mse: 0.1091\n",
      "Epoch 655/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8527 - loss: 0.3616 - mse: 0.1091\n",
      "Epoch 656/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 435ms/step - accuracy: 0.8527 - loss: 0.3615 - mse: 0.1091\n",
      "Epoch 657/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8527 - loss: 0.3615 - mse: 0.1091\n",
      "Epoch 658/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.8527 - loss: 0.3614 - mse: 0.1091\n",
      "Epoch 659/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8527 - loss: 0.3613 - mse: 0.1090\n",
      "Epoch 660/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8527 - loss: 0.3612 - mse: 0.1090\n",
      "Epoch 661/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8527 - loss: 0.3612 - mse: 0.1090\n",
      "Epoch 662/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8527 - loss: 0.3611 - mse: 0.1090\n",
      "Epoch 663/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8527 - loss: 0.3610 - mse: 0.1089\n",
      "Epoch 664/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.8527 - loss: 0.3610 - mse: 0.1089\n",
      "Epoch 665/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8527 - loss: 0.3609 - mse: 0.1089\n",
      "Epoch 666/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8527 - loss: 0.3608 - mse: 0.1089\n",
      "Epoch 667/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8527 - loss: 0.3607 - mse: 0.1089\n",
      "Epoch 668/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8527 - loss: 0.3607 - mse: 0.1088\n",
      "Epoch 669/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8527 - loss: 0.3606 - mse: 0.1088\n",
      "Epoch 670/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8527 - loss: 0.3605 - mse: 0.1088\n",
      "Epoch 671/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8527 - loss: 0.3605 - mse: 0.1088\n",
      "Epoch 672/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8527 - loss: 0.3604 - mse: 0.1088\n",
      "Epoch 673/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8527 - loss: 0.3603 - mse: 0.1087\n",
      "Epoch 674/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8527 - loss: 0.3603 - mse: 0.1087\n",
      "Epoch 675/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8527 - loss: 0.3602 - mse: 0.1087\n",
      "Epoch 676/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8527 - loss: 0.3601 - mse: 0.1087\n",
      "Epoch 677/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8527 - loss: 0.3601 - mse: 0.1087\n",
      "Epoch 678/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8527 - loss: 0.3600 - mse: 0.1086\n",
      "Epoch 679/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8527 - loss: 0.3599 - mse: 0.1086\n",
      "Epoch 680/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8527 - loss: 0.3599 - mse: 0.1086\n",
      "Epoch 681/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8527 - loss: 0.3598 - mse: 0.1086\n",
      "Epoch 682/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8527 - loss: 0.3597 - mse: 0.1085\n",
      "Epoch 683/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8527 - loss: 0.3596 - mse: 0.1085\n",
      "Epoch 684/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8527 - loss: 0.3596 - mse: 0.1085\n",
      "Epoch 685/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8527 - loss: 0.3595 - mse: 0.1085\n",
      "Epoch 686/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.8527 - loss: 0.3595 - mse: 0.1085\n",
      "Epoch 687/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8533 - loss: 0.3594 - mse: 0.1084\n",
      "Epoch 688/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8533 - loss: 0.3593 - mse: 0.1084\n",
      "Epoch 689/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8533 - loss: 0.3593 - mse: 0.1084\n",
      "Epoch 690/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8533 - loss: 0.3592 - mse: 0.1084\n",
      "Epoch 691/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8539 - loss: 0.3591 - mse: 0.1084\n",
      "Epoch 692/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8539 - loss: 0.3591 - mse: 0.1083\n",
      "Epoch 693/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8539 - loss: 0.3590 - mse: 0.1083\n",
      "Epoch 694/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8539 - loss: 0.3589 - mse: 0.1083\n",
      "Epoch 695/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8539 - loss: 0.3589 - mse: 0.1083\n",
      "Epoch 696/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8539 - loss: 0.3588 - mse: 0.1083\n",
      "Epoch 697/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8539 - loss: 0.3587 - mse: 0.1083\n",
      "Epoch 698/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8539 - loss: 0.3587 - mse: 0.1082\n",
      "Epoch 699/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8539 - loss: 0.3586 - mse: 0.1082\n",
      "Epoch 700/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8539 - loss: 0.3585 - mse: 0.1082\n",
      "Epoch 701/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8545 - loss: 0.3585 - mse: 0.1082\n",
      "Epoch 702/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8545 - loss: 0.3584 - mse: 0.1082\n",
      "Epoch 703/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8545 - loss: 0.3584 - mse: 0.1081\n",
      "Epoch 704/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8545 - loss: 0.3583 - mse: 0.1081\n",
      "Epoch 705/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8545 - loss: 0.3582 - mse: 0.1081\n",
      "Epoch 706/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8545 - loss: 0.3582 - mse: 0.1081\n",
      "Epoch 707/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8545 - loss: 0.3581 - mse: 0.1081\n",
      "Epoch 708/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8545 - loss: 0.3580 - mse: 0.1080\n",
      "Epoch 709/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8545 - loss: 0.3580 - mse: 0.1080\n",
      "Epoch 710/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8545 - loss: 0.3579 - mse: 0.1080\n",
      "Epoch 711/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8545 - loss: 0.3579 - mse: 0.1080\n",
      "Epoch 712/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8545 - loss: 0.3578 - mse: 0.1080\n",
      "Epoch 713/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8545 - loss: 0.3577 - mse: 0.1079\n",
      "Epoch 714/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8545 - loss: 0.3577 - mse: 0.1079\n",
      "Epoch 715/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8545 - loss: 0.3576 - mse: 0.1079\n",
      "Epoch 716/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8545 - loss: 0.3576 - mse: 0.1079\n",
      "Epoch 717/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8545 - loss: 0.3575 - mse: 0.1079\n",
      "Epoch 718/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8545 - loss: 0.3574 - mse: 0.1079\n",
      "Epoch 719/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8539 - loss: 0.3574 - mse: 0.1078\n",
      "Epoch 720/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8539 - loss: 0.3573 - mse: 0.1078\n",
      "Epoch 721/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8539 - loss: 0.3573 - mse: 0.1078\n",
      "Epoch 722/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8539 - loss: 0.3572 - mse: 0.1078\n",
      "Epoch 723/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8539 - loss: 0.3571 - mse: 0.1078\n",
      "Epoch 724/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8539 - loss: 0.3571 - mse: 0.1077\n",
      "Epoch 725/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8539 - loss: 0.3570 - mse: 0.1077\n",
      "Epoch 726/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8539 - loss: 0.3570 - mse: 0.1077\n",
      "Epoch 727/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8545 - loss: 0.3569 - mse: 0.1077\n",
      "Epoch 728/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8545 - loss: 0.3568 - mse: 0.1077\n",
      "Epoch 729/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8545 - loss: 0.3568 - mse: 0.1077\n",
      "Epoch 730/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8545 - loss: 0.3567 - mse: 0.1076\n",
      "Epoch 731/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8545 - loss: 0.3567 - mse: 0.1076\n",
      "Epoch 732/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8545 - loss: 0.3566 - mse: 0.1076\n",
      "Epoch 733/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8545 - loss: 0.3565 - mse: 0.1076\n",
      "Epoch 734/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8545 - loss: 0.3565 - mse: 0.1076\n",
      "Epoch 735/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8545 - loss: 0.3564 - mse: 0.1075\n",
      "Epoch 736/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8545 - loss: 0.3563 - mse: 0.1075\n",
      "Epoch 737/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8545 - loss: 0.3563 - mse: 0.1075\n",
      "Epoch 738/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8545 - loss: 0.3562 - mse: 0.1075\n",
      "Epoch 739/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8545 - loss: 0.3561 - mse: 0.1075\n",
      "Epoch 740/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8545 - loss: 0.3561 - mse: 0.1074\n",
      "Epoch 741/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8545 - loss: 0.3560 - mse: 0.1074\n",
      "Epoch 742/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8545 - loss: 0.3559 - mse: 0.1074\n",
      "Epoch 743/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8545 - loss: 0.3559 - mse: 0.1074\n",
      "Epoch 744/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8545 - loss: 0.3558 - mse: 0.1074\n",
      "Epoch 745/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8545 - loss: 0.3558 - mse: 0.1074\n",
      "Epoch 746/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8545 - loss: 0.3557 - mse: 0.1073\n",
      "Epoch 747/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8545 - loss: 0.3557 - mse: 0.1073\n",
      "Epoch 748/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8545 - loss: 0.3556 - mse: 0.1073\n",
      "Epoch 749/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8545 - loss: 0.3555 - mse: 0.1073\n",
      "Epoch 750/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8545 - loss: 0.3555 - mse: 0.1073\n",
      "Epoch 751/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.8545 - loss: 0.3554 - mse: 0.1072\n",
      "Epoch 752/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8545 - loss: 0.3554 - mse: 0.1072\n",
      "Epoch 753/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8545 - loss: 0.3553 - mse: 0.1072\n",
      "Epoch 754/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8545 - loss: 0.3553 - mse: 0.1072\n",
      "Epoch 755/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8545 - loss: 0.3552 - mse: 0.1072\n",
      "Epoch 756/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8545 - loss: 0.3552 - mse: 0.1072\n",
      "Epoch 757/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8545 - loss: 0.3551 - mse: 0.1071\n",
      "Epoch 758/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8545 - loss: 0.3550 - mse: 0.1071\n",
      "Epoch 759/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8545 - loss: 0.3550 - mse: 0.1071\n",
      "Epoch 760/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8545 - loss: 0.3549 - mse: 0.1071\n",
      "Epoch 761/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8545 - loss: 0.3549 - mse: 0.1071\n",
      "Epoch 762/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8545 - loss: 0.3548 - mse: 0.1071\n",
      "Epoch 763/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.8550 - loss: 0.3548 - mse: 0.1070\n",
      "Epoch 764/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8550 - loss: 0.3547 - mse: 0.1070\n",
      "Epoch 765/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8550 - loss: 0.3547 - mse: 0.1070\n",
      "Epoch 766/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8550 - loss: 0.3546 - mse: 0.1070\n",
      "Epoch 767/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.8550 - loss: 0.3545 - mse: 0.1070\n",
      "Epoch 768/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.8550 - loss: 0.3545 - mse: 0.1070\n",
      "Epoch 769/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8550 - loss: 0.3544 - mse: 0.1069\n",
      "Epoch 770/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8550 - loss: 0.3544 - mse: 0.1069\n",
      "Epoch 771/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8550 - loss: 0.3543 - mse: 0.1069\n",
      "Epoch 772/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8556 - loss: 0.3543 - mse: 0.1069\n",
      "Epoch 773/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8556 - loss: 0.3542 - mse: 0.1069\n",
      "Epoch 774/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8550 - loss: 0.3542 - mse: 0.1069\n",
      "Epoch 775/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8550 - loss: 0.3541 - mse: 0.1068\n",
      "Epoch 776/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8550 - loss: 0.3541 - mse: 0.1068\n",
      "Epoch 777/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8556 - loss: 0.3540 - mse: 0.1068\n",
      "Epoch 778/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8550 - loss: 0.3540 - mse: 0.1068\n",
      "Epoch 779/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8556 - loss: 0.3539 - mse: 0.1068\n",
      "Epoch 780/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8550 - loss: 0.3538 - mse: 0.1067\n",
      "Epoch 781/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8556 - loss: 0.3538 - mse: 0.1068\n",
      "Epoch 782/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8550 - loss: 0.3537 - mse: 0.1067\n",
      "Epoch 783/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8556 - loss: 0.3537 - mse: 0.1067\n",
      "Epoch 784/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8550 - loss: 0.3536 - mse: 0.1067\n",
      "Epoch 785/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8556 - loss: 0.3536 - mse: 0.1067\n",
      "Epoch 786/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8556 - loss: 0.3535 - mse: 0.1067\n",
      "Epoch 787/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.8556 - loss: 0.3535 - mse: 0.1066\n",
      "Epoch 788/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8556 - loss: 0.3534 - mse: 0.1066\n",
      "Epoch 789/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8556 - loss: 0.3534 - mse: 0.1066\n",
      "Epoch 790/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8562 - loss: 0.3533 - mse: 0.1066\n",
      "Epoch 791/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8556 - loss: 0.3533 - mse: 0.1066\n",
      "Epoch 792/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8562 - loss: 0.3532 - mse: 0.1066\n",
      "Epoch 793/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8556 - loss: 0.3532 - mse: 0.1065\n",
      "Epoch 794/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8556 - loss: 0.3531 - mse: 0.1065\n",
      "Epoch 795/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8562 - loss: 0.3531 - mse: 0.1065\n",
      "Epoch 796/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8556 - loss: 0.3530 - mse: 0.1065\n",
      "Epoch 797/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.8562 - loss: 0.3530 - mse: 0.1065\n",
      "Epoch 798/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8556 - loss: 0.3529 - mse: 0.1065\n",
      "Epoch 799/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8562 - loss: 0.3529 - mse: 0.1064\n",
      "Epoch 800/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8562 - loss: 0.3528 - mse: 0.1065\n",
      "Epoch 801/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8562 - loss: 0.3528 - mse: 0.1064\n",
      "Epoch 802/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8562 - loss: 0.3527 - mse: 0.1064\n",
      "Epoch 803/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8562 - loss: 0.3527 - mse: 0.1064\n",
      "Epoch 804/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8584 - loss: 0.3541 - mse: 0.1065\n",
      "Epoch 805/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8499 - loss: 0.3666 - mse: 0.1114\n",
      "Epoch 806/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.8539 - loss: 0.3807 - mse: 0.1133\n",
      "Epoch 807/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8453 - loss: 0.3879 - mse: 0.1175\n",
      "Epoch 808/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8573 - loss: 0.3534 - mse: 0.1063\n",
      "Epoch 809/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8545 - loss: 0.3723 - mse: 0.1109\n",
      "Epoch 810/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8419 - loss: 0.4070 - mse: 0.1226\n",
      "Epoch 811/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8573 - loss: 0.3546 - mse: 0.1065\n",
      "Epoch 812/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8533 - loss: 0.3805 - mse: 0.1132\n",
      "Epoch 813/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8271 - loss: 0.4284 - mse: 0.1279\n",
      "Epoch 814/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8562 - loss: 0.3523 - mse: 0.1062\n",
      "Epoch 815/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.8459 - loss: 0.4287 - mse: 0.1284\n",
      "Epoch 816/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.8071 - loss: 0.5134 - mse: 0.1470\n",
      "Epoch 817/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - accuracy: 0.8470 - loss: 0.3750 - mse: 0.1135\n",
      "Epoch 818/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.6981 - loss: 0.6729 - mse: 0.2196\n",
      "Epoch 819/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7734 - loss: 0.8411 - mse: 0.1974\n",
      "Epoch 820/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.7751 - loss: 0.8339 - mse: 0.1962\n",
      "Epoch 821/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8408 - loss: 0.4706 - mse: 0.1410\n",
      "Epoch 822/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8293 - loss: 0.5007 - mse: 0.1509\n",
      "Epoch 823/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7900 - loss: 0.7120 - mse: 0.1775\n",
      "Epoch 824/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.7945 - loss: 0.6620 - mse: 0.1687\n",
      "Epoch 825/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8408 - loss: 0.4763 - mse: 0.1403\n",
      "Epoch 826/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8453 - loss: 0.4444 - mse: 0.1297\n",
      "Epoch 827/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8156 - loss: 0.5407 - mse: 0.1445\n",
      "Epoch 828/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.8196 - loss: 0.5184 - mse: 0.1394\n",
      "Epoch 829/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8470 - loss: 0.4216 - mse: 0.1221\n",
      "Epoch 830/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.8459 - loss: 0.4579 - mse: 0.1318\n",
      "Epoch 831/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8413 - loss: 0.4689 - mse: 0.1285\n",
      "Epoch 832/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8293 - loss: 0.5026 - mse: 0.1343\n",
      "Epoch 833/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8550 - loss: 0.3773 - mse: 0.1109\n",
      "Epoch 834/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8276 - loss: 0.5262 - mse: 0.1505\n",
      "Epoch 835/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8482 - loss: 0.4384 - mse: 0.1224\n",
      "Epoch 836/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.8225 - loss: 0.5368 - mse: 0.1396\n",
      "Epoch 837/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.8584 - loss: 0.3689 - mse: 0.1096\n",
      "Epoch 838/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.8134 - loss: 0.5752 - mse: 0.1650\n",
      "Epoch 839/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.8516 - loss: 0.4034 - mse: 0.1162\n",
      "Epoch 840/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.8248 - loss: 0.5330 - mse: 0.1388\n",
      "Epoch 841/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - accuracy: 0.8550 - loss: 0.3829 - mse: 0.1125\n",
      "Epoch 842/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - accuracy: 0.8339 - loss: 0.5235 - mse: 0.1480\n",
      "Epoch 843/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.8584 - loss: 0.3696 - mse: 0.1097\n",
      "Epoch 844/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8425 - loss: 0.4756 - mse: 0.1280\n",
      "Epoch 845/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8527 - loss: 0.3902 - mse: 0.1138\n",
      "Epoch 846/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8476 - loss: 0.4228 - mse: 0.1204\n",
      "Epoch 847/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8539 - loss: 0.3847 - mse: 0.1121\n",
      "Epoch 848/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8522 - loss: 0.3978 - mse: 0.1153\n",
      "Epoch 849/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8533 - loss: 0.4041 - mse: 0.1166\n",
      "Epoch 850/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8584 - loss: 0.3699 - mse: 0.1094\n",
      "Epoch 851/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8505 - loss: 0.4084 - mse: 0.1174\n",
      "Epoch 852/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8573 - loss: 0.3684 - mse: 0.1099\n",
      "Epoch 853/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8522 - loss: 0.3996 - mse: 0.1163\n",
      "Epoch 854/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8567 - loss: 0.3645 - mse: 0.1090\n",
      "Epoch 855/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8522 - loss: 0.3938 - mse: 0.1144\n",
      "Epoch 856/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8573 - loss: 0.3622 - mse: 0.1081\n",
      "Epoch 857/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8527 - loss: 0.3811 - mse: 0.1131\n",
      "Epoch 858/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8545 - loss: 0.3663 - mse: 0.1098\n",
      "Epoch 859/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8567 - loss: 0.3721 - mse: 0.1098\n",
      "Epoch 860/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8596 - loss: 0.3643 - mse: 0.1083\n",
      "Epoch 861/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8533 - loss: 0.3664 - mse: 0.1100\n",
      "Epoch 862/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8533 - loss: 0.3655 - mse: 0.1098\n",
      "Epoch 863/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8579 - loss: 0.3602 - mse: 0.1075\n",
      "Epoch 864/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8596 - loss: 0.3638 - mse: 0.1082\n",
      "Epoch 865/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8562 - loss: 0.3583 - mse: 0.1080\n",
      "Epoch 866/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8527 - loss: 0.3622 - mse: 0.1091\n",
      "Epoch 867/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8573 - loss: 0.3551 - mse: 0.1066\n",
      "Epoch 868/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8596 - loss: 0.3604 - mse: 0.1075\n",
      "Epoch 869/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8567 - loss: 0.3543 - mse: 0.1069\n",
      "Epoch 870/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8533 - loss: 0.3583 - mse: 0.1082\n",
      "Epoch 871/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8596 - loss: 0.3526 - mse: 0.1060\n",
      "Epoch 872/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8579 - loss: 0.3567 - mse: 0.1067\n",
      "Epoch 873/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8567 - loss: 0.3522 - mse: 0.1064\n",
      "Epoch 874/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8556 - loss: 0.3549 - mse: 0.1074\n",
      "Epoch 875/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8590 - loss: 0.3514 - mse: 0.1057\n",
      "Epoch 876/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8590 - loss: 0.3537 - mse: 0.1061\n",
      "Epoch 877/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8567 - loss: 0.3515 - mse: 0.1063\n",
      "Epoch 878/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8545 - loss: 0.3525 - mse: 0.1067\n",
      "Epoch 879/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8590 - loss: 0.3512 - mse: 0.1056\n",
      "Epoch 880/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8596 - loss: 0.3516 - mse: 0.1056\n",
      "Epoch 881/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8556 - loss: 0.3514 - mse: 0.1063\n",
      "Epoch 882/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8562 - loss: 0.3510 - mse: 0.1061\n",
      "Epoch 883/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8590 - loss: 0.3514 - mse: 0.1055\n",
      "Epoch 884/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8573 - loss: 0.3504 - mse: 0.1054\n",
      "Epoch 885/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8550 - loss: 0.3513 - mse: 0.1063\n",
      "Epoch 886/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8573 - loss: 0.3501 - mse: 0.1057\n",
      "Epoch 887/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8590 - loss: 0.3513 - mse: 0.1055\n",
      "Epoch 888/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8573 - loss: 0.3499 - mse: 0.1055\n",
      "Epoch 889/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8556 - loss: 0.3510 - mse: 0.1061\n",
      "Epoch 890/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8567 - loss: 0.3499 - mse: 0.1054\n",
      "Epoch 891/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8584 - loss: 0.3507 - mse: 0.1054\n",
      "Epoch 892/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8573 - loss: 0.3500 - mse: 0.1056\n",
      "Epoch 893/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8567 - loss: 0.3503 - mse: 0.1058\n",
      "Epoch 894/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8567 - loss: 0.3500 - mse: 0.1053\n",
      "Epoch 895/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8573 - loss: 0.3500 - mse: 0.1053\n",
      "Epoch 896/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8567 - loss: 0.3500 - mse: 0.1057\n",
      "Epoch 897/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8573 - loss: 0.3498 - mse: 0.1055\n",
      "Epoch 898/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8573 - loss: 0.3499 - mse: 0.1052\n",
      "Epoch 899/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8579 - loss: 0.3496 - mse: 0.1053\n",
      "Epoch 900/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8573 - loss: 0.3498 - mse: 0.1056\n",
      "Epoch 901/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8584 - loss: 0.3495 - mse: 0.1053\n",
      "Epoch 902/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8573 - loss: 0.3497 - mse: 0.1052\n",
      "Epoch 903/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8584 - loss: 0.3494 - mse: 0.1053\n",
      "Epoch 904/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8567 - loss: 0.3496 - mse: 0.1055\n",
      "Epoch 905/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8573 - loss: 0.3493 - mse: 0.1052\n",
      "Epoch 906/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8567 - loss: 0.3494 - mse: 0.1052\n",
      "Epoch 907/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.8584 - loss: 0.3493 - mse: 0.1054\n",
      "Epoch 908/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8584 - loss: 0.3493 - mse: 0.1054\n",
      "Epoch 909/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8567 - loss: 0.3493 - mse: 0.1052\n",
      "Epoch 910/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8573 - loss: 0.3492 - mse: 0.1052\n",
      "Epoch 911/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8584 - loss: 0.3492 - mse: 0.1053\n",
      "Epoch 912/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8573 - loss: 0.3491 - mse: 0.1052\n",
      "Epoch 913/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8567 - loss: 0.3492 - mse: 0.1051\n",
      "Epoch 914/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8579 - loss: 0.3490 - mse: 0.1052\n",
      "Epoch 915/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8584 - loss: 0.3491 - mse: 0.1053\n",
      "Epoch 916/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8573 - loss: 0.3490 - mse: 0.1052\n",
      "Epoch 917/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8567 - loss: 0.3490 - mse: 0.1051\n",
      "Epoch 918/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8573 - loss: 0.3489 - mse: 0.1052\n",
      "Epoch 919/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8584 - loss: 0.3489 - mse: 0.1052\n",
      "Epoch 920/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8573 - loss: 0.3489 - mse: 0.1051\n",
      "Epoch 921/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8573 - loss: 0.3489 - mse: 0.1051\n",
      "Epoch 922/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8590 - loss: 0.3488 - mse: 0.1052\n",
      "Epoch 923/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8573 - loss: 0.3488 - mse: 0.1052\n",
      "Epoch 924/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8573 - loss: 0.3488 - mse: 0.1051\n",
      "Epoch 925/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8573 - loss: 0.3487 - mse: 0.1051\n",
      "Epoch 926/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8590 - loss: 0.3487 - mse: 0.1052\n",
      "Epoch 927/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8573 - loss: 0.3486 - mse: 0.1051\n",
      "Epoch 928/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8573 - loss: 0.3486 - mse: 0.1051\n",
      "Epoch 929/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8573 - loss: 0.3486 - mse: 0.1051\n",
      "Epoch 930/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8584 - loss: 0.3486 - mse: 0.1052\n",
      "Epoch 931/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8573 - loss: 0.3486 - mse: 0.1051\n",
      "Epoch 932/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8573 - loss: 0.3485 - mse: 0.1051\n",
      "Epoch 933/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8584 - loss: 0.3485 - mse: 0.1051\n",
      "Epoch 934/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8579 - loss: 0.3485 - mse: 0.1051\n",
      "Epoch 935/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8573 - loss: 0.3485 - mse: 0.1051\n",
      "Epoch 936/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8567 - loss: 0.3484 - mse: 0.1051\n",
      "Epoch 937/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8584 - loss: 0.3484 - mse: 0.1051\n",
      "Epoch 938/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8567 - loss: 0.3484 - mse: 0.1051\n",
      "Epoch 939/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8573 - loss: 0.3484 - mse: 0.1050\n",
      "Epoch 940/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8573 - loss: 0.3483 - mse: 0.1051\n",
      "Epoch 941/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8573 - loss: 0.3483 - mse: 0.1051\n",
      "Epoch 942/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8573 - loss: 0.3483 - mse: 0.1050\n",
      "Epoch 943/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8573 - loss: 0.3483 - mse: 0.1050\n",
      "Epoch 944/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8573 - loss: 0.3483 - mse: 0.1050\n",
      "Epoch 945/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8573 - loss: 0.3482 - mse: 0.1050\n",
      "Epoch 946/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8573 - loss: 0.3482 - mse: 0.1050\n",
      "Epoch 947/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8573 - loss: 0.3482 - mse: 0.1050\n",
      "Epoch 948/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8573 - loss: 0.3482 - mse: 0.1050\n",
      "Epoch 949/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8567 - loss: 0.3481 - mse: 0.1050\n",
      "Epoch 950/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8573 - loss: 0.3481 - mse: 0.1050\n",
      "Epoch 951/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8573 - loss: 0.3481 - mse: 0.1050\n",
      "Epoch 952/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8567 - loss: 0.3481 - mse: 0.1050\n",
      "Epoch 953/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8573 - loss: 0.3480 - mse: 0.1049\n",
      "Epoch 954/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8573 - loss: 0.3480 - mse: 0.1049\n",
      "Epoch 955/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8567 - loss: 0.3480 - mse: 0.1049\n",
      "Epoch 956/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8567 - loss: 0.3479 - mse: 0.1049\n",
      "Epoch 957/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8573 - loss: 0.3479 - mse: 0.1049\n",
      "Epoch 958/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8573 - loss: 0.3479 - mse: 0.1049\n",
      "Epoch 959/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8567 - loss: 0.3478 - mse: 0.1049\n",
      "Epoch 960/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8567 - loss: 0.3478 - mse: 0.1049\n",
      "Epoch 961/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8573 - loss: 0.3478 - mse: 0.1049\n",
      "Epoch 962/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8573 - loss: 0.3477 - mse: 0.1049\n",
      "Epoch 963/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8573 - loss: 0.3477 - mse: 0.1049\n",
      "Epoch 964/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8573 - loss: 0.3477 - mse: 0.1048\n",
      "Epoch 965/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8573 - loss: 0.3477 - mse: 0.1048\n",
      "Epoch 966/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8573 - loss: 0.3477 - mse: 0.1048\n",
      "Epoch 967/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8573 - loss: 0.3476 - mse: 0.1048\n",
      "Epoch 968/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8573 - loss: 0.3476 - mse: 0.1048\n",
      "Epoch 969/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8573 - loss: 0.3476 - mse: 0.1048\n",
      "Epoch 970/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8573 - loss: 0.3476 - mse: 0.1048\n",
      "Epoch 971/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8573 - loss: 0.3476 - mse: 0.1048\n",
      "Epoch 972/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8573 - loss: 0.3475 - mse: 0.1048\n",
      "Epoch 973/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8573 - loss: 0.3475 - mse: 0.1048\n",
      "Epoch 974/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8573 - loss: 0.3475 - mse: 0.1048\n",
      "Epoch 975/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8573 - loss: 0.3475 - mse: 0.1048\n",
      "Epoch 976/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8573 - loss: 0.3474 - mse: 0.1048\n",
      "Epoch 977/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8567 - loss: 0.3474 - mse: 0.1048\n",
      "Epoch 978/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8567 - loss: 0.3474 - mse: 0.1048\n",
      "Epoch 979/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8567 - loss: 0.3474 - mse: 0.1047\n",
      "Epoch 980/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8567 - loss: 0.3473 - mse: 0.1047\n",
      "Epoch 981/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8567 - loss: 0.3473 - mse: 0.1047\n",
      "Epoch 982/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8567 - loss: 0.3473 - mse: 0.1047\n",
      "Epoch 983/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8567 - loss: 0.3473 - mse: 0.1047\n",
      "Epoch 984/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8567 - loss: 0.3472 - mse: 0.1047\n",
      "Epoch 985/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8573 - loss: 0.3472 - mse: 0.1047\n",
      "Epoch 986/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8567 - loss: 0.3472 - mse: 0.1047\n",
      "Epoch 987/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8573 - loss: 0.3472 - mse: 0.1047\n",
      "Epoch 988/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8579 - loss: 0.3472 - mse: 0.1047\n",
      "Epoch 989/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8579 - loss: 0.3471 - mse: 0.1047\n",
      "Epoch 990/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8573 - loss: 0.3471 - mse: 0.1047\n",
      "Epoch 991/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8579 - loss: 0.3471 - mse: 0.1047\n",
      "Epoch 992/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8579 - loss: 0.3471 - mse: 0.1047\n",
      "Epoch 993/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8573 - loss: 0.3470 - mse: 0.1046\n",
      "Epoch 994/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8573 - loss: 0.3470 - mse: 0.1046\n",
      "Epoch 995/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8573 - loss: 0.3470 - mse: 0.1046\n",
      "Epoch 996/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8567 - loss: 0.3470 - mse: 0.1046\n",
      "Epoch 997/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8567 - loss: 0.3469 - mse: 0.1046\n",
      "Epoch 998/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8567 - loss: 0.3469 - mse: 0.1046\n",
      "Epoch 999/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8567 - loss: 0.3469 - mse: 0.1046\n",
      "Epoch 1000/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8567 - loss: 0.3469 - mse: 0.1046\n",
      "Epoch 1001/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8567 - loss: 0.3469 - mse: 0.1046\n",
      "Epoch 1002/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8567 - loss: 0.3468 - mse: 0.1046\n",
      "Epoch 1003/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8567 - loss: 0.3468 - mse: 0.1046\n",
      "Epoch 1004/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8567 - loss: 0.3468 - mse: 0.1046\n",
      "Epoch 1005/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8567 - loss: 0.3468 - mse: 0.1046\n",
      "Epoch 1006/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8573 - loss: 0.3467 - mse: 0.1046\n",
      "Epoch 1007/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8573 - loss: 0.3467 - mse: 0.1046\n",
      "Epoch 1008/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8573 - loss: 0.3467 - mse: 0.1045\n",
      "Epoch 1009/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8573 - loss: 0.3467 - mse: 0.1045\n",
      "Epoch 1010/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8579 - loss: 0.3466 - mse: 0.1045\n",
      "Epoch 1011/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8573 - loss: 0.3466 - mse: 0.1045\n",
      "Epoch 1012/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8573 - loss: 0.3466 - mse: 0.1045\n",
      "Epoch 1013/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8579 - loss: 0.3466 - mse: 0.1045\n",
      "Epoch 1014/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8579 - loss: 0.3466 - mse: 0.1045\n",
      "Epoch 1015/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8573 - loss: 0.3465 - mse: 0.1045\n",
      "Epoch 1016/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8579 - loss: 0.3465 - mse: 0.1045\n",
      "Epoch 1017/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8579 - loss: 0.3465 - mse: 0.1045\n",
      "Epoch 1018/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8579 - loss: 0.3465 - mse: 0.1045\n",
      "Epoch 1019/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8579 - loss: 0.3464 - mse: 0.1045\n",
      "Epoch 1020/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8579 - loss: 0.3464 - mse: 0.1045\n",
      "Epoch 1021/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8579 - loss: 0.3464 - mse: 0.1045\n",
      "Epoch 1022/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8579 - loss: 0.3464 - mse: 0.1045\n",
      "Epoch 1023/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8579 - loss: 0.3464 - mse: 0.1044\n",
      "Epoch 1024/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8579 - loss: 0.3463 - mse: 0.1044\n",
      "Epoch 1025/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8579 - loss: 0.3463 - mse: 0.1044\n",
      "Epoch 1026/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8579 - loss: 0.3463 - mse: 0.1044\n",
      "Epoch 1027/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8579 - loss: 0.3463 - mse: 0.1044\n",
      "Epoch 1028/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8579 - loss: 0.3462 - mse: 0.1044\n",
      "Epoch 1029/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8579 - loss: 0.3462 - mse: 0.1044\n",
      "Epoch 1030/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8579 - loss: 0.3462 - mse: 0.1044\n",
      "Epoch 1031/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8579 - loss: 0.3462 - mse: 0.1044\n",
      "Epoch 1032/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8579 - loss: 0.3462 - mse: 0.1044\n",
      "Epoch 1033/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8579 - loss: 0.3461 - mse: 0.1044\n",
      "Epoch 1034/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8579 - loss: 0.3461 - mse: 0.1044\n",
      "Epoch 1035/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8579 - loss: 0.3461 - mse: 0.1044\n",
      "Epoch 1036/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8584 - loss: 0.3461 - mse: 0.1044\n",
      "Epoch 1037/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8584 - loss: 0.3461 - mse: 0.1044\n",
      "Epoch 1038/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8584 - loss: 0.3460 - mse: 0.1043\n",
      "Epoch 1039/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8584 - loss: 0.3460 - mse: 0.1043\n",
      "Epoch 1040/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8584 - loss: 0.3460 - mse: 0.1043\n",
      "Epoch 1041/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8584 - loss: 0.3460 - mse: 0.1043\n",
      "Epoch 1042/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8584 - loss: 0.3459 - mse: 0.1043\n",
      "Epoch 1043/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8584 - loss: 0.3459 - mse: 0.1043\n",
      "Epoch 1044/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8584 - loss: 0.3459 - mse: 0.1043\n",
      "Epoch 1045/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8584 - loss: 0.3459 - mse: 0.1043\n",
      "Epoch 1046/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8584 - loss: 0.3459 - mse: 0.1043\n",
      "Epoch 1047/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8584 - loss: 0.3458 - mse: 0.1043\n",
      "Epoch 1048/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8584 - loss: 0.3458 - mse: 0.1043\n",
      "Epoch 1049/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8584 - loss: 0.3458 - mse: 0.1043\n",
      "Epoch 1050/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8584 - loss: 0.3458 - mse: 0.1043\n",
      "Epoch 1051/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8584 - loss: 0.3457 - mse: 0.1043\n",
      "Epoch 1052/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8584 - loss: 0.3457 - mse: 0.1043\n",
      "Epoch 1053/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8584 - loss: 0.3457 - mse: 0.1043\n",
      "Epoch 1054/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8584 - loss: 0.3457 - mse: 0.1042\n",
      "Epoch 1055/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8584 - loss: 0.3457 - mse: 0.1042\n",
      "Epoch 1056/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8584 - loss: 0.3456 - mse: 0.1042\n",
      "Epoch 1057/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8584 - loss: 0.3456 - mse: 0.1042\n",
      "Epoch 1058/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8584 - loss: 0.3456 - mse: 0.1042\n",
      "Epoch 1059/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8584 - loss: 0.3456 - mse: 0.1042\n",
      "Epoch 1060/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8584 - loss: 0.3455 - mse: 0.1042\n",
      "Epoch 1061/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8584 - loss: 0.3455 - mse: 0.1042\n",
      "Epoch 1062/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8584 - loss: 0.3455 - mse: 0.1042\n",
      "Epoch 1063/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8584 - loss: 0.3455 - mse: 0.1042\n",
      "Epoch 1064/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8584 - loss: 0.3455 - mse: 0.1042\n",
      "Epoch 1065/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8584 - loss: 0.3454 - mse: 0.1042\n",
      "Epoch 1066/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8584 - loss: 0.3454 - mse: 0.1042\n",
      "Epoch 1067/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8584 - loss: 0.3454 - mse: 0.1042\n",
      "Epoch 1068/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8584 - loss: 0.3454 - mse: 0.1042\n",
      "Epoch 1069/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8584 - loss: 0.3454 - mse: 0.1041\n",
      "Epoch 1070/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8584 - loss: 0.3453 - mse: 0.1041\n",
      "Epoch 1071/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8584 - loss: 0.3453 - mse: 0.1041\n",
      "Epoch 1072/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8584 - loss: 0.3453 - mse: 0.1041\n",
      "Epoch 1073/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8584 - loss: 0.3453 - mse: 0.1041\n",
      "Epoch 1074/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8584 - loss: 0.3452 - mse: 0.1041\n",
      "Epoch 1075/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8584 - loss: 0.3452 - mse: 0.1041\n",
      "Epoch 1076/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8584 - loss: 0.3452 - mse: 0.1041\n",
      "Epoch 1077/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8584 - loss: 0.3452 - mse: 0.1041\n",
      "Epoch 1078/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8584 - loss: 0.3452 - mse: 0.1041\n",
      "Epoch 1079/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8584 - loss: 0.3451 - mse: 0.1041\n",
      "Epoch 1080/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8584 - loss: 0.3451 - mse: 0.1041\n",
      "Epoch 1081/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8584 - loss: 0.3451 - mse: 0.1041\n",
      "Epoch 1082/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8584 - loss: 0.3451 - mse: 0.1041\n",
      "Epoch 1083/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8584 - loss: 0.3451 - mse: 0.1041\n",
      "Epoch 1084/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8584 - loss: 0.3450 - mse: 0.1040\n",
      "Epoch 1085/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8584 - loss: 0.3450 - mse: 0.1040\n",
      "Epoch 1086/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8584 - loss: 0.3450 - mse: 0.1040\n",
      "Epoch 1087/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8584 - loss: 0.3450 - mse: 0.1040\n",
      "Epoch 1088/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8584 - loss: 0.3449 - mse: 0.1040\n",
      "Epoch 1089/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8584 - loss: 0.3449 - mse: 0.1040\n",
      "Epoch 1090/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8584 - loss: 0.3449 - mse: 0.1040\n",
      "Epoch 1091/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8584 - loss: 0.3449 - mse: 0.1040\n",
      "Epoch 1092/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8584 - loss: 0.3449 - mse: 0.1040\n",
      "Epoch 1093/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8584 - loss: 0.3448 - mse: 0.1040\n",
      "Epoch 1094/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8584 - loss: 0.3448 - mse: 0.1040\n",
      "Epoch 1095/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8584 - loss: 0.3448 - mse: 0.1040\n",
      "Epoch 1096/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8584 - loss: 0.3448 - mse: 0.1040\n",
      "Epoch 1097/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8584 - loss: 0.3448 - mse: 0.1040\n",
      "Epoch 1098/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8584 - loss: 0.3447 - mse: 0.1040\n",
      "Epoch 1099/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8584 - loss: 0.3447 - mse: 0.1040\n",
      "Epoch 1100/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8584 - loss: 0.3447 - mse: 0.1039\n",
      "Epoch 1101/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8584 - loss: 0.3447 - mse: 0.1039\n",
      "Epoch 1102/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8584 - loss: 0.3447 - mse: 0.1039\n",
      "Epoch 1103/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8584 - loss: 0.3446 - mse: 0.1039\n",
      "Epoch 1104/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8596 - loss: 0.3446 - mse: 0.1039\n",
      "Epoch 1105/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8596 - loss: 0.3446 - mse: 0.1039\n",
      "Epoch 1106/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8596 - loss: 0.3446 - mse: 0.1039\n",
      "Epoch 1107/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8596 - loss: 0.3445 - mse: 0.1039\n",
      "Epoch 1108/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8596 - loss: 0.3445 - mse: 0.1039\n",
      "Epoch 1109/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8596 - loss: 0.3445 - mse: 0.1039\n",
      "Epoch 1110/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8596 - loss: 0.3445 - mse: 0.1039\n",
      "Epoch 1111/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8596 - loss: 0.3445 - mse: 0.1039\n",
      "Epoch 1112/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8596 - loss: 0.3444 - mse: 0.1039\n",
      "Epoch 1113/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8596 - loss: 0.3444 - mse: 0.1039\n",
      "Epoch 1114/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8596 - loss: 0.3444 - mse: 0.1039\n",
      "Epoch 1115/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8596 - loss: 0.3444 - mse: 0.1039\n",
      "Epoch 1116/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8596 - loss: 0.3444 - mse: 0.1038\n",
      "Epoch 1117/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8596 - loss: 0.3443 - mse: 0.1038\n",
      "Epoch 1118/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8596 - loss: 0.3443 - mse: 0.1038\n",
      "Epoch 1119/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8596 - loss: 0.3443 - mse: 0.1038\n",
      "Epoch 1120/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8596 - loss: 0.3443 - mse: 0.1038\n",
      "Epoch 1121/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8596 - loss: 0.3443 - mse: 0.1038\n",
      "Epoch 1122/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8596 - loss: 0.3442 - mse: 0.1038\n",
      "Epoch 1123/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8596 - loss: 0.3442 - mse: 0.1038\n",
      "Epoch 1124/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8596 - loss: 0.3442 - mse: 0.1038\n",
      "Epoch 1125/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8596 - loss: 0.3442 - mse: 0.1038\n",
      "Epoch 1126/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8596 - loss: 0.3442 - mse: 0.1038\n",
      "Epoch 1127/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8596 - loss: 0.3441 - mse: 0.1038\n",
      "Epoch 1128/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8596 - loss: 0.3441 - mse: 0.1038\n",
      "Epoch 1129/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8596 - loss: 0.3441 - mse: 0.1038\n",
      "Epoch 1130/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8596 - loss: 0.3441 - mse: 0.1038\n",
      "Epoch 1131/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8596 - loss: 0.3440 - mse: 0.1038\n",
      "Epoch 1132/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8596 - loss: 0.3440 - mse: 0.1037\n",
      "Epoch 1133/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8596 - loss: 0.3440 - mse: 0.1037\n",
      "Epoch 1134/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8596 - loss: 0.3440 - mse: 0.1037\n",
      "Epoch 1135/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8596 - loss: 0.3440 - mse: 0.1037\n",
      "Epoch 1136/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8596 - loss: 0.3439 - mse: 0.1037\n",
      "Epoch 1137/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8596 - loss: 0.3439 - mse: 0.1037\n",
      "Epoch 1138/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8596 - loss: 0.3439 - mse: 0.1037\n",
      "Epoch 1139/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8596 - loss: 0.3439 - mse: 0.1037\n",
      "Epoch 1140/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8596 - loss: 0.3439 - mse: 0.1037\n",
      "Epoch 1141/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8596 - loss: 0.3438 - mse: 0.1037\n",
      "Epoch 1142/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8596 - loss: 0.3438 - mse: 0.1037\n",
      "Epoch 1143/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8596 - loss: 0.3438 - mse: 0.1037\n",
      "Epoch 1144/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8596 - loss: 0.3438 - mse: 0.1037\n",
      "Epoch 1145/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8596 - loss: 0.3438 - mse: 0.1037\n",
      "Epoch 1146/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8596 - loss: 0.3437 - mse: 0.1037\n",
      "Epoch 1147/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8596 - loss: 0.3437 - mse: 0.1036\n",
      "Epoch 1148/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8596 - loss: 0.3437 - mse: 0.1036\n",
      "Epoch 1149/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8596 - loss: 0.3437 - mse: 0.1036\n",
      "Epoch 1150/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8596 - loss: 0.3437 - mse: 0.1036\n",
      "Epoch 1151/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8596 - loss: 0.3436 - mse: 0.1036\n",
      "Epoch 1152/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8596 - loss: 0.3436 - mse: 0.1036\n",
      "Epoch 1153/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8596 - loss: 0.3436 - mse: 0.1036\n",
      "Epoch 1154/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8596 - loss: 0.3436 - mse: 0.1036\n",
      "Epoch 1155/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8596 - loss: 0.3436 - mse: 0.1036\n",
      "Epoch 1156/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8596 - loss: 0.3435 - mse: 0.1036\n",
      "Epoch 1157/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8596 - loss: 0.3435 - mse: 0.1036\n",
      "Epoch 1158/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8596 - loss: 0.3435 - mse: 0.1036\n",
      "Epoch 1159/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8596 - loss: 0.3435 - mse: 0.1036\n",
      "Epoch 1160/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8596 - loss: 0.3435 - mse: 0.1036\n",
      "Epoch 1161/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8596 - loss: 0.3434 - mse: 0.1036\n",
      "Epoch 1162/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8596 - loss: 0.3434 - mse: 0.1036\n",
      "Epoch 1163/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8596 - loss: 0.3434 - mse: 0.1036\n",
      "Epoch 1164/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8596 - loss: 0.3434 - mse: 0.1035\n",
      "Epoch 1165/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8596 - loss: 0.3433 - mse: 0.1035\n",
      "Epoch 1166/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8596 - loss: 0.3433 - mse: 0.1035\n",
      "Epoch 1167/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8596 - loss: 0.3433 - mse: 0.1035\n",
      "Epoch 1168/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8596 - loss: 0.3433 - mse: 0.1035\n",
      "Epoch 1169/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8596 - loss: 0.3433 - mse: 0.1035\n",
      "Epoch 1170/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8596 - loss: 0.3432 - mse: 0.1035\n",
      "Epoch 1171/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8596 - loss: 0.3432 - mse: 0.1035\n",
      "Epoch 1172/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8596 - loss: 0.3432 - mse: 0.1035\n",
      "Epoch 1173/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8596 - loss: 0.3432 - mse: 0.1035\n",
      "Epoch 1174/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8596 - loss: 0.3432 - mse: 0.1035\n",
      "Epoch 1175/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8596 - loss: 0.3431 - mse: 0.1035\n",
      "Epoch 1176/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8596 - loss: 0.3431 - mse: 0.1035\n",
      "Epoch 1177/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8596 - loss: 0.3431 - mse: 0.1035\n",
      "Epoch 1178/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8596 - loss: 0.3431 - mse: 0.1035\n",
      "Epoch 1179/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8596 - loss: 0.3431 - mse: 0.1035\n",
      "Epoch 1180/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8596 - loss: 0.3430 - mse: 0.1034\n",
      "Epoch 1181/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8596 - loss: 0.3430 - mse: 0.1034\n",
      "Epoch 1182/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8596 - loss: 0.3430 - mse: 0.1034\n",
      "Epoch 1183/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8596 - loss: 0.3430 - mse: 0.1034\n",
      "Epoch 1184/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8596 - loss: 0.3430 - mse: 0.1034\n",
      "Epoch 1185/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8596 - loss: 0.3429 - mse: 0.1034\n",
      "Epoch 1186/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8596 - loss: 0.3429 - mse: 0.1034\n",
      "Epoch 1187/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8596 - loss: 0.3429 - mse: 0.1034\n",
      "Epoch 1188/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8602 - loss: 0.3429 - mse: 0.1034\n",
      "Epoch 1189/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8602 - loss: 0.3429 - mse: 0.1034\n",
      "Epoch 1190/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8602 - loss: 0.3428 - mse: 0.1034\n",
      "Epoch 1191/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8602 - loss: 0.3428 - mse: 0.1034\n",
      "Epoch 1192/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8607 - loss: 0.3428 - mse: 0.1034\n",
      "Epoch 1193/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8607 - loss: 0.3428 - mse: 0.1034\n",
      "Epoch 1194/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8607 - loss: 0.3428 - mse: 0.1034\n",
      "Epoch 1195/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8607 - loss: 0.3427 - mse: 0.1034\n",
      "Epoch 1196/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8607 - loss: 0.3427 - mse: 0.1033\n",
      "Epoch 1197/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8607 - loss: 0.3427 - mse: 0.1033\n",
      "Epoch 1198/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8607 - loss: 0.3427 - mse: 0.1033\n",
      "Epoch 1199/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8607 - loss: 0.3427 - mse: 0.1033\n",
      "Epoch 1200/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8607 - loss: 0.3426 - mse: 0.1033\n",
      "Epoch 1201/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8607 - loss: 0.3426 - mse: 0.1033\n",
      "Epoch 1202/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8607 - loss: 0.3426 - mse: 0.1033\n",
      "Epoch 1203/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8607 - loss: 0.3426 - mse: 0.1033\n",
      "Epoch 1204/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8607 - loss: 0.3426 - mse: 0.1033\n",
      "Epoch 1205/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8607 - loss: 0.3425 - mse: 0.1033\n",
      "Epoch 1206/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8607 - loss: 0.3425 - mse: 0.1033\n",
      "Epoch 1207/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8607 - loss: 0.3425 - mse: 0.1033\n",
      "Epoch 1208/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8607 - loss: 0.3425 - mse: 0.1033\n",
      "Epoch 1209/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8607 - loss: 0.3425 - mse: 0.1033\n",
      "Epoch 1210/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8607 - loss: 0.3424 - mse: 0.1033\n",
      "Epoch 1211/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8607 - loss: 0.3424 - mse: 0.1033\n",
      "Epoch 1212/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8607 - loss: 0.3424 - mse: 0.1033\n",
      "Epoch 1213/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8607 - loss: 0.3424 - mse: 0.1032\n",
      "Epoch 1214/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8607 - loss: 0.3424 - mse: 0.1032\n",
      "Epoch 1215/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8607 - loss: 0.3423 - mse: 0.1032\n",
      "Epoch 1216/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8607 - loss: 0.3423 - mse: 0.1032\n",
      "Epoch 1217/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8613 - loss: 0.3423 - mse: 0.1032\n",
      "Epoch 1218/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8613 - loss: 0.3423 - mse: 0.1032\n",
      "Epoch 1219/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8613 - loss: 0.3423 - mse: 0.1032\n",
      "Epoch 1220/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8613 - loss: 0.3422 - mse: 0.1032\n",
      "Epoch 1221/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8613 - loss: 0.3422 - mse: 0.1032\n",
      "Epoch 1222/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8613 - loss: 0.3422 - mse: 0.1032\n",
      "Epoch 1223/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8613 - loss: 0.3422 - mse: 0.1032\n",
      "Epoch 1224/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8613 - loss: 0.3422 - mse: 0.1032\n",
      "Epoch 1225/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8613 - loss: 0.3421 - mse: 0.1032\n",
      "Epoch 1226/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8613 - loss: 0.3421 - mse: 0.1032\n",
      "Epoch 1227/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8613 - loss: 0.3421 - mse: 0.1032\n",
      "Epoch 1228/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8613 - loss: 0.3421 - mse: 0.1032\n",
      "Epoch 1229/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8613 - loss: 0.3421 - mse: 0.1031\n",
      "Epoch 1230/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8613 - loss: 0.3420 - mse: 0.1031\n",
      "Epoch 1231/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8613 - loss: 0.3420 - mse: 0.1031\n",
      "Epoch 1232/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8613 - loss: 0.3420 - mse: 0.1031\n",
      "Epoch 1233/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8613 - loss: 0.3420 - mse: 0.1031\n",
      "Epoch 1234/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8613 - loss: 0.3420 - mse: 0.1031\n",
      "Epoch 1235/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8613 - loss: 0.3419 - mse: 0.1031\n",
      "Epoch 1236/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8613 - loss: 0.3419 - mse: 0.1031\n",
      "Epoch 1237/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8613 - loss: 0.3419 - mse: 0.1031\n",
      "Epoch 1238/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8619 - loss: 0.3419 - mse: 0.1031\n",
      "Epoch 1239/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8619 - loss: 0.3419 - mse: 0.1031\n",
      "Epoch 1240/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8624 - loss: 0.3418 - mse: 0.1031\n",
      "Epoch 1241/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8624 - loss: 0.3418 - mse: 0.1031\n",
      "Epoch 1242/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8624 - loss: 0.3418 - mse: 0.1031\n",
      "Epoch 1243/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8624 - loss: 0.3418 - mse: 0.1031\n",
      "Epoch 1244/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8624 - loss: 0.3418 - mse: 0.1031\n",
      "Epoch 1245/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8624 - loss: 0.3417 - mse: 0.1031\n",
      "Epoch 1246/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8624 - loss: 0.3417 - mse: 0.1030\n",
      "Epoch 1247/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8624 - loss: 0.3417 - mse: 0.1030\n",
      "Epoch 1248/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8630 - loss: 0.3417 - mse: 0.1030\n",
      "Epoch 1249/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8630 - loss: 0.3417 - mse: 0.1030\n",
      "Epoch 1250/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8630 - loss: 0.3417 - mse: 0.1030\n",
      "Epoch 1251/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8630 - loss: 0.3416 - mse: 0.1030\n",
      "Epoch 1252/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8630 - loss: 0.3416 - mse: 0.1030\n",
      "Epoch 1253/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8630 - loss: 0.3416 - mse: 0.1030\n",
      "Epoch 1254/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8630 - loss: 0.3416 - mse: 0.1030\n",
      "Epoch 1255/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8630 - loss: 0.3416 - mse: 0.1030\n",
      "Epoch 1256/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8630 - loss: 0.3415 - mse: 0.1030\n",
      "Epoch 1257/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8630 - loss: 0.3415 - mse: 0.1030\n",
      "Epoch 1258/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8630 - loss: 0.3415 - mse: 0.1030\n",
      "Epoch 1259/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8630 - loss: 0.3415 - mse: 0.1030\n",
      "Epoch 1260/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.8630 - loss: 0.3415 - mse: 0.1030\n",
      "Epoch 1261/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8630 - loss: 0.3414 - mse: 0.1030\n",
      "Epoch 1262/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8630 - loss: 0.3414 - mse: 0.1030\n",
      "Epoch 1263/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8630 - loss: 0.3414 - mse: 0.1029\n",
      "Epoch 1264/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8630 - loss: 0.3414 - mse: 0.1029\n",
      "Epoch 1265/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8630 - loss: 0.3414 - mse: 0.1029\n",
      "Epoch 1266/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8630 - loss: 0.3413 - mse: 0.1029\n",
      "Epoch 1267/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8630 - loss: 0.3413 - mse: 0.1029\n",
      "Epoch 1268/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8630 - loss: 0.3413 - mse: 0.1029\n",
      "Epoch 1269/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8630 - loss: 0.3413 - mse: 0.1029\n",
      "Epoch 1270/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8630 - loss: 0.3413 - mse: 0.1029\n",
      "Epoch 1271/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8630 - loss: 0.3412 - mse: 0.1029\n",
      "Epoch 1272/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.8630 - loss: 0.3412 - mse: 0.1029\n",
      "Epoch 1273/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.8630 - loss: 0.3412 - mse: 0.1029\n",
      "Epoch 1274/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.8630 - loss: 0.3412 - mse: 0.1029\n",
      "Epoch 1275/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8630 - loss: 0.3412 - mse: 0.1029\n",
      "Epoch 1276/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8630 - loss: 0.3411 - mse: 0.1029\n",
      "Epoch 1277/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8630 - loss: 0.3411 - mse: 0.1029\n",
      "Epoch 1278/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8630 - loss: 0.3411 - mse: 0.1029\n",
      "Epoch 1279/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8630 - loss: 0.3411 - mse: 0.1028\n",
      "Epoch 1280/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8630 - loss: 0.3411 - mse: 0.1028\n",
      "Epoch 1281/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8630 - loss: 0.3410 - mse: 0.1028\n",
      "Epoch 1282/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.8630 - loss: 0.3410 - mse: 0.1028\n",
      "Epoch 1283/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8636 - loss: 0.3410 - mse: 0.1028\n",
      "Epoch 1284/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8636 - loss: 0.3410 - mse: 0.1028\n",
      "Epoch 1285/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8636 - loss: 0.3410 - mse: 0.1028\n",
      "Epoch 1286/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8636 - loss: 0.3410 - mse: 0.1028\n",
      "Epoch 1287/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 0.8636 - loss: 0.3409 - mse: 0.1028\n",
      "Epoch 1288/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8636 - loss: 0.3409 - mse: 0.1028\n",
      "Epoch 1289/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8636 - loss: 0.3409 - mse: 0.1028\n",
      "Epoch 1290/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.8636 - loss: 0.3409 - mse: 0.1028\n",
      "Epoch 1291/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8636 - loss: 0.3409 - mse: 0.1028\n",
      "Epoch 1292/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8636 - loss: 0.3408 - mse: 0.1028\n",
      "Epoch 1293/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8636 - loss: 0.3408 - mse: 0.1028\n",
      "Epoch 1294/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8636 - loss: 0.3408 - mse: 0.1028\n",
      "Epoch 1295/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8636 - loss: 0.3408 - mse: 0.1028\n",
      "Epoch 1296/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8636 - loss: 0.3408 - mse: 0.1027\n",
      "Epoch 1297/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8636 - loss: 0.3407 - mse: 0.1027\n",
      "Epoch 1298/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8636 - loss: 0.3407 - mse: 0.1027\n",
      "Epoch 1299/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8636 - loss: 0.3407 - mse: 0.1027\n",
      "Epoch 1300/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8636 - loss: 0.3407 - mse: 0.1027\n",
      "Epoch 1301/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8636 - loss: 0.3407 - mse: 0.1027\n",
      "Epoch 1302/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8636 - loss: 0.3406 - mse: 0.1027\n",
      "Epoch 1303/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8636 - loss: 0.3406 - mse: 0.1027\n",
      "Epoch 1304/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8642 - loss: 0.3406 - mse: 0.1027\n",
      "Epoch 1305/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8642 - loss: 0.3406 - mse: 0.1027\n",
      "Epoch 1306/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8636 - loss: 0.3406 - mse: 0.1027\n",
      "Epoch 1307/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8636 - loss: 0.3405 - mse: 0.1027\n",
      "Epoch 1308/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8636 - loss: 0.3405 - mse: 0.1027\n",
      "Epoch 1309/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8636 - loss: 0.3405 - mse: 0.1027\n",
      "Epoch 1310/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.8636 - loss: 0.3405 - mse: 0.1027\n",
      "Epoch 1311/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8636 - loss: 0.3405 - mse: 0.1027\n",
      "Epoch 1312/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8636 - loss: 0.3405 - mse: 0.1027\n",
      "Epoch 1313/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8636 - loss: 0.3404 - mse: 0.1026\n",
      "Epoch 1314/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8636 - loss: 0.3404 - mse: 0.1026\n",
      "Epoch 1315/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8636 - loss: 0.3404 - mse: 0.1026\n",
      "Epoch 1316/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8636 - loss: 0.3404 - mse: 0.1026\n",
      "Epoch 1317/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.8636 - loss: 0.3404 - mse: 0.1026\n",
      "Epoch 1318/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8636 - loss: 0.3403 - mse: 0.1026\n",
      "Epoch 1319/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8636 - loss: 0.3403 - mse: 0.1026\n",
      "Epoch 1320/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8636 - loss: 0.3403 - mse: 0.1026\n",
      "Epoch 1321/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8636 - loss: 0.3403 - mse: 0.1026\n",
      "Epoch 1322/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8636 - loss: 0.3403 - mse: 0.1026\n",
      "Epoch 1323/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8636 - loss: 0.3402 - mse: 0.1026\n",
      "Epoch 1324/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8636 - loss: 0.3402 - mse: 0.1026\n",
      "Epoch 1325/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8636 - loss: 0.3402 - mse: 0.1026\n",
      "Epoch 1326/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8636 - loss: 0.3402 - mse: 0.1026\n",
      "Epoch 1327/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8636 - loss: 0.3402 - mse: 0.1026\n",
      "Epoch 1328/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8636 - loss: 0.3401 - mse: 0.1026\n",
      "Epoch 1329/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8636 - loss: 0.3401 - mse: 0.1026\n",
      "Epoch 1330/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8636 - loss: 0.3401 - mse: 0.1026\n",
      "Epoch 1331/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8636 - loss: 0.3401 - mse: 0.1026\n",
      "Epoch 1332/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8636 - loss: 0.3401 - mse: 0.1025\n",
      "Epoch 1333/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8636 - loss: 0.3400 - mse: 0.1025\n",
      "Epoch 1334/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8636 - loss: 0.3400 - mse: 0.1025\n",
      "Epoch 1335/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8636 - loss: 0.3400 - mse: 0.1025\n",
      "Epoch 1336/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.8636 - loss: 0.3400 - mse: 0.1025\n",
      "Epoch 1337/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.8636 - loss: 0.3400 - mse: 0.1025\n",
      "Epoch 1338/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8636 - loss: 0.3399 - mse: 0.1025\n",
      "Epoch 1339/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8636 - loss: 0.3399 - mse: 0.1025\n",
      "Epoch 1340/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8636 - loss: 0.3399 - mse: 0.1025\n",
      "Epoch 1341/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8636 - loss: 0.3399 - mse: 0.1025\n",
      "Epoch 1342/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8636 - loss: 0.3399 - mse: 0.1025\n",
      "Epoch 1343/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8636 - loss: 0.3399 - mse: 0.1025\n",
      "Epoch 1344/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.8636 - loss: 0.3398 - mse: 0.1025\n",
      "Epoch 1345/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8636 - loss: 0.3398 - mse: 0.1025\n",
      "Epoch 1346/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8636 - loss: 0.3398 - mse: 0.1025\n",
      "Epoch 1347/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8636 - loss: 0.3398 - mse: 0.1025\n",
      "Epoch 1348/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8636 - loss: 0.3398 - mse: 0.1025\n",
      "Epoch 1349/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8636 - loss: 0.3397 - mse: 0.1024\n",
      "Epoch 1350/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8636 - loss: 0.3397 - mse: 0.1024\n",
      "Epoch 1351/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8636 - loss: 0.3397 - mse: 0.1024\n",
      "Epoch 1352/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8636 - loss: 0.3397 - mse: 0.1024\n",
      "Epoch 1353/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8636 - loss: 0.3397 - mse: 0.1024\n",
      "Epoch 1354/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8636 - loss: 0.3396 - mse: 0.1024\n",
      "Epoch 1355/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.8636 - loss: 0.3396 - mse: 0.1024\n",
      "Epoch 1356/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.8636 - loss: 0.3396 - mse: 0.1024\n",
      "Epoch 1357/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8636 - loss: 0.3396 - mse: 0.1024\n",
      "Epoch 1358/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8636 - loss: 0.3396 - mse: 0.1024\n",
      "Epoch 1359/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8636 - loss: 0.3395 - mse: 0.1024\n",
      "Epoch 1360/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8636 - loss: 0.3395 - mse: 0.1024\n",
      "Epoch 1361/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.8636 - loss: 0.3395 - mse: 0.1024\n",
      "Epoch 1362/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8636 - loss: 0.3395 - mse: 0.1024\n",
      "Epoch 1363/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8636 - loss: 0.3395 - mse: 0.1024\n",
      "Epoch 1364/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8636 - loss: 0.3395 - mse: 0.1024\n",
      "Epoch 1365/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8636 - loss: 0.3394 - mse: 0.1024\n",
      "Epoch 1366/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8636 - loss: 0.3394 - mse: 0.1023\n",
      "Epoch 1367/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8636 - loss: 0.3394 - mse: 0.1023\n",
      "Epoch 1368/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.8636 - loss: 0.3394 - mse: 0.1023\n",
      "Epoch 1369/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.8636 - loss: 0.3394 - mse: 0.1023\n",
      "Epoch 1370/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8636 - loss: 0.3393 - mse: 0.1023\n",
      "Epoch 1371/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8636 - loss: 0.3393 - mse: 0.1023\n",
      "Epoch 1372/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8636 - loss: 0.3393 - mse: 0.1023\n",
      "Epoch 1373/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - accuracy: 0.8636 - loss: 0.3393 - mse: 0.1023\n",
      "Epoch 1374/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8636 - loss: 0.3393 - mse: 0.1023\n",
      "Epoch 1375/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8636 - loss: 0.3392 - mse: 0.1023\n",
      "Epoch 1376/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8636 - loss: 0.3392 - mse: 0.1023\n",
      "Epoch 1377/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8636 - loss: 0.3392 - mse: 0.1023\n",
      "Epoch 1378/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8636 - loss: 0.3392 - mse: 0.1023\n",
      "Epoch 1379/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8636 - loss: 0.3392 - mse: 0.1023\n",
      "Epoch 1380/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8636 - loss: 0.3391 - mse: 0.1023\n",
      "Epoch 1381/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.8636 - loss: 0.3391 - mse: 0.1023\n",
      "Epoch 1382/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - accuracy: 0.8636 - loss: 0.3391 - mse: 0.1023\n",
      "Epoch 1383/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.8636 - loss: 0.3391 - mse: 0.1022\n",
      "Epoch 1384/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.8636 - loss: 0.3391 - mse: 0.1022\n",
      "Epoch 1385/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8636 - loss: 0.3391 - mse: 0.1022\n",
      "Epoch 1386/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.8642 - loss: 0.3390 - mse: 0.1022\n",
      "Epoch 1387/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8636 - loss: 0.3390 - mse: 0.1022\n",
      "Epoch 1388/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8642 - loss: 0.3390 - mse: 0.1022\n",
      "Epoch 1389/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8642 - loss: 0.3390 - mse: 0.1022\n",
      "Epoch 1390/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8636 - loss: 0.3390 - mse: 0.1022\n",
      "Epoch 1391/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.8636 - loss: 0.3389 - mse: 0.1022\n",
      "Epoch 1392/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - accuracy: 0.8642 - loss: 0.3389 - mse: 0.1022\n",
      "Epoch 1393/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.8636 - loss: 0.3389 - mse: 0.1022\n",
      "Epoch 1394/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8642 - loss: 0.3389 - mse: 0.1022\n",
      "Epoch 1395/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.8642 - loss: 0.3389 - mse: 0.1022\n",
      "Epoch 1396/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8636 - loss: 0.3388 - mse: 0.1022\n",
      "Epoch 1397/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 0.8642 - loss: 0.3388 - mse: 0.1022\n",
      "Epoch 1398/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8642 - loss: 0.3388 - mse: 0.1022\n",
      "Epoch 1399/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.8642 - loss: 0.3388 - mse: 0.1022\n",
      "Epoch 1400/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8642 - loss: 0.3388 - mse: 0.1022\n",
      "Epoch 1401/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8642 - loss: 0.3387 - mse: 0.1021\n",
      "Epoch 1402/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.8636 - loss: 0.3387 - mse: 0.1021\n",
      "Epoch 1403/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.8642 - loss: 0.3387 - mse: 0.1021\n",
      "Epoch 1404/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8642 - loss: 0.3387 - mse: 0.1021\n",
      "Epoch 1405/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8642 - loss: 0.3387 - mse: 0.1021\n",
      "Epoch 1406/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8642 - loss: 0.3387 - mse: 0.1021\n",
      "Epoch 1407/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8642 - loss: 0.3386 - mse: 0.1021\n",
      "Epoch 1408/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8642 - loss: 0.3386 - mse: 0.1021\n",
      "Epoch 1409/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8642 - loss: 0.3386 - mse: 0.1021\n",
      "Epoch 1410/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8642 - loss: 0.3386 - mse: 0.1021\n",
      "Epoch 1411/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8642 - loss: 0.3386 - mse: 0.1021\n",
      "Epoch 1412/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8642 - loss: 0.3385 - mse: 0.1021\n",
      "Epoch 1413/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8642 - loss: 0.3385 - mse: 0.1021\n",
      "Epoch 1414/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8642 - loss: 0.3385 - mse: 0.1021\n",
      "Epoch 1415/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8642 - loss: 0.3385 - mse: 0.1021\n",
      "Epoch 1416/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8642 - loss: 0.3385 - mse: 0.1021\n",
      "Epoch 1417/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8642 - loss: 0.3384 - mse: 0.1021\n",
      "Epoch 1418/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8642 - loss: 0.3384 - mse: 0.1020\n",
      "Epoch 1419/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.8642 - loss: 0.3384 - mse: 0.1020\n",
      "Epoch 1420/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8642 - loss: 0.3384 - mse: 0.1020\n",
      "Epoch 1421/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8642 - loss: 0.3384 - mse: 0.1020\n",
      "Epoch 1422/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8642 - loss: 0.3383 - mse: 0.1020\n",
      "Epoch 1423/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8642 - loss: 0.3383 - mse: 0.1020\n",
      "Epoch 1424/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8642 - loss: 0.3383 - mse: 0.1020\n",
      "Epoch 1425/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8642 - loss: 0.3383 - mse: 0.1020\n",
      "Epoch 1426/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8642 - loss: 0.3383 - mse: 0.1020\n",
      "Epoch 1427/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8642 - loss: 0.3383 - mse: 0.1020\n",
      "Epoch 1428/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8642 - loss: 0.3382 - mse: 0.1020\n",
      "Epoch 1429/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8642 - loss: 0.3382 - mse: 0.1020\n",
      "Epoch 1430/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8642 - loss: 0.3382 - mse: 0.1020\n",
      "Epoch 1431/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8642 - loss: 0.3382 - mse: 0.1020\n",
      "Epoch 1432/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8642 - loss: 0.3382 - mse: 0.1020\n",
      "Epoch 1433/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8642 - loss: 0.3381 - mse: 0.1020\n",
      "Epoch 1434/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8642 - loss: 0.3381 - mse: 0.1020\n",
      "Epoch 1435/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8647 - loss: 0.3381 - mse: 0.1020\n",
      "Epoch 1436/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8647 - loss: 0.3381 - mse: 0.1019\n",
      "Epoch 1437/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8647 - loss: 0.3381 - mse: 0.1019\n",
      "Epoch 1438/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8647 - loss: 0.3380 - mse: 0.1019\n",
      "Epoch 1439/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8647 - loss: 0.3380 - mse: 0.1019\n",
      "Epoch 1440/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8647 - loss: 0.3380 - mse: 0.1019\n",
      "Epoch 1441/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8647 - loss: 0.3380 - mse: 0.1019\n",
      "Epoch 1442/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8647 - loss: 0.3380 - mse: 0.1019\n",
      "Epoch 1443/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8647 - loss: 0.3379 - mse: 0.1019\n",
      "Epoch 1444/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 0.8647 - loss: 0.3379 - mse: 0.1019\n",
      "Epoch 1445/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.8647 - loss: 0.3379 - mse: 0.1019\n",
      "Epoch 1446/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8647 - loss: 0.3379 - mse: 0.1019\n",
      "Epoch 1447/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8647 - loss: 0.3379 - mse: 0.1019\n",
      "Epoch 1448/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8647 - loss: 0.3379 - mse: 0.1019\n",
      "Epoch 1449/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8647 - loss: 0.3378 - mse: 0.1019\n",
      "Epoch 1450/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.8647 - loss: 0.3378 - mse: 0.1019\n",
      "Epoch 1451/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.8647 - loss: 0.3378 - mse: 0.1019\n",
      "Epoch 1452/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8647 - loss: 0.3378 - mse: 0.1019\n",
      "Epoch 1453/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.8647 - loss: 0.3378 - mse: 0.1018\n",
      "Epoch 1454/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 0.8647 - loss: 0.3377 - mse: 0.1018\n",
      "Epoch 1455/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8647 - loss: 0.3377 - mse: 0.1018\n",
      "Epoch 1456/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 0.8647 - loss: 0.3377 - mse: 0.1018\n",
      "Epoch 1457/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - accuracy: 0.8647 - loss: 0.3377 - mse: 0.1018\n",
      "Epoch 1458/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8647 - loss: 0.3377 - mse: 0.1018\n",
      "Epoch 1459/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8647 - loss: 0.3376 - mse: 0.1018\n",
      "Epoch 1460/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8647 - loss: 0.3376 - mse: 0.1018\n",
      "Epoch 1461/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.8647 - loss: 0.3376 - mse: 0.1018\n",
      "Epoch 1462/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8647 - loss: 0.3376 - mse: 0.1018\n",
      "Epoch 1463/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8647 - loss: 0.3376 - mse: 0.1018\n",
      "Epoch 1464/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8647 - loss: 0.3376 - mse: 0.1018\n",
      "Epoch 1465/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8647 - loss: 0.3375 - mse: 0.1018\n",
      "Epoch 1466/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8647 - loss: 0.3375 - mse: 0.1018\n",
      "Epoch 1467/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8647 - loss: 0.3375 - mse: 0.1018\n",
      "Epoch 1468/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8647 - loss: 0.3375 - mse: 0.1018\n",
      "Epoch 1469/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8647 - loss: 0.3375 - mse: 0.1018\n",
      "Epoch 1470/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.8647 - loss: 0.3374 - mse: 0.1017\n",
      "Epoch 1471/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8647 - loss: 0.3374 - mse: 0.1017\n",
      "Epoch 1472/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8647 - loss: 0.3374 - mse: 0.1017\n",
      "Epoch 1473/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8647 - loss: 0.3374 - mse: 0.1017\n",
      "Epoch 1474/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.8647 - loss: 0.3374 - mse: 0.1017\n",
      "Epoch 1475/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8647 - loss: 0.3373 - mse: 0.1017\n",
      "Epoch 1476/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8647 - loss: 0.3373 - mse: 0.1017\n",
      "Epoch 1477/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8647 - loss: 0.3373 - mse: 0.1017\n",
      "Epoch 1478/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8647 - loss: 0.3373 - mse: 0.1017\n",
      "Epoch 1479/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8647 - loss: 0.3373 - mse: 0.1017\n",
      "Epoch 1480/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8647 - loss: 0.3373 - mse: 0.1017\n",
      "Epoch 1481/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8647 - loss: 0.3372 - mse: 0.1017\n",
      "Epoch 1482/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8647 - loss: 0.3372 - mse: 0.1017\n",
      "Epoch 1483/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8647 - loss: 0.3372 - mse: 0.1017\n",
      "Epoch 1484/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8647 - loss: 0.3372 - mse: 0.1017\n",
      "Epoch 1485/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8647 - loss: 0.3372 - mse: 0.1017\n",
      "Epoch 1486/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8647 - loss: 0.3371 - mse: 0.1017\n",
      "Epoch 1487/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8647 - loss: 0.3371 - mse: 0.1016\n",
      "Epoch 1488/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8647 - loss: 0.3371 - mse: 0.1016\n",
      "Epoch 1489/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8647 - loss: 0.3371 - mse: 0.1016\n",
      "Epoch 1490/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8647 - loss: 0.3371 - mse: 0.1016\n",
      "Epoch 1491/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8647 - loss: 0.3370 - mse: 0.1016\n",
      "Epoch 1492/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8647 - loss: 0.3370 - mse: 0.1016\n",
      "Epoch 1493/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8647 - loss: 0.3370 - mse: 0.1016\n",
      "Epoch 1494/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8647 - loss: 0.3370 - mse: 0.1016\n",
      "Epoch 1495/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.8647 - loss: 0.3370 - mse: 0.1016\n",
      "Epoch 1496/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8647 - loss: 0.3370 - mse: 0.1016\n",
      "Epoch 1497/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8647 - loss: 0.3369 - mse: 0.1016\n",
      "Epoch 1498/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8642 - loss: 0.3369 - mse: 0.1016\n",
      "Epoch 1499/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8642 - loss: 0.3369 - mse: 0.1016\n",
      "Epoch 1500/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8642 - loss: 0.3369 - mse: 0.1016\n",
      "Epoch 1501/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.8642 - loss: 0.3369 - mse: 0.1016\n",
      "Epoch 1502/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8642 - loss: 0.3368 - mse: 0.1016\n",
      "Epoch 1503/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8642 - loss: 0.3368 - mse: 0.1016\n",
      "Epoch 1504/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8642 - loss: 0.3368 - mse: 0.1016\n",
      "Epoch 1505/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8642 - loss: 0.3368 - mse: 0.1015\n",
      "Epoch 1506/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8642 - loss: 0.3368 - mse: 0.1015\n",
      "Epoch 1507/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8642 - loss: 0.3368 - mse: 0.1015\n",
      "Epoch 1508/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8642 - loss: 0.3367 - mse: 0.1015\n",
      "Epoch 1509/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8642 - loss: 0.3367 - mse: 0.1015\n",
      "Epoch 1510/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8642 - loss: 0.3367 - mse: 0.1015\n",
      "Epoch 1511/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8642 - loss: 0.3367 - mse: 0.1015\n",
      "Epoch 1512/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8642 - loss: 0.3367 - mse: 0.1015\n",
      "Epoch 1513/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8642 - loss: 0.3366 - mse: 0.1015\n",
      "Epoch 1514/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8642 - loss: 0.3366 - mse: 0.1015\n",
      "Epoch 1515/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8642 - loss: 0.3366 - mse: 0.1015\n",
      "Epoch 1516/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8642 - loss: 0.3366 - mse: 0.1015\n",
      "Epoch 1517/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8642 - loss: 0.3366 - mse: 0.1015\n",
      "Epoch 1518/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8642 - loss: 0.3365 - mse: 0.1015\n",
      "Epoch 1519/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8642 - loss: 0.3365 - mse: 0.1015\n",
      "Epoch 1520/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8642 - loss: 0.3365 - mse: 0.1015\n",
      "Epoch 1521/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8642 - loss: 0.3365 - mse: 0.1015\n",
      "Epoch 1522/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8642 - loss: 0.3365 - mse: 0.1015\n",
      "Epoch 1523/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8642 - loss: 0.3365 - mse: 0.1014\n",
      "Epoch 1524/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8642 - loss: 0.3364 - mse: 0.1014\n",
      "Epoch 1525/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8642 - loss: 0.3364 - mse: 0.1014\n",
      "Epoch 1526/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8642 - loss: 0.3364 - mse: 0.1014\n",
      "Epoch 1527/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8642 - loss: 0.3364 - mse: 0.1014\n",
      "Epoch 1528/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8642 - loss: 0.3364 - mse: 0.1014\n",
      "Epoch 1529/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8642 - loss: 0.3363 - mse: 0.1014\n",
      "Epoch 1530/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8642 - loss: 0.3363 - mse: 0.1014\n",
      "Epoch 1531/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8642 - loss: 0.3363 - mse: 0.1014\n",
      "Epoch 1532/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8642 - loss: 0.3363 - mse: 0.1014\n",
      "Epoch 1533/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8636 - loss: 0.3363 - mse: 0.1014\n",
      "Epoch 1534/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8642 - loss: 0.3363 - mse: 0.1014\n",
      "Epoch 1535/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8642 - loss: 0.3362 - mse: 0.1014\n",
      "Epoch 1536/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8636 - loss: 0.3362 - mse: 0.1014\n",
      "Epoch 1537/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8642 - loss: 0.3362 - mse: 0.1014\n",
      "Epoch 1538/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8636 - loss: 0.3362 - mse: 0.1014\n",
      "Epoch 1539/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8636 - loss: 0.3362 - mse: 0.1014\n",
      "Epoch 1540/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8636 - loss: 0.3361 - mse: 0.1014\n",
      "Epoch 1541/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8636 - loss: 0.3361 - mse: 0.1013\n",
      "Epoch 1542/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8636 - loss: 0.3361 - mse: 0.1013\n",
      "Epoch 1543/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.8636 - loss: 0.3361 - mse: 0.1013\n",
      "Epoch 1544/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8636 - loss: 0.3361 - mse: 0.1013\n",
      "Epoch 1545/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8636 - loss: 0.3360 - mse: 0.1013\n",
      "Epoch 1546/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8636 - loss: 0.3360 - mse: 0.1013\n",
      "Epoch 1547/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8636 - loss: 0.3360 - mse: 0.1013\n",
      "Epoch 1548/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8636 - loss: 0.3360 - mse: 0.1013\n",
      "Epoch 1549/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8636 - loss: 0.3360 - mse: 0.1013\n",
      "Epoch 1550/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8636 - loss: 0.3360 - mse: 0.1013\n",
      "Epoch 1551/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8636 - loss: 0.3359 - mse: 0.1013\n",
      "Epoch 1552/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8636 - loss: 0.3359 - mse: 0.1013\n",
      "Epoch 1553/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8636 - loss: 0.3359 - mse: 0.1013\n",
      "Epoch 1554/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8636 - loss: 0.3359 - mse: 0.1013\n",
      "Epoch 1555/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8636 - loss: 0.3359 - mse: 0.1013\n",
      "Epoch 1556/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8636 - loss: 0.3358 - mse: 0.1013\n",
      "Epoch 1557/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8636 - loss: 0.3358 - mse: 0.1013\n",
      "Epoch 1558/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8636 - loss: 0.3358 - mse: 0.1012\n",
      "Epoch 1559/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8636 - loss: 0.3358 - mse: 0.1012\n",
      "Epoch 1560/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8636 - loss: 0.3358 - mse: 0.1012\n",
      "Epoch 1561/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8636 - loss: 0.3358 - mse: 0.1012\n",
      "Epoch 1562/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8636 - loss: 0.3357 - mse: 0.1012\n",
      "Epoch 1563/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8636 - loss: 0.3357 - mse: 0.1012\n",
      "Epoch 1564/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8636 - loss: 0.3357 - mse: 0.1012\n",
      "Epoch 1565/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3357 - mse: 0.1012\n",
      "Epoch 1566/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8636 - loss: 0.3357 - mse: 0.1012\n",
      "Epoch 1567/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8636 - loss: 0.3356 - mse: 0.1012\n",
      "Epoch 1568/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8636 - loss: 0.3356 - mse: 0.1012\n",
      "Epoch 1569/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8636 - loss: 0.3356 - mse: 0.1012\n",
      "Epoch 1570/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8636 - loss: 0.3356 - mse: 0.1012\n",
      "Epoch 1571/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3356 - mse: 0.1012\n",
      "Epoch 1572/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8636 - loss: 0.3356 - mse: 0.1012\n",
      "Epoch 1573/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8636 - loss: 0.3355 - mse: 0.1012\n",
      "Epoch 1574/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8636 - loss: 0.3355 - mse: 0.1012\n",
      "Epoch 1575/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8636 - loss: 0.3355 - mse: 0.1012\n",
      "Epoch 1576/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8636 - loss: 0.3355 - mse: 0.1011\n",
      "Epoch 1577/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8636 - loss: 0.3355 - mse: 0.1011\n",
      "Epoch 1578/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8636 - loss: 0.3354 - mse: 0.1011\n",
      "Epoch 1579/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8636 - loss: 0.3354 - mse: 0.1011\n",
      "Epoch 1580/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8636 - loss: 0.3354 - mse: 0.1011\n",
      "Epoch 1581/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8636 - loss: 0.3354 - mse: 0.1011\n",
      "Epoch 1582/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8636 - loss: 0.3354 - mse: 0.1011\n",
      "Epoch 1583/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8636 - loss: 0.3354 - mse: 0.1011\n",
      "Epoch 1584/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8636 - loss: 0.3353 - mse: 0.1011\n",
      "Epoch 1585/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8636 - loss: 0.3353 - mse: 0.1011\n",
      "Epoch 1586/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8636 - loss: 0.3353 - mse: 0.1011\n",
      "Epoch 1587/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8636 - loss: 0.3353 - mse: 0.1011\n",
      "Epoch 1588/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8636 - loss: 0.3353 - mse: 0.1011\n",
      "Epoch 1589/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8636 - loss: 0.3352 - mse: 0.1011\n",
      "Epoch 1590/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8636 - loss: 0.3352 - mse: 0.1011\n",
      "Epoch 1591/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8636 - loss: 0.3352 - mse: 0.1011\n",
      "Epoch 1592/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8636 - loss: 0.3352 - mse: 0.1011\n",
      "Epoch 1593/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8636 - loss: 0.3352 - mse: 0.1011\n",
      "Epoch 1594/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8636 - loss: 0.3352 - mse: 0.1010\n",
      "Epoch 1595/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.8636 - loss: 0.3351 - mse: 0.1010\n",
      "Epoch 1596/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3351 - mse: 0.1010\n",
      "Epoch 1597/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3351 - mse: 0.1010\n",
      "Epoch 1598/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8636 - loss: 0.3351 - mse: 0.1010\n",
      "Epoch 1599/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8636 - loss: 0.3351 - mse: 0.1010\n",
      "Epoch 1600/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.8636 - loss: 0.3350 - mse: 0.1010\n",
      "Epoch 1601/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8636 - loss: 0.3350 - mse: 0.1010\n",
      "Epoch 1602/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8636 - loss: 0.3350 - mse: 0.1010\n",
      "Epoch 1603/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3350 - mse: 0.1010\n",
      "Epoch 1604/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8636 - loss: 0.3350 - mse: 0.1010\n",
      "Epoch 1605/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8636 - loss: 0.3350 - mse: 0.1010\n",
      "Epoch 1606/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8636 - loss: 0.3349 - mse: 0.1010\n",
      "Epoch 1607/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8636 - loss: 0.3349 - mse: 0.1010\n",
      "Epoch 1608/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8636 - loss: 0.3349 - mse: 0.1010\n",
      "Epoch 1609/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8636 - loss: 0.3349 - mse: 0.1010\n",
      "Epoch 1610/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8636 - loss: 0.3349 - mse: 0.1010\n",
      "Epoch 1611/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8636 - loss: 0.3348 - mse: 0.1010\n",
      "Epoch 1612/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8636 - loss: 0.3348 - mse: 0.1009\n",
      "Epoch 1613/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8636 - loss: 0.3348 - mse: 0.1009\n",
      "Epoch 1614/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3348 - mse: 0.1009\n",
      "Epoch 1615/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8636 - loss: 0.3348 - mse: 0.1009\n",
      "Epoch 1616/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8636 - loss: 0.3348 - mse: 0.1009\n",
      "Epoch 1617/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3347 - mse: 0.1009\n",
      "Epoch 1618/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8636 - loss: 0.3347 - mse: 0.1009\n",
      "Epoch 1619/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8636 - loss: 0.3347 - mse: 0.1009\n",
      "Epoch 1620/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8636 - loss: 0.3347 - mse: 0.1009\n",
      "Epoch 1621/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8636 - loss: 0.3347 - mse: 0.1009\n",
      "Epoch 1622/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8636 - loss: 0.3346 - mse: 0.1009\n",
      "Epoch 1623/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8636 - loss: 0.3346 - mse: 0.1009\n",
      "Epoch 1624/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8636 - loss: 0.3346 - mse: 0.1009\n",
      "Epoch 1625/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3346 - mse: 0.1009\n",
      "Epoch 1626/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8636 - loss: 0.3346 - mse: 0.1009\n",
      "Epoch 1627/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8636 - loss: 0.3346 - mse: 0.1009\n",
      "Epoch 1628/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8636 - loss: 0.3345 - mse: 0.1009\n",
      "Epoch 1629/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8636 - loss: 0.3345 - mse: 0.1009\n",
      "Epoch 1630/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8636 - loss: 0.3345 - mse: 0.1008\n",
      "Epoch 1631/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8636 - loss: 0.3345 - mse: 0.1008\n",
      "Epoch 1632/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8636 - loss: 0.3345 - mse: 0.1008\n",
      "Epoch 1633/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8636 - loss: 0.3345 - mse: 0.1008\n",
      "Epoch 1634/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.8636 - loss: 0.3344 - mse: 0.1008\n",
      "Epoch 1635/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8636 - loss: 0.3344 - mse: 0.1008\n",
      "Epoch 1636/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8636 - loss: 0.3344 - mse: 0.1008\n",
      "Epoch 1637/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8636 - loss: 0.3344 - mse: 0.1008\n",
      "Epoch 1638/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8636 - loss: 0.3344 - mse: 0.1008\n",
      "Epoch 1639/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8636 - loss: 0.3343 - mse: 0.1008\n",
      "Epoch 1640/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.8636 - loss: 0.3343 - mse: 0.1008\n",
      "Epoch 1641/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8636 - loss: 0.3343 - mse: 0.1008\n",
      "Epoch 1642/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8636 - loss: 0.3343 - mse: 0.1008\n",
      "Epoch 1643/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8636 - loss: 0.3343 - mse: 0.1008\n",
      "Epoch 1644/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8636 - loss: 0.3343 - mse: 0.1008\n",
      "Epoch 1645/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8636 - loss: 0.3342 - mse: 0.1008\n",
      "Epoch 1646/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8636 - loss: 0.3342 - mse: 0.1008\n",
      "Epoch 1647/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8636 - loss: 0.3342 - mse: 0.1008\n",
      "Epoch 1648/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8636 - loss: 0.3342 - mse: 0.1007\n",
      "Epoch 1649/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8636 - loss: 0.3342 - mse: 0.1007\n",
      "Epoch 1650/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8636 - loss: 0.3342 - mse: 0.1007\n",
      "Epoch 1651/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8636 - loss: 0.3341 - mse: 0.1007\n",
      "Epoch 1652/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8636 - loss: 0.3341 - mse: 0.1007\n",
      "Epoch 1653/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8636 - loss: 0.3341 - mse: 0.1007\n",
      "Epoch 1654/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8636 - loss: 0.3341 - mse: 0.1007\n",
      "Epoch 1655/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8636 - loss: 0.3341 - mse: 0.1007\n",
      "Epoch 1656/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8636 - loss: 0.3340 - mse: 0.1007\n",
      "Epoch 1657/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8636 - loss: 0.3340 - mse: 0.1007\n",
      "Epoch 1658/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8636 - loss: 0.3340 - mse: 0.1007\n",
      "Epoch 1659/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8636 - loss: 0.3340 - mse: 0.1007\n",
      "Epoch 1660/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3340 - mse: 0.1007\n",
      "Epoch 1661/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8636 - loss: 0.3340 - mse: 0.1007\n",
      "Epoch 1662/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8636 - loss: 0.3339 - mse: 0.1007\n",
      "Epoch 1663/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8636 - loss: 0.3339 - mse: 0.1007\n",
      "Epoch 1664/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8636 - loss: 0.3339 - mse: 0.1007\n",
      "Epoch 1665/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8636 - loss: 0.3339 - mse: 0.1007\n",
      "Epoch 1666/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8636 - loss: 0.3339 - mse: 0.1007\n",
      "Epoch 1667/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8636 - loss: 0.3339 - mse: 0.1006\n",
      "Epoch 1668/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8636 - loss: 0.3338 - mse: 0.1006\n",
      "Epoch 1669/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8636 - loss: 0.3338 - mse: 0.1006\n",
      "Epoch 1670/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8636 - loss: 0.3338 - mse: 0.1006\n",
      "Epoch 1671/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8636 - loss: 0.3338 - mse: 0.1006\n",
      "Epoch 1672/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3338 - mse: 0.1006\n",
      "Epoch 1673/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8636 - loss: 0.3337 - mse: 0.1006\n",
      "Epoch 1674/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8636 - loss: 0.3337 - mse: 0.1006\n",
      "Epoch 1675/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8630 - loss: 0.3337 - mse: 0.1006\n",
      "Epoch 1676/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - accuracy: 0.8636 - loss: 0.3337 - mse: 0.1006\n",
      "Epoch 1677/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8636 - loss: 0.3337 - mse: 0.1006\n",
      "Epoch 1678/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8630 - loss: 0.3337 - mse: 0.1006\n",
      "Epoch 1679/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8630 - loss: 0.3336 - mse: 0.1006\n",
      "Epoch 1680/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8636 - loss: 0.3336 - mse: 0.1006\n",
      "Epoch 1681/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8630 - loss: 0.3336 - mse: 0.1006\n",
      "Epoch 1682/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8636 - loss: 0.3336 - mse: 0.1006\n",
      "Epoch 1683/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8630 - loss: 0.3336 - mse: 0.1006\n",
      "Epoch 1684/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.8636 - loss: 0.3336 - mse: 0.1006\n",
      "Epoch 1685/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8636 - loss: 0.3335 - mse: 0.1005\n",
      "Epoch 1686/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8636 - loss: 0.3335 - mse: 0.1005\n",
      "Epoch 1687/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8636 - loss: 0.3335 - mse: 0.1005\n",
      "Epoch 1688/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8630 - loss: 0.3335 - mse: 0.1005\n",
      "Epoch 1689/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8636 - loss: 0.3335 - mse: 0.1005\n",
      "Epoch 1690/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8636 - loss: 0.3334 - mse: 0.1005\n",
      "Epoch 1691/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8636 - loss: 0.3334 - mse: 0.1005\n",
      "Epoch 1692/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8636 - loss: 0.3334 - mse: 0.1005\n",
      "Epoch 1693/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8636 - loss: 0.3334 - mse: 0.1005\n",
      "Epoch 1694/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8636 - loss: 0.3334 - mse: 0.1005\n",
      "Epoch 1695/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8636 - loss: 0.3334 - mse: 0.1005\n",
      "Epoch 1696/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8636 - loss: 0.3333 - mse: 0.1005\n",
      "Epoch 1697/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3333 - mse: 0.1005\n",
      "Epoch 1698/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8636 - loss: 0.3333 - mse: 0.1005\n",
      "Epoch 1699/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3333 - mse: 0.1005\n",
      "Epoch 1700/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8636 - loss: 0.3333 - mse: 0.1005\n",
      "Epoch 1701/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8636 - loss: 0.3333 - mse: 0.1005\n",
      "Epoch 1702/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1005\n",
      "Epoch 1703/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1004\n",
      "Epoch 1704/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1004\n",
      "Epoch 1705/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1004\n",
      "Epoch 1706/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1004\n",
      "Epoch 1707/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1004\n",
      "Epoch 1708/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1004\n",
      "Epoch 1709/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8636 - loss: 0.3331 - mse: 0.1004\n",
      "Epoch 1710/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1004\n",
      "Epoch 1711/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1004\n",
      "Epoch 1712/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1004\n",
      "Epoch 1713/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1004\n",
      "Epoch 1714/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8642 - loss: 0.3330 - mse: 0.1004\n",
      "Epoch 1715/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8642 - loss: 0.3330 - mse: 0.1004\n",
      "Epoch 1716/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8642 - loss: 0.3330 - mse: 0.1004\n",
      "Epoch 1717/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8642 - loss: 0.3330 - mse: 0.1004\n",
      "Epoch 1718/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8642 - loss: 0.3330 - mse: 0.1004\n",
      "Epoch 1719/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8642 - loss: 0.3330 - mse: 0.1004\n",
      "Epoch 1720/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1004\n",
      "Epoch 1721/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1004\n",
      "Epoch 1722/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1003\n",
      "Epoch 1723/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1003\n",
      "Epoch 1724/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1003\n",
      "Epoch 1725/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1003\n",
      "Epoch 1726/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8642 - loss: 0.3328 - mse: 0.1003\n",
      "Epoch 1727/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8642 - loss: 0.3328 - mse: 0.1003\n",
      "Epoch 1728/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8642 - loss: 0.3328 - mse: 0.1003\n",
      "Epoch 1729/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8647 - loss: 0.3328 - mse: 0.1003\n",
      "Epoch 1730/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8647 - loss: 0.3328 - mse: 0.1003\n",
      "Epoch 1731/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8647 - loss: 0.3327 - mse: 0.1003\n",
      "Epoch 1732/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8647 - loss: 0.3327 - mse: 0.1003\n",
      "Epoch 1733/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8647 - loss: 0.3327 - mse: 0.1003\n",
      "Epoch 1734/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8647 - loss: 0.3327 - mse: 0.1003\n",
      "Epoch 1735/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8647 - loss: 0.3327 - mse: 0.1003\n",
      "Epoch 1736/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8647 - loss: 0.3327 - mse: 0.1003\n",
      "Epoch 1737/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8647 - loss: 0.3326 - mse: 0.1003\n",
      "Epoch 1738/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8647 - loss: 0.3326 - mse: 0.1003\n",
      "Epoch 1739/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8647 - loss: 0.3326 - mse: 0.1003\n",
      "Epoch 1740/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8647 - loss: 0.3326 - mse: 0.1003\n",
      "Epoch 1741/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8647 - loss: 0.3326 - mse: 0.1003\n",
      "Epoch 1742/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8642 - loss: 0.3326 - mse: 0.1002\n",
      "Epoch 1743/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8642 - loss: 0.3325 - mse: 0.1002\n",
      "Epoch 1744/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8642 - loss: 0.3325 - mse: 0.1002\n",
      "Epoch 1745/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8647 - loss: 0.3325 - mse: 0.1002\n",
      "Epoch 1746/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8642 - loss: 0.3325 - mse: 0.1002\n",
      "Epoch 1747/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8647 - loss: 0.3325 - mse: 0.1002\n",
      "Epoch 1748/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8642 - loss: 0.3325 - mse: 0.1002\n",
      "Epoch 1749/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8647 - loss: 0.3324 - mse: 0.1002\n",
      "Epoch 1750/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8642 - loss: 0.3324 - mse: 0.1002\n",
      "Epoch 1751/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8642 - loss: 0.3324 - mse: 0.1002\n",
      "Epoch 1752/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8642 - loss: 0.3324 - mse: 0.1002\n",
      "Epoch 1753/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8642 - loss: 0.3324 - mse: 0.1002\n",
      "Epoch 1754/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8642 - loss: 0.3324 - mse: 0.1002\n",
      "Epoch 1755/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8647 - loss: 0.3323 - mse: 0.1002\n",
      "Epoch 1756/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8647 - loss: 0.3323 - mse: 0.1002\n",
      "Epoch 1757/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8647 - loss: 0.3323 - mse: 0.1002\n",
      "Epoch 1758/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8647 - loss: 0.3323 - mse: 0.1002\n",
      "Epoch 1759/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8642 - loss: 0.3323 - mse: 0.1002\n",
      "Epoch 1760/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8642 - loss: 0.3323 - mse: 0.1002\n",
      "Epoch 1761/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8642 - loss: 0.3322 - mse: 0.1001\n",
      "Epoch 1762/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8642 - loss: 0.3322 - mse: 0.1001\n",
      "Epoch 1763/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8647 - loss: 0.3322 - mse: 0.1001\n",
      "Epoch 1764/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8647 - loss: 0.3322 - mse: 0.1001\n",
      "Epoch 1765/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8647 - loss: 0.3322 - mse: 0.1001\n",
      "Epoch 1766/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8642 - loss: 0.3322 - mse: 0.1001\n",
      "Epoch 1767/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8647 - loss: 0.3322 - mse: 0.1001\n",
      "Epoch 1768/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8642 - loss: 0.3321 - mse: 0.1001\n",
      "Epoch 1769/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8642 - loss: 0.3321 - mse: 0.1001\n",
      "Epoch 1770/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8642 - loss: 0.3321 - mse: 0.1001\n",
      "Epoch 1771/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8647 - loss: 0.3321 - mse: 0.1001\n",
      "Epoch 1772/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8647 - loss: 0.3321 - mse: 0.1001\n",
      "Epoch 1773/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1001\n",
      "Epoch 1774/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1001\n",
      "Epoch 1775/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1001\n",
      "Epoch 1776/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1001\n",
      "Epoch 1777/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1001\n",
      "Epoch 1778/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1001\n",
      "Epoch 1779/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1001\n",
      "Epoch 1780/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8647 - loss: 0.3319 - mse: 0.1001\n",
      "Epoch 1781/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8647 - loss: 0.3319 - mse: 0.1000\n",
      "Epoch 1782/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8647 - loss: 0.3319 - mse: 0.1000\n",
      "Epoch 1783/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8647 - loss: 0.3319 - mse: 0.1000\n",
      "Epoch 1784/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8647 - loss: 0.3319 - mse: 0.1000\n",
      "Epoch 1785/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8647 - loss: 0.3319 - mse: 0.1000\n",
      "Epoch 1786/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1000\n",
      "Epoch 1787/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1000\n",
      "Epoch 1788/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1000\n",
      "Epoch 1789/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1000\n",
      "Epoch 1790/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1000\n",
      "Epoch 1791/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1000\n",
      "Epoch 1792/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1000\n",
      "Epoch 1793/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1000\n",
      "Epoch 1794/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1000\n",
      "Epoch 1795/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1000\n",
      "Epoch 1796/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1000\n",
      "Epoch 1797/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1000\n",
      "Epoch 1798/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.1000\n",
      "Epoch 1799/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.1000\n",
      "Epoch 1800/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.1000\n",
      "Epoch 1801/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.0999\n",
      "Epoch 1802/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.0999\n",
      "Epoch 1803/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.0999\n",
      "Epoch 1804/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8647 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 1805/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8647 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 1806/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8647 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 1807/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8647 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 1808/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8647 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 1809/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8647 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 1810/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8647 - loss: 0.3314 - mse: 0.0999\n",
      "Epoch 1811/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8647 - loss: 0.3314 - mse: 0.0999\n",
      "Epoch 1812/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8647 - loss: 0.3314 - mse: 0.0999\n",
      "Epoch 1813/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8647 - loss: 0.3314 - mse: 0.0999\n",
      "Epoch 1814/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8647 - loss: 0.3314 - mse: 0.0999\n",
      "Epoch 1815/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8647 - loss: 0.3314 - mse: 0.0999\n",
      "Epoch 1816/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8647 - loss: 0.3314 - mse: 0.0999\n",
      "Epoch 1817/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8647 - loss: 0.3313 - mse: 0.0999\n",
      "Epoch 1818/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8653 - loss: 0.3313 - mse: 0.0999\n",
      "Epoch 1819/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8647 - loss: 0.3313 - mse: 0.0999\n",
      "Epoch 1820/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8653 - loss: 0.3313 - mse: 0.0998\n",
      "Epoch 1821/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8647 - loss: 0.3313 - mse: 0.0999\n",
      "Epoch 1822/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8653 - loss: 0.3313 - mse: 0.0998\n",
      "Epoch 1823/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8647 - loss: 0.3312 - mse: 0.0998\n",
      "Epoch 1824/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8647 - loss: 0.3312 - mse: 0.0998\n",
      "Epoch 1825/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8647 - loss: 0.3312 - mse: 0.0998\n",
      "Epoch 1826/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8647 - loss: 0.3312 - mse: 0.0998\n",
      "Epoch 1827/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8647 - loss: 0.3312 - mse: 0.0998\n",
      "Epoch 1828/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8647 - loss: 0.3312 - mse: 0.0998\n",
      "Epoch 1829/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8647 - loss: 0.3312 - mse: 0.0998\n",
      "Epoch 1830/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0998\n",
      "Epoch 1831/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0998\n",
      "Epoch 1832/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0998\n",
      "Epoch 1833/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8642 - loss: 0.3311 - mse: 0.0998\n",
      "Epoch 1834/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0998\n",
      "Epoch 1835/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0998\n",
      "Epoch 1836/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8659 - loss: 0.3311 - mse: 0.0997\n",
      "Epoch 1837/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0998\n",
      "Epoch 1838/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8659 - loss: 0.3311 - mse: 0.0997\n",
      "Epoch 1839/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8642 - loss: 0.3311 - mse: 0.0999\n",
      "Epoch 1840/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0997\n",
      "Epoch 1841/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8636 - loss: 0.3312 - mse: 0.0999\n",
      "Epoch 1842/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8670 - loss: 0.3312 - mse: 0.0997\n",
      "Epoch 1843/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8642 - loss: 0.3314 - mse: 0.1000\n",
      "Epoch 1844/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.0998\n",
      "Epoch 1845/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8636 - loss: 0.3319 - mse: 0.1002\n",
      "Epoch 1846/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8642 - loss: 0.3323 - mse: 0.0999\n",
      "Epoch 1847/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1007\n",
      "Epoch 1848/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8664 - loss: 0.3340 - mse: 0.1003\n",
      "Epoch 1849/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8590 - loss: 0.3361 - mse: 0.1017\n",
      "Epoch 1850/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.8624 - loss: 0.3377 - mse: 0.1013\n",
      "Epoch 1851/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8573 - loss: 0.3426 - mse: 0.1037\n",
      "Epoch 1852/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8590 - loss: 0.3442 - mse: 0.1031\n",
      "Epoch 1853/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8556 - loss: 0.3525 - mse: 0.1065\n",
      "Epoch 1854/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8567 - loss: 0.3499 - mse: 0.1048\n",
      "Epoch 1855/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8545 - loss: 0.3554 - mse: 0.1071\n",
      "Epoch 1856/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8590 - loss: 0.3454 - mse: 0.1036\n",
      "Epoch 1857/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8602 - loss: 0.3406 - mse: 0.1026\n",
      "Epoch 1858/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8647 - loss: 0.3336 - mse: 0.1004\n",
      "Epoch 1859/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8647 - loss: 0.3313 - mse: 0.0999\n",
      "Epoch 1860/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1005\n",
      "Epoch 1861/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8624 - loss: 0.3366 - mse: 0.1011\n",
      "Epoch 1862/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8584 - loss: 0.3417 - mse: 0.1032\n",
      "Epoch 1863/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8624 - loss: 0.3420 - mse: 0.1025\n",
      "Epoch 1864/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8562 - loss: 0.3458 - mse: 0.1047\n",
      "Epoch 1865/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8624 - loss: 0.3430 - mse: 0.1027\n",
      "Epoch 1866/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8562 - loss: 0.3452 - mse: 0.1046\n",
      "Epoch 1867/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8630 - loss: 0.3414 - mse: 0.1023\n",
      "Epoch 1868/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8579 - loss: 0.3412 - mse: 0.1032\n",
      "Epoch 1869/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8624 - loss: 0.3369 - mse: 0.1011\n",
      "Epoch 1870/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8647 - loss: 0.3344 - mse: 0.1010\n",
      "Epoch 1871/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8659 - loss: 0.3318 - mse: 0.0999\n",
      "Epoch 1872/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8653 - loss: 0.3308 - mse: 0.0997\n",
      "Epoch 1873/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8642 - loss: 0.3310 - mse: 0.0999\n",
      "Epoch 1874/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8664 - loss: 0.3322 - mse: 0.0999\n",
      "Epoch 1875/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8630 - loss: 0.3339 - mse: 0.1009\n",
      "Epoch 1876/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8653 - loss: 0.3348 - mse: 0.1005\n",
      "Epoch 1877/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8602 - loss: 0.3367 - mse: 0.1019\n",
      "Epoch 1878/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.8647 - loss: 0.3367 - mse: 0.1010\n",
      "Epoch 1879/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - accuracy: 0.8573 - loss: 0.3384 - mse: 0.1024\n",
      "Epoch 1880/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8624 - loss: 0.3375 - mse: 0.1012\n",
      "Epoch 1881/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8579 - loss: 0.3385 - mse: 0.1024\n",
      "Epoch 1882/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8624 - loss: 0.3365 - mse: 0.1010\n",
      "Epoch 1883/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8607 - loss: 0.3359 - mse: 0.1015\n",
      "Epoch 1884/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8647 - loss: 0.3335 - mse: 0.1003\n",
      "Epoch 1885/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8642 - loss: 0.3319 - mse: 0.1002\n",
      "Epoch 1886/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8647 - loss: 0.3307 - mse: 0.0996\n",
      "Epoch 1887/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8670 - loss: 0.3304 - mse: 0.0996\n",
      "Epoch 1888/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8642 - loss: 0.3309 - mse: 0.0998\n",
      "Epoch 1889/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8653 - loss: 0.3318 - mse: 0.0998\n",
      "Epoch 1890/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1007\n",
      "Epoch 1891/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8659 - loss: 0.3341 - mse: 0.1003\n",
      "Epoch 1892/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8602 - loss: 0.3361 - mse: 0.1017\n",
      "Epoch 1893/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8630 - loss: 0.3367 - mse: 0.1010\n",
      "Epoch 1894/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8584 - loss: 0.3391 - mse: 0.1026\n",
      "Epoch 1895/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8636 - loss: 0.3386 - mse: 0.1016\n",
      "Epoch 1896/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8584 - loss: 0.3402 - mse: 0.1029\n",
      "Epoch 1897/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8636 - loss: 0.3375 - mse: 0.1014\n",
      "Epoch 1898/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8607 - loss: 0.3363 - mse: 0.1015\n",
      "Epoch 1899/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8647 - loss: 0.3330 - mse: 0.1002\n",
      "Epoch 1900/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8636 - loss: 0.3311 - mse: 0.0999\n",
      "Epoch 1901/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.8670 - loss: 0.3303 - mse: 0.0996\n",
      "Epoch 1902/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8653 - loss: 0.3307 - mse: 0.0996\n",
      "Epoch 1903/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1003\n",
      "Epoch 1904/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8659 - loss: 0.3334 - mse: 0.1002\n",
      "Epoch 1905/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8602 - loss: 0.3356 - mse: 0.1015\n",
      "Epoch 1906/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8636 - loss: 0.3365 - mse: 0.1010\n",
      "Epoch 1907/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8579 - loss: 0.3395 - mse: 0.1028\n",
      "Epoch 1908/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8642 - loss: 0.3395 - mse: 0.1018\n",
      "Epoch 1909/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8573 - loss: 0.3425 - mse: 0.1036\n",
      "Epoch 1910/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8619 - loss: 0.3399 - mse: 0.1020\n",
      "Epoch 1911/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8579 - loss: 0.3397 - mse: 0.1026\n",
      "Epoch 1912/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8624 - loss: 0.3351 - mse: 0.1008\n",
      "Epoch 1913/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8642 - loss: 0.3322 - mse: 0.1002\n",
      "Epoch 1914/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8647 - loss: 0.3304 - mse: 0.0996\n",
      "Epoch 1915/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8647 - loss: 0.3306 - mse: 0.0996\n",
      "Epoch 1916/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.8647 - loss: 0.3323 - mse: 0.1003\n",
      "Epoch 1917/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8642 - loss: 0.3338 - mse: 0.1003\n",
      "Epoch 1918/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8596 - loss: 0.3359 - mse: 0.1016\n",
      "Epoch 1919/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8636 - loss: 0.3358 - mse: 0.1008\n",
      "Epoch 1920/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8590 - loss: 0.3368 - mse: 0.1019\n",
      "Epoch 1921/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8647 - loss: 0.3356 - mse: 0.1007\n",
      "Epoch 1922/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8602 - loss: 0.3358 - mse: 0.1016\n",
      "Epoch 1923/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8647 - loss: 0.3342 - mse: 0.1004\n",
      "Epoch 1924/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8630 - loss: 0.3335 - mse: 0.1008\n",
      "Epoch 1925/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8670 - loss: 0.3319 - mse: 0.0998\n",
      "Epoch 1926/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8642 - loss: 0.3309 - mse: 0.0998\n",
      "Epoch 1927/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8647 - loss: 0.3301 - mse: 0.0994\n",
      "Epoch 1928/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8659 - loss: 0.3298 - mse: 0.0994\n",
      "Epoch 1929/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8653 - loss: 0.3300 - mse: 0.0995\n",
      "Epoch 1930/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8659 - loss: 0.3304 - mse: 0.0995\n",
      "Epoch 1931/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0999\n",
      "Epoch 1932/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8670 - loss: 0.3315 - mse: 0.0997\n",
      "Epoch 1933/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8653 - loss: 0.3321 - mse: 0.1003\n",
      "Epoch 1934/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.8670 - loss: 0.3322 - mse: 0.0998\n",
      "Epoch 1935/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8642 - loss: 0.3326 - mse: 0.1005\n",
      "Epoch 1936/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8653 - loss: 0.3324 - mse: 0.0999\n",
      "Epoch 1937/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8653 - loss: 0.3326 - mse: 0.1005\n",
      "Epoch 1938/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8659 - loss: 0.3321 - mse: 0.0999\n",
      "Epoch 1939/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.8642 - loss: 0.3320 - mse: 0.1002\n",
      "Epoch 1940/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8676 - loss: 0.3313 - mse: 0.0997\n",
      "Epoch 1941/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8642 - loss: 0.3308 - mse: 0.0998\n",
      "Epoch 1942/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8664 - loss: 0.3302 - mse: 0.0994\n",
      "Epoch 1943/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 0.8647 - loss: 0.3299 - mse: 0.0995\n",
      "Epoch 1944/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8676 - loss: 0.3296 - mse: 0.0993\n",
      "Epoch 1945/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8647 - loss: 0.3296 - mse: 0.0993\n",
      "Epoch 1946/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8647 - loss: 0.3296 - mse: 0.0994\n",
      "Epoch 1947/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.8676 - loss: 0.3297 - mse: 0.0993\n",
      "Epoch 1948/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8636 - loss: 0.3299 - mse: 0.0995\n",
      "Epoch 1949/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8647 - loss: 0.3300 - mse: 0.0993\n",
      "Epoch 1950/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8642 - loss: 0.3303 - mse: 0.0997\n",
      "Epoch 1951/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8659 - loss: 0.3305 - mse: 0.0995\n",
      "Epoch 1952/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8642 - loss: 0.3309 - mse: 0.0999\n",
      "Epoch 1953/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8676 - loss: 0.3311 - mse: 0.0996\n",
      "Epoch 1954/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8653 - loss: 0.3315 - mse: 0.1001\n",
      "Epoch 1955/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8659 - loss: 0.3315 - mse: 0.0997\n",
      "Epoch 1956/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1002\n",
      "Epoch 1957/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8653 - loss: 0.3316 - mse: 0.0997\n",
      "Epoch 1958/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1002\n",
      "Epoch 1959/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8653 - loss: 0.3317 - mse: 0.0997\n",
      "Epoch 1960/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8647 - loss: 0.3320 - mse: 0.1002\n",
      "Epoch 1961/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8659 - loss: 0.3318 - mse: 0.0998\n",
      "Epoch 1962/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8653 - loss: 0.3322 - mse: 0.1003\n",
      "Epoch 1963/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8659 - loss: 0.3322 - mse: 0.0999\n",
      "Epoch 1964/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8642 - loss: 0.3327 - mse: 0.1005\n",
      "Epoch 1965/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8653 - loss: 0.3326 - mse: 0.1000\n",
      "Epoch 1966/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.8642 - loss: 0.3331 - mse: 0.1006\n",
      "Epoch 1967/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8642 - loss: 0.3327 - mse: 0.1000\n",
      "Epoch 1968/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1005\n",
      "Epoch 1969/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8653 - loss: 0.3323 - mse: 0.0999\n",
      "Epoch 1970/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8653 - loss: 0.3323 - mse: 0.1003\n",
      "Epoch 1971/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8664 - loss: 0.3317 - mse: 0.0998\n",
      "Epoch 1972/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8647 - loss: 0.3316 - mse: 0.1001\n",
      "Epoch 1973/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8670 - loss: 0.3311 - mse: 0.0996\n",
      "Epoch 1974/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8642 - loss: 0.3309 - mse: 0.0999\n",
      "Epoch 1975/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8676 - loss: 0.3305 - mse: 0.0995\n",
      "Epoch 1976/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8642 - loss: 0.3304 - mse: 0.0997\n",
      "Epoch 1977/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8659 - loss: 0.3301 - mse: 0.0994\n",
      "Epoch 1978/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8642 - loss: 0.3300 - mse: 0.0996\n",
      "Epoch 1979/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8659 - loss: 0.3299 - mse: 0.0993\n",
      "Epoch 1980/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8642 - loss: 0.3298 - mse: 0.0995\n",
      "Epoch 1981/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8676 - loss: 0.3298 - mse: 0.0993\n",
      "Epoch 1982/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.8636 - loss: 0.3297 - mse: 0.0995\n",
      "Epoch 1983/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8676 - loss: 0.3297 - mse: 0.0993\n",
      "Epoch 1984/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8642 - loss: 0.3298 - mse: 0.0995\n",
      "Epoch 1985/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8664 - loss: 0.3298 - mse: 0.0993\n",
      "Epoch 1986/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8642 - loss: 0.3299 - mse: 0.0995\n",
      "Epoch 1987/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.8659 - loss: 0.3300 - mse: 0.0993\n",
      "Epoch 1988/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8647 - loss: 0.3303 - mse: 0.0997\n",
      "Epoch 1989/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8676 - loss: 0.3305 - mse: 0.0994\n",
      "Epoch 1990/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.1000\n",
      "Epoch 1991/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8659 - loss: 0.3316 - mse: 0.0997\n",
      "Epoch 1992/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8642 - loss: 0.3328 - mse: 0.1005\n",
      "Epoch 1993/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8630 - loss: 0.3336 - mse: 0.1003\n",
      "Epoch 1994/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.8602 - loss: 0.3359 - mse: 0.1015\n",
      "Epoch 1995/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8630 - loss: 0.3367 - mse: 0.1012\n",
      "Epoch 1996/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8584 - loss: 0.3403 - mse: 0.1028\n",
      "Epoch 1997/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8596 - loss: 0.3401 - mse: 0.1022\n",
      "Epoch 1998/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8567 - loss: 0.3438 - mse: 0.1038\n",
      "Epoch 1999/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8584 - loss: 0.3410 - mse: 0.1025\n",
      "Epoch 2000/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8584 - loss: 0.3417 - mse: 0.1030\n",
      "Epoch 2001/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8607 - loss: 0.3368 - mse: 0.1013\n",
      "Epoch 2002/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8642 - loss: 0.3341 - mse: 0.1008\n",
      "Epoch 2003/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8659 - loss: 0.3308 - mse: 0.0996\n",
      "Epoch 2004/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8653 - loss: 0.3292 - mse: 0.0993\n",
      "Epoch 2005/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8653 - loss: 0.3290 - mse: 0.0992\n",
      "Epoch 2006/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8659 - loss: 0.3300 - mse: 0.0993\n",
      "Epoch 2007/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1002\n",
      "Epoch 2008/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.8647 - loss: 0.3339 - mse: 0.1003\n",
      "Epoch 2009/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8584 - loss: 0.3382 - mse: 0.1024\n",
      "Epoch 2010/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8613 - loss: 0.3409 - mse: 0.1023\n",
      "Epoch 2011/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8556 - loss: 0.3499 - mse: 0.1059\n",
      "Epoch 2012/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8584 - loss: 0.3494 - mse: 0.1049\n",
      "Epoch 2013/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 0.8539 - loss: 0.3576 - mse: 0.1076\n",
      "Epoch 2014/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8590 - loss: 0.3458 - mse: 0.1039\n",
      "Epoch 2015/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.8602 - loss: 0.3392 - mse: 0.1020\n",
      "Epoch 2016/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8659 - loss: 0.3310 - mse: 0.0998\n",
      "Epoch 2017/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8653 - loss: 0.3307 - mse: 0.0997\n",
      "Epoch 2018/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8642 - loss: 0.3363 - mse: 0.1011\n",
      "Epoch 2019/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.8602 - loss: 0.3388 - mse: 0.1020\n",
      "Epoch 2020/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8584 - loss: 0.3404 - mse: 0.1025\n",
      "Epoch 2021/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8642 - loss: 0.3350 - mse: 0.1007\n",
      "Epoch 2022/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8664 - loss: 0.3316 - mse: 0.1001\n",
      "Epoch 2023/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8664 - loss: 0.3293 - mse: 0.0991\n",
      "Epoch 2024/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.8647 - loss: 0.3288 - mse: 0.0991\n",
      "Epoch 2025/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8642 - loss: 0.3298 - mse: 0.0996\n",
      "Epoch 2026/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8676 - loss: 0.3316 - mse: 0.0996\n",
      "Epoch 2027/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8584 - loss: 0.3351 - mse: 0.1015\n",
      "Epoch 2028/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8647 - loss: 0.3380 - mse: 0.1014\n",
      "Epoch 2029/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8584 - loss: 0.3447 - mse: 0.1043\n",
      "Epoch 2030/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8596 - loss: 0.3437 - mse: 0.1032\n",
      "Epoch 2031/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8573 - loss: 0.3463 - mse: 0.1043\n",
      "Epoch 2032/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8602 - loss: 0.3377 - mse: 0.1016\n",
      "Epoch 2033/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8653 - loss: 0.3320 - mse: 0.1000\n",
      "Epoch 2034/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8670 - loss: 0.3293 - mse: 0.0993\n",
      "Epoch 2035/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8636 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 2036/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8619 - loss: 0.3359 - mse: 0.1011\n",
      "Epoch 2037/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8613 - loss: 0.3364 - mse: 0.1012\n",
      "Epoch 2038/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8596 - loss: 0.3368 - mse: 0.1016\n",
      "Epoch 2039/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8636 - loss: 0.3333 - mse: 0.1002\n",
      "Epoch 2040/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8664 - loss: 0.3312 - mse: 0.1000\n",
      "Epoch 2041/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8670 - loss: 0.3294 - mse: 0.0991\n",
      "Epoch 2042/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 0.8647 - loss: 0.3287 - mse: 0.0991\n",
      "Epoch 2043/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8653 - loss: 0.3286 - mse: 0.0991\n",
      "Epoch 2044/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8670 - loss: 0.3291 - mse: 0.0991\n",
      "Epoch 2045/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3300 - mse: 0.0996\n",
      "Epoch 2046/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.8659 - loss: 0.3309 - mse: 0.0995\n",
      "Epoch 2047/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8647 - loss: 0.3321 - mse: 0.1003\n",
      "Epoch 2048/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8653 - loss: 0.3322 - mse: 0.0999\n",
      "Epoch 2049/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8653 - loss: 0.3324 - mse: 0.1003\n",
      "Epoch 2050/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8670 - loss: 0.3312 - mse: 0.0997\n",
      "Epoch 2051/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.8642 - loss: 0.3301 - mse: 0.0995\n",
      "Epoch 2052/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8659 - loss: 0.3289 - mse: 0.0991\n",
      "Epoch 2053/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8676 - loss: 0.3284 - mse: 0.0990\n",
      "Epoch 2054/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8653 - loss: 0.3286 - mse: 0.0991\n",
      "Epoch 2055/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8659 - loss: 0.3292 - mse: 0.0991\n",
      "Epoch 2056/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8636 - loss: 0.3300 - mse: 0.0996\n",
      "Epoch 2057/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8664 - loss: 0.3306 - mse: 0.0994\n",
      "Epoch 2058/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8653 - loss: 0.3315 - mse: 0.1001\n",
      "Epoch 2059/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8659 - loss: 0.3317 - mse: 0.0997\n",
      "Epoch 2060/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8619 - loss: 0.3326 - mse: 0.1005\n",
      "Epoch 2061/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.8653 - loss: 0.3325 - mse: 0.1000\n",
      "Epoch 2062/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8624 - loss: 0.3330 - mse: 0.1006\n",
      "Epoch 2063/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8636 - loss: 0.3321 - mse: 0.0999\n",
      "Epoch 2064/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8659 - loss: 0.3317 - mse: 0.1001\n",
      "Epoch 2065/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8659 - loss: 0.3303 - mse: 0.0994\n",
      "Epoch 2066/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8647 - loss: 0.3293 - mse: 0.0993\n",
      "Epoch 2067/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8670 - loss: 0.3285 - mse: 0.0990\n",
      "Epoch 2068/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8670 - loss: 0.3283 - mse: 0.0989\n",
      "Epoch 2069/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8664 - loss: 0.3285 - mse: 0.0990\n",
      "Epoch 2070/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8659 - loss: 0.3290 - mse: 0.0991\n",
      "Epoch 2071/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8647 - loss: 0.3297 - mse: 0.0995\n",
      "Epoch 2072/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8670 - loss: 0.3301 - mse: 0.0993\n",
      "Epoch 2073/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8659 - loss: 0.3309 - mse: 0.0999\n",
      "Epoch 2074/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8659 - loss: 0.3311 - mse: 0.0996\n",
      "Epoch 2075/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8647 - loss: 0.3318 - mse: 0.1002\n",
      "Epoch 2076/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8664 - loss: 0.3317 - mse: 0.0997\n",
      "Epoch 2077/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8647 - loss: 0.3321 - mse: 0.1003\n",
      "Epoch 2078/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8653 - loss: 0.3314 - mse: 0.0997\n",
      "Epoch 2079/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0999\n",
      "Epoch 2080/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8659 - loss: 0.3300 - mse: 0.0994\n",
      "Epoch 2081/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8642 - loss: 0.3292 - mse: 0.0993\n",
      "Epoch 2082/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8670 - loss: 0.3285 - mse: 0.0990\n",
      "Epoch 2083/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8682 - loss: 0.3281 - mse: 0.0989\n",
      "Epoch 2084/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8682 - loss: 0.3281 - mse: 0.0989\n",
      "Epoch 2085/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8676 - loss: 0.3283 - mse: 0.0989\n",
      "Epoch 2086/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8642 - loss: 0.3287 - mse: 0.0991\n",
      "Epoch 2087/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8670 - loss: 0.3292 - mse: 0.0991\n",
      "Epoch 2088/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8630 - loss: 0.3298 - mse: 0.0995\n",
      "Epoch 2089/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - accuracy: 0.8659 - loss: 0.3302 - mse: 0.0993\n",
      "Epoch 2090/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8653 - loss: 0.3310 - mse: 0.0999\n",
      "Epoch 2091/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8664 - loss: 0.3313 - mse: 0.0997\n",
      "Epoch 2092/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8642 - loss: 0.3323 - mse: 0.1003\n",
      "Epoch 2093/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8630 - loss: 0.3323 - mse: 0.1000\n",
      "Epoch 2094/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8642 - loss: 0.3329 - mse: 0.1005\n",
      "Epoch 2095/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8630 - loss: 0.3321 - mse: 0.0999\n",
      "Epoch 2096/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8659 - loss: 0.3317 - mse: 0.1001\n",
      "Epoch 2097/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8664 - loss: 0.3304 - mse: 0.0995\n",
      "Epoch 2098/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8647 - loss: 0.3295 - mse: 0.0993\n",
      "Epoch 2099/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8670 - loss: 0.3285 - mse: 0.0990\n",
      "Epoch 2100/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8642 - loss: 0.3281 - mse: 0.0989\n",
      "Epoch 2101/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8670 - loss: 0.3279 - mse: 0.0988\n",
      "Epoch 2102/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8682 - loss: 0.3280 - mse: 0.0988\n",
      "Epoch 2103/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.8647 - loss: 0.3284 - mse: 0.0990\n",
      "Epoch 2104/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8659 - loss: 0.3289 - mse: 0.0990\n",
      "Epoch 2105/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.8636 - loss: 0.3297 - mse: 0.0995\n",
      "Epoch 2106/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8659 - loss: 0.3305 - mse: 0.0994\n",
      "Epoch 2107/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8647 - loss: 0.3321 - mse: 0.1003\n",
      "Epoch 2108/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8624 - loss: 0.3330 - mse: 0.1002\n",
      "Epoch 2109/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8602 - loss: 0.3354 - mse: 0.1012\n",
      "Epoch 2110/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8613 - loss: 0.3353 - mse: 0.1009\n",
      "Epoch 2111/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8590 - loss: 0.3368 - mse: 0.1016\n",
      "Epoch 2112/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8613 - loss: 0.3346 - mse: 0.1007\n",
      "Epoch 2113/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8653 - loss: 0.3334 - mse: 0.1005\n",
      "Epoch 2114/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8636 - loss: 0.3305 - mse: 0.0996\n",
      "Epoch 2115/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8659 - loss: 0.3288 - mse: 0.0991\n",
      "Epoch 2116/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8687 - loss: 0.3279 - mse: 0.0988\n",
      "Epoch 2117/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.8670 - loss: 0.3281 - mse: 0.0988\n",
      "Epoch 2118/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8653 - loss: 0.3290 - mse: 0.0992\n",
      "Epoch 2119/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8659 - loss: 0.3303 - mse: 0.0994\n",
      "Epoch 2120/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8607 - loss: 0.3325 - mse: 0.1005\n",
      "Epoch 2121/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8630 - loss: 0.3342 - mse: 0.1004\n",
      "Epoch 2122/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8584 - loss: 0.3383 - mse: 0.1023\n",
      "Epoch 2123/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8607 - loss: 0.3396 - mse: 0.1021\n",
      "Epoch 2124/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8596 - loss: 0.3456 - mse: 0.1044\n",
      "Epoch 2125/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8590 - loss: 0.3429 - mse: 0.1032\n",
      "Epoch 2126/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8567 - loss: 0.3441 - mse: 0.1035\n",
      "Epoch 2127/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8607 - loss: 0.3360 - mse: 0.1012\n",
      "Epoch 2128/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8647 - loss: 0.3308 - mse: 0.0996\n",
      "Epoch 2129/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8670 - loss: 0.3283 - mse: 0.0990\n",
      "Epoch 2130/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8653 - loss: 0.3299 - mse: 0.0994\n",
      "Epoch 2131/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8642 - loss: 0.3338 - mse: 0.1006\n",
      "Epoch 2132/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8619 - loss: 0.3355 - mse: 0.1010\n",
      "Epoch 2133/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8590 - loss: 0.3381 - mse: 0.1021\n",
      "Epoch 2134/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8630 - loss: 0.3360 - mse: 0.1010\n",
      "Epoch 2135/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8584 - loss: 0.3361 - mse: 0.1016\n",
      "Epoch 2136/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8630 - loss: 0.3336 - mse: 0.1003\n",
      "Epoch 2137/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8602 - loss: 0.3328 - mse: 0.1005\n",
      "Epoch 2138/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8676 - loss: 0.3308 - mse: 0.0995\n",
      "Epoch 2139/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8630 - loss: 0.3297 - mse: 0.0995\n",
      "Epoch 2140/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8676 - loss: 0.3285 - mse: 0.0989\n",
      "Epoch 2141/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8653 - loss: 0.3278 - mse: 0.0988\n",
      "Epoch 2142/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8682 - loss: 0.3276 - mse: 0.0987\n",
      "Epoch 2143/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8676 - loss: 0.3278 - mse: 0.0988\n",
      "Epoch 2144/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8647 - loss: 0.3283 - mse: 0.0990\n",
      "Epoch 2145/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8676 - loss: 0.3287 - mse: 0.0990\n",
      "Epoch 2146/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8647 - loss: 0.3291 - mse: 0.0993\n",
      "Epoch 2147/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8670 - loss: 0.3291 - mse: 0.0991\n",
      "Epoch 2148/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.8630 - loss: 0.3292 - mse: 0.0993\n",
      "Epoch 2149/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8676 - loss: 0.3291 - mse: 0.0990\n",
      "Epoch 2150/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8642 - loss: 0.3290 - mse: 0.0993\n",
      "Epoch 2151/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8676 - loss: 0.3288 - mse: 0.0990\n",
      "Epoch 2152/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8647 - loss: 0.3287 - mse: 0.0992\n",
      "Epoch 2153/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8676 - loss: 0.3285 - mse: 0.0989\n",
      "Epoch 2154/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8647 - loss: 0.3283 - mse: 0.0990\n",
      "Epoch 2155/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8659 - loss: 0.3281 - mse: 0.0988\n",
      "Epoch 2156/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8659 - loss: 0.3279 - mse: 0.0989\n",
      "Epoch 2157/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8676 - loss: 0.3277 - mse: 0.0987\n",
      "Epoch 2158/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8659 - loss: 0.3276 - mse: 0.0987\n",
      "Epoch 2159/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8687 - loss: 0.3275 - mse: 0.0987\n",
      "Epoch 2160/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8682 - loss: 0.3274 - mse: 0.0987\n",
      "Epoch 2161/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8682 - loss: 0.3274 - mse: 0.0987\n",
      "Epoch 2162/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8687 - loss: 0.3274 - mse: 0.0986\n",
      "Epoch 2163/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.8653 - loss: 0.3274 - mse: 0.0987\n",
      "Epoch 2164/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8687 - loss: 0.3275 - mse: 0.0986\n",
      "Epoch 2165/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8659 - loss: 0.3275 - mse: 0.0987\n",
      "Epoch 2166/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8682 - loss: 0.3276 - mse: 0.0987\n",
      "Epoch 2167/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.8659 - loss: 0.3277 - mse: 0.0988\n",
      "Epoch 2168/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8676 - loss: 0.3278 - mse: 0.0987\n",
      "Epoch 2169/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8653 - loss: 0.3280 - mse: 0.0989\n",
      "Epoch 2170/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8676 - loss: 0.3281 - mse: 0.0988\n",
      "Epoch 2171/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8647 - loss: 0.3284 - mse: 0.0990\n",
      "Epoch 2172/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8682 - loss: 0.3285 - mse: 0.0989\n",
      "Epoch 2173/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8647 - loss: 0.3288 - mse: 0.0992\n",
      "Epoch 2174/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8659 - loss: 0.3289 - mse: 0.0990\n",
      "Epoch 2175/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8630 - loss: 0.3293 - mse: 0.0993\n",
      "Epoch 2176/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.8670 - loss: 0.3295 - mse: 0.0992\n",
      "Epoch 2177/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8647 - loss: 0.3300 - mse: 0.0996\n",
      "Epoch 2178/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8659 - loss: 0.3301 - mse: 0.0994\n",
      "Epoch 2179/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8647 - loss: 0.3309 - mse: 0.0998\n",
      "Epoch 2180/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8642 - loss: 0.3309 - mse: 0.0996\n",
      "Epoch 2181/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8653 - loss: 0.3316 - mse: 0.1000\n",
      "Epoch 2182/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8630 - loss: 0.3314 - mse: 0.0998\n",
      "Epoch 2183/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8653 - loss: 0.3318 - mse: 0.1001\n",
      "Epoch 2184/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8630 - loss: 0.3311 - mse: 0.0997\n",
      "Epoch 2185/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8647 - loss: 0.3311 - mse: 0.0999\n",
      "Epoch 2186/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8642 - loss: 0.3302 - mse: 0.0994\n",
      "Epoch 2187/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.8630 - loss: 0.3298 - mse: 0.0995\n",
      "Epoch 2188/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.8664 - loss: 0.3290 - mse: 0.0991\n",
      "Epoch 2189/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8653 - loss: 0.3286 - mse: 0.0991\n",
      "Epoch 2190/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8676 - loss: 0.3281 - mse: 0.0988\n",
      "Epoch 2191/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8653 - loss: 0.3279 - mse: 0.0989\n",
      "Epoch 2192/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.8670 - loss: 0.3276 - mse: 0.0987\n",
      "Epoch 2193/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8659 - loss: 0.3275 - mse: 0.0987\n",
      "Epoch 2194/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8682 - loss: 0.3274 - mse: 0.0986\n",
      "Epoch 2195/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8670 - loss: 0.3274 - mse: 0.0987\n",
      "Epoch 2196/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8687 - loss: 0.3273 - mse: 0.0986\n",
      "Epoch 2197/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8670 - loss: 0.3273 - mse: 0.0987\n",
      "Epoch 2198/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8682 - loss: 0.3274 - mse: 0.0986\n",
      "Epoch 2199/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8664 - loss: 0.3274 - mse: 0.0987\n",
      "Epoch 2200/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8670 - loss: 0.3275 - mse: 0.0987\n",
      "Epoch 2201/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8653 - loss: 0.3277 - mse: 0.0988\n",
      "Epoch 2202/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.8670 - loss: 0.3278 - mse: 0.0987\n",
      "Epoch 2203/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.8647 - loss: 0.3280 - mse: 0.0989\n",
      "Epoch 2204/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8687 - loss: 0.3282 - mse: 0.0988\n",
      "Epoch 2205/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8647 - loss: 0.3286 - mse: 0.0991\n",
      "Epoch 2206/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8664 - loss: 0.3289 - mse: 0.0990\n",
      "Epoch 2207/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8647 - loss: 0.3297 - mse: 0.0995\n",
      "Epoch 2208/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8642 - loss: 0.3302 - mse: 0.0994\n",
      "Epoch 2209/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8647 - loss: 0.3317 - mse: 0.1001\n",
      "Epoch 2210/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8647 - loss: 0.3326 - mse: 0.1001\n",
      "Epoch 2211/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8596 - loss: 0.3352 - mse: 0.1011\n",
      "Epoch 2212/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8613 - loss: 0.3358 - mse: 0.1011\n",
      "Epoch 2213/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8584 - loss: 0.3393 - mse: 0.1023\n",
      "Epoch 2214/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8613 - loss: 0.3383 - mse: 0.1019\n",
      "Epoch 2215/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8584 - loss: 0.3403 - mse: 0.1025\n",
      "Epoch 2216/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8619 - loss: 0.3363 - mse: 0.1013\n",
      "Epoch 2217/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8642 - loss: 0.3343 - mse: 0.1006\n",
      "Epoch 2218/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8630 - loss: 0.3303 - mse: 0.0996\n",
      "Epoch 2219/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8664 - loss: 0.3279 - mse: 0.0988\n",
      "Epoch 2220/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8687 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2221/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.8682 - loss: 0.3277 - mse: 0.0987\n",
      "Epoch 2222/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8647 - loss: 0.3296 - mse: 0.0994\n",
      "Epoch 2223/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8636 - loss: 0.3318 - mse: 0.0998\n",
      "Epoch 2224/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8590 - loss: 0.3365 - mse: 0.1018\n",
      "Epoch 2225/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step - accuracy: 0.8619 - loss: 0.3399 - mse: 0.1022\n",
      "Epoch 2226/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8562 - loss: 0.3508 - mse: 0.1061\n",
      "Epoch 2227/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.8550 - loss: 0.3520 - mse: 0.1059\n",
      "Epoch 2228/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.8487 - loss: 0.3656 - mse: 0.1098\n",
      "Epoch 2229/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.8562 - loss: 0.3499 - mse: 0.1054\n",
      "Epoch 2230/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8596 - loss: 0.3411 - mse: 0.1021\n",
      "Epoch 2231/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.8630 - loss: 0.3296 - mse: 0.0995\n",
      "Epoch 2232/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.8630 - loss: 0.3312 - mse: 0.1000\n",
      "Epoch 2233/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8607 - loss: 0.3407 - mse: 0.1018\n",
      "Epoch 2234/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8602 - loss: 0.3402 - mse: 0.1026\n",
      "Epoch 2235/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8613 - loss: 0.3371 - mse: 0.1012\n",
      "Epoch 2236/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - accuracy: 0.8653 - loss: 0.3294 - mse: 0.0993\n",
      "Epoch 2237/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.8687 - loss: 0.3268 - mse: 0.0985\n",
      "Epoch 2238/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step - accuracy: 0.8653 - loss: 0.3292 - mse: 0.0994\n",
      "Epoch 2239/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8642 - loss: 0.3343 - mse: 0.1004\n",
      "Epoch 2240/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - accuracy: 0.8567 - loss: 0.3449 - mse: 0.1048\n",
      "Epoch 2241/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568ms/step - accuracy: 0.8579 - loss: 0.3514 - mse: 0.1055\n",
      "Epoch 2242/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step - accuracy: 0.8408 - loss: 0.3747 - mse: 0.1136\n",
      "Epoch 2243/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - accuracy: 0.8556 - loss: 0.3626 - mse: 0.1092\n",
      "Epoch 2244/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.8516 - loss: 0.3642 - mse: 0.1088\n",
      "Epoch 2245/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.8607 - loss: 0.3360 - mse: 0.1014\n",
      "Epoch 2246/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8642 - loss: 0.3300 - mse: 0.0996\n",
      "Epoch 2247/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.8636 - loss: 0.3432 - mse: 0.1021\n",
      "Epoch 2248/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8613 - loss: 0.3426 - mse: 0.1034\n",
      "Epoch 2249/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8653 - loss: 0.3343 - mse: 0.1001\n",
      "Epoch 2250/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8676 - loss: 0.3285 - mse: 0.0990\n",
      "Epoch 2251/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8613 - loss: 0.3344 - mse: 0.1009\n",
      "Epoch 2252/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8584 - loss: 0.3438 - mse: 0.1034\n",
      "Epoch 2253/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8624 - loss: 0.3394 - mse: 0.1021\n",
      "Epoch 2254/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.8584 - loss: 0.3363 - mse: 0.1018\n",
      "Epoch 2255/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8664 - loss: 0.3304 - mse: 0.0993\n",
      "Epoch 2256/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8642 - loss: 0.3278 - mse: 0.0989\n",
      "Epoch 2257/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8653 - loss: 0.3272 - mse: 0.0986\n",
      "Epoch 2258/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - accuracy: 0.8682 - loss: 0.3282 - mse: 0.0987\n",
      "Epoch 2259/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.8642 - loss: 0.3305 - mse: 0.0999\n",
      "Epoch 2260/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8642 - loss: 0.3321 - mse: 0.0998\n",
      "Epoch 2261/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8602 - loss: 0.3338 - mse: 0.1008\n",
      "Epoch 2262/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.8642 - loss: 0.3316 - mse: 0.0999\n",
      "Epoch 2263/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8642 - loss: 0.3292 - mse: 0.0992\n",
      "Epoch 2264/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8682 - loss: 0.3271 - mse: 0.0986\n",
      "Epoch 2265/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8670 - loss: 0.3275 - mse: 0.0987\n",
      "Epoch 2266/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8659 - loss: 0.3293 - mse: 0.0992\n",
      "Epoch 2267/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8642 - loss: 0.3296 - mse: 0.0994\n",
      "Epoch 2268/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8653 - loss: 0.3289 - mse: 0.0991\n",
      "Epoch 2269/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8670 - loss: 0.3273 - mse: 0.0986\n",
      "Epoch 2270/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8687 - loss: 0.3266 - mse: 0.0984\n",
      "Epoch 2271/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.8653 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2272/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8670 - loss: 0.3280 - mse: 0.0987\n",
      "Epoch 2273/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8647 - loss: 0.3291 - mse: 0.0994\n",
      "Epoch 2274/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8664 - loss: 0.3293 - mse: 0.0991\n",
      "Epoch 2275/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.8647 - loss: 0.3295 - mse: 0.0995\n",
      "Epoch 2276/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8670 - loss: 0.3286 - mse: 0.0989\n",
      "Epoch 2277/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.8659 - loss: 0.3278 - mse: 0.0989\n",
      "Epoch 2278/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8676 - loss: 0.3269 - mse: 0.0985\n",
      "Epoch 2279/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8693 - loss: 0.3265 - mse: 0.0984\n",
      "Epoch 2280/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8682 - loss: 0.3267 - mse: 0.0985\n",
      "Epoch 2281/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8682 - loss: 0.3271 - mse: 0.0986\n",
      "Epoch 2282/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8659 - loss: 0.3274 - mse: 0.0987\n",
      "Epoch 2283/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8676 - loss: 0.3273 - mse: 0.0986\n",
      "Epoch 2284/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8664 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2285/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.8682 - loss: 0.3266 - mse: 0.0984\n",
      "Epoch 2286/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8682 - loss: 0.3264 - mse: 0.0984\n",
      "Epoch 2287/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8670 - loss: 0.3265 - mse: 0.0984\n",
      "Epoch 2288/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8687 - loss: 0.3266 - mse: 0.0984\n",
      "Epoch 2289/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8659 - loss: 0.3268 - mse: 0.0985\n",
      "Epoch 2290/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8682 - loss: 0.3269 - mse: 0.0985\n",
      "Epoch 2291/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - accuracy: 0.8653 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2292/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - accuracy: 0.8676 - loss: 0.3269 - mse: 0.0985\n",
      "Epoch 2293/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8664 - loss: 0.3268 - mse: 0.0985\n",
      "Epoch 2294/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8682 - loss: 0.3266 - mse: 0.0984\n",
      "Epoch 2295/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.8682 - loss: 0.3264 - mse: 0.0984\n",
      "Epoch 2296/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8693 - loss: 0.3264 - mse: 0.0984\n",
      "Epoch 2297/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.8687 - loss: 0.3264 - mse: 0.0984\n",
      "Epoch 2298/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8670 - loss: 0.3265 - mse: 0.0984\n",
      "Epoch 2299/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8682 - loss: 0.3265 - mse: 0.0984\n",
      "Epoch 2300/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8682 - loss: 0.3266 - mse: 0.0984\n",
      "Epoch 2301/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8687 - loss: 0.3265 - mse: 0.0984\n",
      "Epoch 2302/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8682 - loss: 0.3265 - mse: 0.0984\n",
      "Epoch 2303/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8687 - loss: 0.3264 - mse: 0.0983\n",
      "Epoch 2304/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8670 - loss: 0.3264 - mse: 0.0984\n",
      "Epoch 2305/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8693 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2306/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2307/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2308/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2309/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8670 - loss: 0.3263 - mse: 0.0984\n",
      "Epoch 2310/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2311/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.8670 - loss: 0.3263 - mse: 0.0984\n",
      "Epoch 2312/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2313/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8676 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2314/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2315/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2316/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2317/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2318/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8693 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2319/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2320/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8682 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2321/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2322/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8676 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2323/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2324/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.8682 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2325/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2326/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8682 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2327/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8693 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2328/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2329/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2330/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2331/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2332/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8699 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2333/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2334/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2335/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2336/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8693 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2337/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8682 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2338/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2339/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8682 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2340/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8699 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2341/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8682 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2342/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.8699 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2343/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2344/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2345/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2346/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2347/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2348/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8693 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2349/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2350/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2351/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2352/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2353/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8693 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2354/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2355/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8693 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2356/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2357/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2358/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2359/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8699 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2360/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8682 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2361/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2362/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8682 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2363/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8693 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2364/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8682 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2365/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2366/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8676 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2367/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2368/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8676 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2369/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0982\n",
      "Epoch 2370/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8676 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2371/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2372/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2373/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8682 - loss: 0.3262 - mse: 0.0983\n",
      "Epoch 2374/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8676 - loss: 0.3264 - mse: 0.0984\n",
      "Epoch 2375/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8693 - loss: 0.3266 - mse: 0.0984\n",
      "Epoch 2376/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8670 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2377/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8670 - loss: 0.3275 - mse: 0.0987\n",
      "Epoch 2378/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8642 - loss: 0.3284 - mse: 0.0991\n",
      "Epoch 2379/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8642 - loss: 0.3293 - mse: 0.0992\n",
      "Epoch 2380/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8647 - loss: 0.3313 - mse: 0.0999\n",
      "Epoch 2381/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8636 - loss: 0.3327 - mse: 0.1003\n",
      "Epoch 2382/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8584 - loss: 0.3366 - mse: 0.1015\n",
      "Epoch 2383/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.8607 - loss: 0.3375 - mse: 0.1018\n",
      "Epoch 2384/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8584 - loss: 0.3426 - mse: 0.1031\n",
      "Epoch 2385/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8584 - loss: 0.3401 - mse: 0.1026\n",
      "Epoch 2386/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8590 - loss: 0.3413 - mse: 0.1025\n",
      "Epoch 2387/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8624 - loss: 0.3348 - mse: 0.1011\n",
      "Epoch 2388/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8642 - loss: 0.3307 - mse: 0.0995\n",
      "Epoch 2389/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8676 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2390/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8676 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2391/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8664 - loss: 0.3277 - mse: 0.0987\n",
      "Epoch 2392/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8636 - loss: 0.3303 - mse: 0.0996\n",
      "Epoch 2393/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8596 - loss: 0.3348 - mse: 0.1011\n",
      "Epoch 2394/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8630 - loss: 0.3374 - mse: 0.1016\n",
      "Epoch 2395/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8584 - loss: 0.3456 - mse: 0.1045\n",
      "Epoch 2396/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8567 - loss: 0.3467 - mse: 0.1045\n",
      "Epoch 2397/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8499 - loss: 0.3581 - mse: 0.1079\n",
      "Epoch 2398/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8567 - loss: 0.3491 - mse: 0.1054\n",
      "Epoch 2399/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8602 - loss: 0.3467 - mse: 0.1038\n",
      "Epoch 2400/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8636 - loss: 0.3323 - mse: 0.1004\n",
      "Epoch 2401/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8682 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2402/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8653 - loss: 0.3314 - mse: 0.0994\n",
      "Epoch 2403/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8607 - loss: 0.3365 - mse: 0.1017\n",
      "Epoch 2404/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8596 - loss: 0.3394 - mse: 0.1018\n",
      "Epoch 2405/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8630 - loss: 0.3325 - mse: 0.1003\n",
      "Epoch 2406/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8653 - loss: 0.3278 - mse: 0.0988\n",
      "Epoch 2407/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8699 - loss: 0.3256 - mse: 0.0982\n",
      "Epoch 2408/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8687 - loss: 0.3268 - mse: 0.0984\n",
      "Epoch 2409/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8624 - loss: 0.3308 - mse: 0.1000\n",
      "Epoch 2410/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8636 - loss: 0.3357 - mse: 0.1009\n",
      "Epoch 2411/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8562 - loss: 0.3469 - mse: 0.1053\n",
      "Epoch 2412/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8556 - loss: 0.3507 - mse: 0.1057\n",
      "Epoch 2413/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8505 - loss: 0.3663 - mse: 0.1104\n",
      "Epoch 2414/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8567 - loss: 0.3491 - mse: 0.1054\n",
      "Epoch 2415/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8624 - loss: 0.3381 - mse: 0.1012\n",
      "Epoch 2416/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8676 - loss: 0.3275 - mse: 0.0988\n",
      "Epoch 2417/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8636 - loss: 0.3332 - mse: 0.1007\n",
      "Epoch 2418/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.8596 - loss: 0.3439 - mse: 0.1023\n",
      "Epoch 2419/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8624 - loss: 0.3359 - mse: 0.1015\n",
      "Epoch 2420/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8682 - loss: 0.3278 - mse: 0.0986\n",
      "Epoch 2421/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8682 - loss: 0.3274 - mse: 0.0986\n",
      "Epoch 2422/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8630 - loss: 0.3333 - mse: 0.1005\n",
      "Epoch 2423/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8596 - loss: 0.3417 - mse: 0.1033\n",
      "Epoch 2424/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8619 - loss: 0.3405 - mse: 0.1025\n",
      "Epoch 2425/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8584 - loss: 0.3428 - mse: 0.1040\n",
      "Epoch 2426/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8636 - loss: 0.3369 - mse: 0.1013\n",
      "Epoch 2427/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8590 - loss: 0.3346 - mse: 0.1012\n",
      "Epoch 2428/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8642 - loss: 0.3295 - mse: 0.0992\n",
      "Epoch 2429/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8664 - loss: 0.3266 - mse: 0.0985\n",
      "Epoch 2430/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8693 - loss: 0.3256 - mse: 0.0982\n",
      "Epoch 2431/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8676 - loss: 0.3268 - mse: 0.0986\n",
      "Epoch 2432/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8653 - loss: 0.3286 - mse: 0.0990\n",
      "Epoch 2433/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8647 - loss: 0.3285 - mse: 0.0991\n",
      "Epoch 2434/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.8664 - loss: 0.3274 - mse: 0.0986\n",
      "Epoch 2435/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.8693 - loss: 0.3259 - mse: 0.0983\n",
      "Epoch 2436/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8682 - loss: 0.3256 - mse: 0.0982\n",
      "Epoch 2437/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8676 - loss: 0.3264 - mse: 0.0984\n",
      "Epoch 2438/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.8676 - loss: 0.3272 - mse: 0.0986\n",
      "Epoch 2439/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8636 - loss: 0.3277 - mse: 0.0989\n",
      "Epoch 2440/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8676 - loss: 0.3273 - mse: 0.0986\n",
      "Epoch 2441/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8653 - loss: 0.3267 - mse: 0.0986\n",
      "Epoch 2442/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8682 - loss: 0.3260 - mse: 0.0982\n",
      "Epoch 2443/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3255 - mse: 0.0982\n",
      "Epoch 2444/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8676 - loss: 0.3254 - mse: 0.0981\n",
      "Epoch 2445/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8687 - loss: 0.3256 - mse: 0.0981\n",
      "Epoch 2446/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0983\n",
      "Epoch 2447/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8693 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2448/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2449/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8693 - loss: 0.3258 - mse: 0.0982\n",
      "Epoch 2450/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8676 - loss: 0.3256 - mse: 0.0981\n",
      "Epoch 2451/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8687 - loss: 0.3254 - mse: 0.0981\n",
      "Epoch 2452/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8687 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2453/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8682 - loss: 0.3255 - mse: 0.0981\n",
      "Epoch 2454/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8682 - loss: 0.3256 - mse: 0.0981\n",
      "Epoch 2455/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8693 - loss: 0.3257 - mse: 0.0982\n",
      "Epoch 2456/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8682 - loss: 0.3257 - mse: 0.0982\n",
      "Epoch 2457/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8693 - loss: 0.3256 - mse: 0.0982\n",
      "Epoch 2458/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8687 - loss: 0.3255 - mse: 0.0981\n",
      "Epoch 2459/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8682 - loss: 0.3254 - mse: 0.0981\n",
      "Epoch 2460/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8687 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2461/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8699 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2462/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8693 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2463/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8687 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2464/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8676 - loss: 0.3254 - mse: 0.0981\n",
      "Epoch 2465/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8687 - loss: 0.3254 - mse: 0.0981\n",
      "Epoch 2466/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8676 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2467/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8687 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2468/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8676 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2469/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8687 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2470/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2471/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.8699 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2472/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8687 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2473/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8682 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2474/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8687 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2475/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8676 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2476/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8687 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2477/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8682 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2478/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8687 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2479/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2480/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2481/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2482/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2483/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2484/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2485/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2486/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2487/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2488/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2489/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2490/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2491/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8693 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2492/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2493/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2494/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2495/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2496/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2497/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8699 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2498/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2499/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2500/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8687 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2501/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2502/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8687 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2503/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2504/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2505/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2506/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8693 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2507/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2508/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8693 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2509/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2510/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2511/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8693 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2512/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2513/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8693 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2514/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2515/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8687 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2516/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2517/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2518/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2519/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2520/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2521/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2522/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2523/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2524/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2525/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2526/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2527/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0979\n",
      "Epoch 2528/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2529/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0979\n",
      "Epoch 2530/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2531/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0979\n",
      "Epoch 2532/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8682 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2533/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2534/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8676 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2535/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2536/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8676 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2537/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8693 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2538/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8682 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2539/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8676 - loss: 0.3252 - mse: 0.0980\n",
      "Epoch 2540/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2541/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8687 - loss: 0.3255 - mse: 0.0981\n",
      "Epoch 2542/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8676 - loss: 0.3259 - mse: 0.0983\n",
      "Epoch 2543/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8670 - loss: 0.3263 - mse: 0.0984\n",
      "Epoch 2544/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8647 - loss: 0.3271 - mse: 0.0986\n",
      "Epoch 2545/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8647 - loss: 0.3279 - mse: 0.0989\n",
      "Epoch 2546/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8647 - loss: 0.3298 - mse: 0.0995\n",
      "Epoch 2547/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8642 - loss: 0.3313 - mse: 0.0999\n",
      "Epoch 2548/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8596 - loss: 0.3352 - mse: 0.1011\n",
      "Epoch 2549/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8596 - loss: 0.3367 - mse: 0.1016\n",
      "Epoch 2550/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8596 - loss: 0.3426 - mse: 0.1031\n",
      "Epoch 2551/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8567 - loss: 0.3406 - mse: 0.1029\n",
      "Epoch 2552/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8590 - loss: 0.3429 - mse: 0.1029\n",
      "Epoch 2553/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8602 - loss: 0.3356 - mse: 0.1014\n",
      "Epoch 2554/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8647 - loss: 0.3308 - mse: 0.0994\n",
      "Epoch 2555/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0985\n",
      "Epoch 2556/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8693 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2557/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8664 - loss: 0.3274 - mse: 0.0986\n",
      "Epoch 2558/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8642 - loss: 0.3307 - mse: 0.0998\n",
      "Epoch 2559/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8596 - loss: 0.3365 - mse: 0.1016\n",
      "Epoch 2560/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.8602 - loss: 0.3395 - mse: 0.1023\n",
      "Epoch 2561/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.8550 - loss: 0.3504 - mse: 0.1061\n",
      "Epoch 2562/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8567 - loss: 0.3505 - mse: 0.1058\n",
      "Epoch 2563/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - accuracy: 0.8499 - loss: 0.3632 - mse: 0.1092\n",
      "Epoch 2564/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.8573 - loss: 0.3479 - mse: 0.1051\n",
      "Epoch 2565/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8596 - loss: 0.3389 - mse: 0.1013\n",
      "Epoch 2566/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8642 - loss: 0.3273 - mse: 0.0988\n",
      "Epoch 2567/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.8642 - loss: 0.3294 - mse: 0.0996\n",
      "Epoch 2568/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8619 - loss: 0.3391 - mse: 0.1012\n",
      "Epoch 2569/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.8596 - loss: 0.3371 - mse: 0.1019\n",
      "Epoch 2570/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8653 - loss: 0.3319 - mse: 0.0997\n",
      "Epoch 2571/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664ms/step - accuracy: 0.8676 - loss: 0.3257 - mse: 0.0983\n",
      "Epoch 2572/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 0.8687 - loss: 0.3257 - mse: 0.0982\n",
      "Epoch 2573/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8619 - loss: 0.3308 - mse: 0.1000\n",
      "Epoch 2574/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8647 - loss: 0.3368 - mse: 0.1013\n",
      "Epoch 2575/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8556 - loss: 0.3494 - mse: 0.1062\n",
      "Epoch 2576/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8567 - loss: 0.3518 - mse: 0.1059\n",
      "Epoch 2577/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8465 - loss: 0.3683 - mse: 0.1115\n",
      "Epoch 2578/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8573 - loss: 0.3514 - mse: 0.1060\n",
      "Epoch 2579/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8596 - loss: 0.3423 - mse: 0.1026\n",
      "Epoch 2580/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8670 - loss: 0.3273 - mse: 0.0988\n",
      "Epoch 2581/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8647 - loss: 0.3300 - mse: 0.0997\n",
      "Epoch 2582/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8619 - loss: 0.3412 - mse: 0.1016\n",
      "Epoch 2583/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8642 - loss: 0.3353 - mse: 0.1013\n",
      "Epoch 2584/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8687 - loss: 0.3273 - mse: 0.0984\n",
      "Epoch 2585/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.8682 - loss: 0.3268 - mse: 0.0984\n",
      "Epoch 2586/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.8642 - loss: 0.3322 - mse: 0.1002\n",
      "Epoch 2587/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8590 - loss: 0.3378 - mse: 0.1019\n",
      "Epoch 2588/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.8642 - loss: 0.3337 - mse: 0.1005\n",
      "Epoch 2589/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8619 - loss: 0.3304 - mse: 0.1000\n",
      "Epoch 2590/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.8699 - loss: 0.3265 - mse: 0.0983\n",
      "Epoch 2591/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445ms/step - accuracy: 0.8699 - loss: 0.3250 - mse: 0.0980\n",
      "Epoch 2592/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8682 - loss: 0.3254 - mse: 0.0982\n",
      "Epoch 2593/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8687 - loss: 0.3270 - mse: 0.0984\n",
      "Epoch 2594/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8630 - loss: 0.3294 - mse: 0.0996\n",
      "Epoch 2595/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - accuracy: 0.8647 - loss: 0.3301 - mse: 0.0994\n",
      "Epoch 2596/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - accuracy: 0.8636 - loss: 0.3304 - mse: 0.0998\n",
      "Epoch 2597/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8687 - loss: 0.3277 - mse: 0.0988\n",
      "Epoch 2598/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.8699 - loss: 0.3255 - mse: 0.0982\n",
      "Epoch 2599/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2600/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8682 - loss: 0.3260 - mse: 0.0983\n",
      "Epoch 2601/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8682 - loss: 0.3273 - mse: 0.0986\n",
      "Epoch 2602/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.8682 - loss: 0.3268 - mse: 0.0986\n",
      "Epoch 2603/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8699 - loss: 0.3257 - mse: 0.0982\n",
      "Epoch 2604/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8687 - loss: 0.3247 - mse: 0.0979\n",
      "Epoch 2605/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8693 - loss: 0.3248 - mse: 0.0979\n",
      "Epoch 2606/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.8676 - loss: 0.3256 - mse: 0.0982\n",
      "Epoch 2607/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0983\n",
      "Epoch 2608/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8647 - loss: 0.3266 - mse: 0.0986\n",
      "Epoch 2609/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8687 - loss: 0.3261 - mse: 0.0983\n",
      "Epoch 2610/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.8670 - loss: 0.3256 - mse: 0.0982\n",
      "Epoch 2611/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8693 - loss: 0.3249 - mse: 0.0979\n",
      "Epoch 2612/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8676 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2613/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366ms/step - accuracy: 0.8676 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2614/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8682 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2615/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2616/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8693 - loss: 0.3251 - mse: 0.0980\n",
      "Epoch 2617/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8676 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2618/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8687 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2619/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8687 - loss: 0.3245 - mse: 0.0979\n",
      "Epoch 2620/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.8687 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2621/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8682 - loss: 0.3247 - mse: 0.0979\n",
      "Epoch 2622/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8682 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2623/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.8687 - loss: 0.3248 - mse: 0.0979\n",
      "Epoch 2624/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8682 - loss: 0.3247 - mse: 0.0979\n",
      "Epoch 2625/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.8693 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2626/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 0.8687 - loss: 0.3245 - mse: 0.0978\n",
      "Epoch 2627/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.8693 - loss: 0.3245 - mse: 0.0978\n",
      "Epoch 2628/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.8687 - loss: 0.3245 - mse: 0.0978\n",
      "Epoch 2629/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.8676 - loss: 0.3245 - mse: 0.0979\n",
      "Epoch 2630/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8693 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2631/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8676 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2632/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8693 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2633/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8676 - loss: 0.3245 - mse: 0.0978\n",
      "Epoch 2634/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8687 - loss: 0.3245 - mse: 0.0978\n",
      "Epoch 2635/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.8693 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2636/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2637/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2638/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2639/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.8693 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2640/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8682 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2641/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2642/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.8699 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2643/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2644/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8699 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2645/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2646/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2647/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2648/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2649/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2650/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2651/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8699 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2652/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2653/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8699 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2654/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2655/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2656/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2657/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2658/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2659/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2660/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2661/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2662/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2663/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2664/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2665/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2666/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2667/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2668/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2669/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8699 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2670/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2671/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2672/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2673/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2674/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2675/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2676/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2677/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2678/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2679/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2680/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2681/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2682/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2683/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2684/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2685/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2686/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2687/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2688/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2689/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2690/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2691/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2692/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2693/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2694/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2695/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2696/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2697/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2698/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2699/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2700/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8693 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2701/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2702/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2703/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2704/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2705/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2706/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2707/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2708/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2709/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2710/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2711/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2712/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2713/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2714/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2715/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8693 - loss: 0.3240 - mse: 0.0977\n",
      "Epoch 2716/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2717/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8693 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2718/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2719/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8693 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2720/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8687 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2721/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8693 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2722/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8676 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2723/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3241 - mse: 0.0977\n",
      "Epoch 2724/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8676 - loss: 0.3242 - mse: 0.0977\n",
      "Epoch 2725/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8687 - loss: 0.3242 - mse: 0.0978\n",
      "Epoch 2726/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8687 - loss: 0.3243 - mse: 0.0978\n",
      "Epoch 2727/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8693 - loss: 0.3244 - mse: 0.0978\n",
      "Epoch 2728/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3246 - mse: 0.0979\n",
      "Epoch 2729/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8693 - loss: 0.3249 - mse: 0.0980\n",
      "Epoch 2730/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8687 - loss: 0.3253 - mse: 0.0981\n",
      "Epoch 2731/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8682 - loss: 0.3258 - mse: 0.0983\n",
      "Epoch 2732/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8659 - loss: 0.3269 - mse: 0.0986\n",
      "Epoch 2733/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8653 - loss: 0.3278 - mse: 0.0989\n",
      "Epoch 2734/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8642 - loss: 0.3301 - mse: 0.0995\n",
      "Epoch 2735/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8659 - loss: 0.3314 - mse: 0.1000\n",
      "Epoch 2736/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8596 - loss: 0.3355 - mse: 0.1011\n",
      "Epoch 2737/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.8619 - loss: 0.3361 - mse: 0.1015\n",
      "Epoch 2738/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8596 - loss: 0.3402 - mse: 0.1023\n",
      "Epoch 2739/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8647 - loss: 0.3278 - mse: 0.0990\n",
      "Epoch 2740/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.8693 - loss: 0.3260 - mse: 0.0981\n",
      "Epoch 2741/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8642 - loss: 0.3318 - mse: 0.1003\n",
      "Epoch 2742/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8487 - loss: 0.3630 - mse: 0.1087\n",
      "Epoch 2743/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8231 - loss: 0.4549 - mse: 0.1393\n",
      "Epoch 2744/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7654 - loss: 0.7947 - mse: 0.1970\n",
      "Epoch 2745/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.8704 - loss: 0.3300 - mse: 0.0990\n",
      "Epoch 2746/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.5554 - loss: 0.9545 - mse: 0.2961\n",
      "Epoch 2747/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7597 - loss: 1.9051 - mse: 0.2378\n",
      "Epoch 2748/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7597 - loss: 2.4272 - mse: 0.2395\n",
      "Epoch 2749/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.7962 - loss: 0.9128 - mse: 0.1750\n",
      "Epoch 2750/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.2426 - loss: 2.5895 - mse: 0.6216\n",
      "Epoch 2751/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8396 - loss: 0.7134 - mse: 0.1365\n",
      "Epoch 2752/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.7814 - loss: 1.7740 - mse: 0.2084\n",
      "Epoch 2753/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8025 - loss: 1.4572 - mse: 0.1863\n",
      "Epoch 2754/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.8664 - loss: 0.6293 - mse: 0.1194\n",
      "Epoch 2755/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7523 - loss: 1.1976 - mse: 0.2094\n",
      "Epoch 2756/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8373 - loss: 0.7992 - mse: 0.1412\n",
      "Epoch 2757/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8653 - loss: 0.5838 - mse: 0.1184\n",
      "Epoch 2758/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8425 - loss: 0.7929 - mse: 0.1389\n",
      "Epoch 2759/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8413 - loss: 0.7585 - mse: 0.1387\n",
      "Epoch 2760/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8607 - loss: 0.4932 - mse: 0.1160\n",
      "Epoch 2761/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8476 - loss: 0.5622 - mse: 0.1284\n",
      "Epoch 2762/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8345 - loss: 0.6278 - mse: 0.1467\n",
      "Epoch 2763/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8642 - loss: 0.4360 - mse: 0.1121\n",
      "Epoch 2764/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8419 - loss: 0.6130 - mse: 0.1351\n",
      "Epoch 2765/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8425 - loss: 0.5822 - mse: 0.1332\n",
      "Epoch 2766/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8647 - loss: 0.3943 - mse: 0.1091\n",
      "Epoch 2767/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8362 - loss: 0.5633 - mse: 0.1481\n",
      "Epoch 2768/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8687 - loss: 0.3929 - mse: 0.1060\n",
      "Epoch 2769/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8579 - loss: 0.4237 - mse: 0.1165\n",
      "Epoch 2770/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8493 - loss: 0.4806 - mse: 0.1262\n",
      "Epoch 2771/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8602 - loss: 0.3870 - mse: 0.1118\n",
      "Epoch 2772/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8664 - loss: 0.4027 - mse: 0.1100\n",
      "Epoch 2773/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8624 - loss: 0.4150 - mse: 0.1139\n",
      "Epoch 2774/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8596 - loss: 0.3660 - mse: 0.1089\n",
      "Epoch 2775/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8487 - loss: 0.4262 - mse: 0.1225\n",
      "Epoch 2776/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8584 - loss: 0.3698 - mse: 0.1110\n",
      "Epoch 2777/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8687 - loss: 0.3739 - mse: 0.1061\n",
      "Epoch 2778/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8664 - loss: 0.3831 - mse: 0.1090\n",
      "Epoch 2779/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8584 - loss: 0.3537 - mse: 0.1073\n",
      "Epoch 2780/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8419 - loss: 0.3898 - mse: 0.1176\n",
      "Epoch 2781/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8590 - loss: 0.3441 - mse: 0.1046\n",
      "Epoch 2782/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8647 - loss: 0.3742 - mse: 0.1082\n",
      "Epoch 2783/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8682 - loss: 0.3464 - mse: 0.1016\n",
      "Epoch 2784/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8505 - loss: 0.3547 - mse: 0.1091\n",
      "Epoch 2785/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8447 - loss: 0.3602 - mse: 0.1111\n",
      "Epoch 2786/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8664 - loss: 0.3367 - mse: 0.1006\n",
      "Epoch 2787/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.8676 - loss: 0.3617 - mse: 0.1058\n",
      "Epoch 2788/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8653 - loss: 0.3347 - mse: 0.1008\n",
      "Epoch 2789/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8539 - loss: 0.3521 - mse: 0.1083\n",
      "Epoch 2790/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8619 - loss: 0.3390 - mse: 0.1034\n",
      "Epoch 2791/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8670 - loss: 0.3418 - mse: 0.1013\n",
      "Epoch 2792/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8670 - loss: 0.3412 - mse: 0.1012\n",
      "Epoch 2793/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8607 - loss: 0.3359 - mse: 0.1020\n",
      "Epoch 2794/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8607 - loss: 0.3425 - mse: 0.1047\n",
      "Epoch 2795/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8664 - loss: 0.3315 - mse: 0.0999\n",
      "Epoch 2796/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8682 - loss: 0.3406 - mse: 0.1011\n",
      "Epoch 2797/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8682 - loss: 0.3314 - mse: 0.0995\n",
      "Epoch 2798/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8619 - loss: 0.3361 - mse: 0.1026\n",
      "Epoch 2799/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8602 - loss: 0.3338 - mse: 0.1016\n",
      "Epoch 2800/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8687 - loss: 0.3319 - mse: 0.0994\n",
      "Epoch 2801/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8687 - loss: 0.3347 - mse: 0.0998\n",
      "Epoch 2802/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8664 - loss: 0.3299 - mse: 0.0998\n",
      "Epoch 2803/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8613 - loss: 0.3338 - mse: 0.1017\n",
      "Epoch 2804/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8664 - loss: 0.3297 - mse: 0.0998\n",
      "Epoch 2805/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8687 - loss: 0.3320 - mse: 0.0994\n",
      "Epoch 2806/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8676 - loss: 0.3303 - mse: 0.0992\n",
      "Epoch 2807/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8664 - loss: 0.3301 - mse: 0.1000\n",
      "Epoch 2808/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8653 - loss: 0.3311 - mse: 0.1004\n",
      "Epoch 2809/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8670 - loss: 0.3288 - mse: 0.0991\n",
      "Epoch 2810/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8693 - loss: 0.3308 - mse: 0.0992\n",
      "Epoch 2811/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8687 - loss: 0.3286 - mse: 0.0990\n",
      "Epoch 2812/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8659 - loss: 0.3297 - mse: 0.0999\n",
      "Epoch 2813/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.8659 - loss: 0.3290 - mse: 0.0996\n",
      "Epoch 2814/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8676 - loss: 0.3286 - mse: 0.0989\n",
      "Epoch 2815/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8682 - loss: 0.3292 - mse: 0.0989\n",
      "Epoch 2816/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8653 - loss: 0.3281 - mse: 0.0991\n",
      "Epoch 2817/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8659 - loss: 0.3290 - mse: 0.0997\n",
      "Epoch 2818/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8647 - loss: 0.3278 - mse: 0.0991\n",
      "Epoch 2819/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8687 - loss: 0.3281 - mse: 0.0988\n",
      "Epoch 2820/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8687 - loss: 0.3279 - mse: 0.0988\n",
      "Epoch 2821/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8659 - loss: 0.3277 - mse: 0.0991\n",
      "Epoch 2822/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8647 - loss: 0.3279 - mse: 0.0992\n",
      "Epoch 2823/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8682 - loss: 0.3273 - mse: 0.0988\n",
      "Epoch 2824/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8693 - loss: 0.3276 - mse: 0.0987\n",
      "Epoch 2825/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8682 - loss: 0.3272 - mse: 0.0987\n",
      "Epoch 2826/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8653 - loss: 0.3274 - mse: 0.0991\n",
      "Epoch 2827/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8659 - loss: 0.3270 - mse: 0.0989\n",
      "Epoch 2828/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8687 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2829/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8687 - loss: 0.3270 - mse: 0.0986\n",
      "Epoch 2830/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8664 - loss: 0.3269 - mse: 0.0988\n",
      "Epoch 2831/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8653 - loss: 0.3269 - mse: 0.0989\n",
      "Epoch 2832/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8670 - loss: 0.3266 - mse: 0.0987\n",
      "Epoch 2833/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8687 - loss: 0.3267 - mse: 0.0985\n",
      "Epoch 2834/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8693 - loss: 0.3265 - mse: 0.0986\n",
      "Epoch 2835/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8670 - loss: 0.3266 - mse: 0.0988\n",
      "Epoch 2836/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8682 - loss: 0.3264 - mse: 0.0987\n",
      "Epoch 2837/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0985\n",
      "Epoch 2838/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8687 - loss: 0.3264 - mse: 0.0985\n",
      "Epoch 2839/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8687 - loss: 0.3263 - mse: 0.0986\n",
      "Epoch 2840/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8670 - loss: 0.3263 - mse: 0.0987\n",
      "Epoch 2841/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8682 - loss: 0.3261 - mse: 0.0985\n",
      "Epoch 2842/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8687 - loss: 0.3262 - mse: 0.0984\n",
      "Epoch 2843/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8687 - loss: 0.3260 - mse: 0.0985\n",
      "Epoch 2844/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8670 - loss: 0.3260 - mse: 0.0986\n",
      "Epoch 2845/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8670 - loss: 0.3260 - mse: 0.0985\n",
      "Epoch 2846/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0984\n",
      "Epoch 2847/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3259 - mse: 0.0984\n",
      "Epoch 2848/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8670 - loss: 0.3258 - mse: 0.0985\n",
      "Epoch 2849/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8670 - loss: 0.3258 - mse: 0.0985\n",
      "Epoch 2850/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8670 - loss: 0.3257 - mse: 0.0984\n",
      "Epoch 2851/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8687 - loss: 0.3257 - mse: 0.0984\n",
      "Epoch 2852/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8687 - loss: 0.3256 - mse: 0.0984\n",
      "Epoch 2853/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8670 - loss: 0.3256 - mse: 0.0984\n",
      "Epoch 2854/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8670 - loss: 0.3255 - mse: 0.0984\n",
      "Epoch 2855/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8693 - loss: 0.3255 - mse: 0.0984\n",
      "Epoch 2856/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8693 - loss: 0.3255 - mse: 0.0983\n",
      "Epoch 2857/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8693 - loss: 0.3255 - mse: 0.0984\n",
      "Epoch 2858/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8687 - loss: 0.3254 - mse: 0.0984\n",
      "Epoch 2859/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8693 - loss: 0.3253 - mse: 0.0983\n",
      "Epoch 2860/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8693 - loss: 0.3254 - mse: 0.0983\n",
      "Epoch 2861/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8693 - loss: 0.3253 - mse: 0.0983\n",
      "Epoch 2862/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3253 - mse: 0.0983\n",
      "Epoch 2863/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8693 - loss: 0.3252 - mse: 0.0983\n",
      "Epoch 2864/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8693 - loss: 0.3252 - mse: 0.0983\n",
      "Epoch 2865/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8693 - loss: 0.3251 - mse: 0.0983\n",
      "Epoch 2866/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0983\n",
      "Epoch 2867/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8693 - loss: 0.3252 - mse: 0.0983\n",
      "Epoch 2868/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0982\n",
      "Epoch 2869/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8693 - loss: 0.3251 - mse: 0.0982\n",
      "Epoch 2870/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8687 - loss: 0.3251 - mse: 0.0983\n",
      "Epoch 2871/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8687 - loss: 0.3250 - mse: 0.0983\n",
      "Epoch 2872/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8687 - loss: 0.3250 - mse: 0.0982\n",
      "Epoch 2873/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8693 - loss: 0.3249 - mse: 0.0982\n",
      "Epoch 2874/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8693 - loss: 0.3249 - mse: 0.0982\n",
      "Epoch 2875/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8676 - loss: 0.3249 - mse: 0.0983\n",
      "Epoch 2876/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8693 - loss: 0.3248 - mse: 0.0982\n",
      "Epoch 2877/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8693 - loss: 0.3249 - mse: 0.0982\n",
      "Epoch 2878/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8693 - loss: 0.3247 - mse: 0.0982\n",
      "Epoch 2879/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8687 - loss: 0.3249 - mse: 0.0982\n",
      "Epoch 2880/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8693 - loss: 0.3248 - mse: 0.0982\n",
      "Epoch 2881/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3247 - mse: 0.0982\n",
      "Epoch 2882/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8693 - loss: 0.3247 - mse: 0.0981\n",
      "Epoch 2883/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8693 - loss: 0.3246 - mse: 0.0982\n",
      "Epoch 2884/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8693 - loss: 0.3247 - mse: 0.0982\n",
      "Epoch 2885/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3246 - mse: 0.0982\n",
      "Epoch 2886/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8687 - loss: 0.3245 - mse: 0.0981\n",
      "Epoch 2887/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8687 - loss: 0.3247 - mse: 0.0981\n",
      "Epoch 2888/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8687 - loss: 0.3245 - mse: 0.0981\n",
      "Epoch 2889/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8687 - loss: 0.3246 - mse: 0.0981\n",
      "Epoch 2890/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8687 - loss: 0.3247 - mse: 0.0981\n",
      "Epoch 2891/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8687 - loss: 0.3246 - mse: 0.0981\n",
      "Epoch 2892/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8687 - loss: 0.3244 - mse: 0.0981\n",
      "Epoch 2893/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8693 - loss: 0.3245 - mse: 0.0981\n",
      "Epoch 2894/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8699 - loss: 0.3245 - mse: 0.0981\n",
      "Epoch 2895/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8699 - loss: 0.3243 - mse: 0.0981\n",
      "Epoch 2896/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3244 - mse: 0.0981\n",
      "Epoch 2897/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0981\n",
      "Epoch 2898/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0980\n",
      "Epoch 2899/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0980\n",
      "Epoch 2900/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0981\n",
      "Epoch 2901/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3242 - mse: 0.0981\n",
      "Epoch 2902/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0980\n",
      "Epoch 2903/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0980\n",
      "Epoch 2904/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0980\n",
      "Epoch 2905/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.8699 - loss: 0.3241 - mse: 0.0980\n",
      "Epoch 2906/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3240 - mse: 0.0980\n",
      "Epoch 2907/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8699 - loss: 0.3240 - mse: 0.0980\n",
      "Epoch 2908/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3240 - mse: 0.0980\n",
      "Epoch 2909/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3240 - mse: 0.0980\n",
      "Epoch 2910/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8699 - loss: 0.3240 - mse: 0.0980\n",
      "Epoch 2911/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3239 - mse: 0.0980\n",
      "Epoch 2912/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8693 - loss: 0.3240 - mse: 0.0980\n",
      "Epoch 2913/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8693 - loss: 0.3239 - mse: 0.0980\n",
      "Epoch 2914/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3239 - mse: 0.0980\n",
      "Epoch 2915/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8699 - loss: 0.3239 - mse: 0.0980\n",
      "Epoch 2916/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8693 - loss: 0.3238 - mse: 0.0979\n",
      "Epoch 2917/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8693 - loss: 0.3239 - mse: 0.0980\n",
      "Epoch 2918/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3238 - mse: 0.0979\n",
      "Epoch 2919/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8693 - loss: 0.3238 - mse: 0.0979\n",
      "Epoch 2920/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8693 - loss: 0.3238 - mse: 0.0979\n",
      "Epoch 2921/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8693 - loss: 0.3237 - mse: 0.0979\n",
      "Epoch 2922/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8693 - loss: 0.3237 - mse: 0.0979\n",
      "Epoch 2923/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8699 - loss: 0.3237 - mse: 0.0979\n",
      "Epoch 2924/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8693 - loss: 0.3237 - mse: 0.0979\n",
      "Epoch 2925/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8693 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2926/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8699 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2927/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2928/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2929/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2930/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3235 - mse: 0.0979\n",
      "Epoch 2931/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8693 - loss: 0.3235 - mse: 0.0979\n",
      "Epoch 2932/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8693 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2933/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8687 - loss: 0.3235 - mse: 0.0978\n",
      "Epoch 2934/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2935/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3236 - mse: 0.0979\n",
      "Epoch 2936/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8693 - loss: 0.3235 - mse: 0.0978\n",
      "Epoch 2937/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8693 - loss: 0.3234 - mse: 0.0978\n",
      "Epoch 2938/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8699 - loss: 0.3234 - mse: 0.0978\n",
      "Epoch 2939/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.8687 - loss: 0.3234 - mse: 0.0978\n",
      "Epoch 2940/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8693 - loss: 0.3234 - mse: 0.0978\n",
      "Epoch 2941/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8699 - loss: 0.3234 - mse: 0.0978\n",
      "Epoch 2942/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8699 - loss: 0.3234 - mse: 0.0978\n",
      "Epoch 2943/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8693 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2944/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8693 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2945/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8687 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2946/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8682 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2947/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8687 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2948/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8687 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2949/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8682 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2950/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.8682 - loss: 0.3232 - mse: 0.0978\n",
      "Epoch 2951/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8682 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2952/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8682 - loss: 0.3232 - mse: 0.0978\n",
      "Epoch 2953/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8682 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2954/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8682 - loss: 0.3233 - mse: 0.0978\n",
      "Epoch 2955/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8682 - loss: 0.3232 - mse: 0.0978\n",
      "Epoch 2956/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8682 - loss: 0.3232 - mse: 0.0978\n",
      "Epoch 2957/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3232 - mse: 0.0977\n",
      "Epoch 2958/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0978\n",
      "Epoch 2959/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0978\n",
      "Epoch 2960/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2961/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3231 - mse: 0.0978\n",
      "Epoch 2962/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2963/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2964/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2965/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2966/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8682 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2967/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8682 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2968/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8682 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2969/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8687 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2970/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8687 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2971/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8687 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2972/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8682 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2973/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.8682 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2974/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8682 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2975/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8687 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2976/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8687 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2977/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8693 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2978/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2979/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8682 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2980/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8693 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2981/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2982/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8682 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2983/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2984/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2985/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8693 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2986/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.8699 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2987/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8699 - loss: 0.3230 - mse: 0.0977\n",
      "Epoch 2988/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8682 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2989/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.8699 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2990/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8682 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2991/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8693 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2992/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3228 - mse: 0.0977\n",
      "Epoch 2993/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3230 - mse: 0.0976\n",
      "Epoch 2994/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8693 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 2995/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8693 - loss: 0.3230 - mse: 0.0976\n",
      "Epoch 2996/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3230 - mse: 0.0976\n",
      "Epoch 2997/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 2998/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8682 - loss: 0.3229 - mse: 0.0977\n",
      "Epoch 2999/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8687 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3000/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8693 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3001/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3002/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8693 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3003/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3004/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3005/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8682 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3006/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8682 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3007/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.8682 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3008/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3009/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3010/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3011/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3012/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3013/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3014/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3015/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3016/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3017/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3018/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3019/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3020/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3021/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3022/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3023/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.8693 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3024/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.8687 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3025/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3226 - mse: 0.0976\n",
      "Epoch 3026/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3027/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8693 - loss: 0.3226 - mse: 0.0976\n",
      "Epoch 3028/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8687 - loss: 0.3226 - mse: 0.0976\n",
      "Epoch 3029/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3030/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8687 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3031/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3032/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0975\n",
      "Epoch 3033/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.8687 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3034/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8687 - loss: 0.3229 - mse: 0.0976\n",
      "Epoch 3035/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8682 - loss: 0.3226 - mse: 0.0976\n",
      "Epoch 3036/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8699 - loss: 0.3225 - mse: 0.0975\n",
      "Epoch 3037/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8699 - loss: 0.3229 - mse: 0.0976\n",
      "Epoch 3038/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3226 - mse: 0.0976\n",
      "Epoch 3039/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8693 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3040/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3228 - mse: 0.0975\n",
      "Epoch 3041/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8693 - loss: 0.3229 - mse: 0.0976\n",
      "Epoch 3042/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3229 - mse: 0.0975\n",
      "Epoch 3043/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8693 - loss: 0.3228 - mse: 0.0975\n",
      "Epoch 3044/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8687 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3045/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8687 - loss: 0.3226 - mse: 0.0976\n",
      "Epoch 3046/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8693 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3047/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8699 - loss: 0.3231 - mse: 0.0975\n",
      "Epoch 3048/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3230 - mse: 0.0976\n",
      "Epoch 3049/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 3050/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3225 - mse: 0.0975\n",
      "Epoch 3051/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3052/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.8693 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3053/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0975\n",
      "Epoch 3054/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3055/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8693 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 3056/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8693 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 3057/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.8693 - loss: 0.3228 - mse: 0.0975\n",
      "Epoch 3058/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8704 - loss: 0.3227 - mse: 0.0975\n",
      "Epoch 3059/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8682 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3060/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 3061/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8693 - loss: 0.3230 - mse: 0.0976\n",
      "Epoch 3062/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3230 - mse: 0.0976\n",
      "Epoch 3063/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8699 - loss: 0.3230 - mse: 0.0976\n",
      "Epoch 3064/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8687 - loss: 0.3230 - mse: 0.0975\n",
      "Epoch 3065/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8699 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 3066/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8693 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3067/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8699 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 3068/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8687 - loss: 0.3225 - mse: 0.0976\n",
      "Epoch 3069/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3225 - mse: 0.0975\n",
      "Epoch 3070/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.8699 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3071/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8682 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3072/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3225 - mse: 0.0975\n",
      "Epoch 3073/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 3074/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 3075/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8699 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 3076/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0975\n",
      "Epoch 3077/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0975\n",
      "Epoch 3078/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0975\n",
      "Epoch 3079/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3080/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0975\n",
      "Epoch 3081/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0975\n",
      "Epoch 3082/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8704 - loss: 0.3225 - mse: 0.0975\n",
      "Epoch 3083/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8687 - loss: 0.3225 - mse: 0.0976\n",
      "Epoch 3084/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8704 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 3085/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3086/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3087/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8693 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3088/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3089/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3090/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3091/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.8699 - loss: 0.3223 - mse: 0.0974\n",
      "Epoch 3092/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8699 - loss: 0.3223 - mse: 0.0974\n",
      "Epoch 3093/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0974\n",
      "Epoch 3094/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0975\n",
      "Epoch 3095/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3096/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3097/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3098/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3099/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8704 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3100/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3101/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3223 - mse: 0.0974\n",
      "Epoch 3102/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3223 - mse: 0.0974\n",
      "Epoch 3103/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8693 - loss: 0.3223 - mse: 0.0974\n",
      "Epoch 3104/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0974\n",
      "Epoch 3105/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3106/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3107/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3108/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8699 - loss: 0.3224 - mse: 0.0974\n",
      "Epoch 3109/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3110/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3111/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8682 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3112/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3113/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3114/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3115/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3116/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0974\n",
      "Epoch 3117/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3118/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3119/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3120/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0974\n",
      "Epoch 3121/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8693 - loss: 0.3222 - mse: 0.0974\n",
      "Epoch 3122/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3123/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8693 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3124/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3125/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3126/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8704 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3127/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3128/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3222 - mse: 0.0975\n",
      "Epoch 3129/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3130/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3131/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8687 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3132/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0975\n",
      "Epoch 3133/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3134/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3135/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0975\n",
      "Epoch 3136/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3137/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3138/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8693 - loss: 0.3223 - mse: 0.0975\n",
      "Epoch 3139/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3140/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8704 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3141/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8704 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3142/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3143/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3144/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3145/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3146/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3147/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3148/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3149/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3150/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3151/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3152/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3153/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3154/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3155/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3156/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3157/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3158/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3159/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8687 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3160/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3161/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3162/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3163/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8704 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3164/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3165/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3166/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8693 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3167/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3168/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3169/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3170/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0973\n",
      "Epoch 3171/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3172/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3173/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3174/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3175/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3176/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3177/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3178/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3179/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3180/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3181/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3182/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3183/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3184/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0974\n",
      "Epoch 3185/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3186/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3187/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3188/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3189/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3190/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8687 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3191/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3192/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3193/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3219 - mse: 0.0973\n",
      "Epoch 3194/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3195/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3196/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3197/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8687 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3198/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3199/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3200/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3201/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8687 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3202/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 3203/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8693 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3204/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3205/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8682 - loss: 0.3221 - mse: 0.0974\n",
      "Epoch 3206/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8682 - loss: 0.3219 - mse: 0.0973\n",
      "Epoch 3207/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3208/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3209/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8687 - loss: 0.3219 - mse: 0.0973\n",
      "Epoch 3210/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8693 - loss: 0.3219 - mse: 0.0974\n",
      "Epoch 3211/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3212/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3213/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3214/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3215/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3216/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3217/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3218/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3219/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3220/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3221/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3222/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3223/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3224/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3225/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3226/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3227/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3228/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3229/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3230/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3231/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3232/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3233/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3234/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3235/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3236/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3237/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3238/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3218 - mse: 0.0973\n",
      "Epoch 3239/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8687 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3240/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3241/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3242/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3243/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3244/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3245/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3246/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3247/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3248/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3249/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3250/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3251/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8687 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3252/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3253/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3254/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3255/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3256/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8693 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3257/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3258/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3259/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3260/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3261/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3262/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3263/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3264/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3265/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3266/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3267/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3268/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3269/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3270/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3271/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3272/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3273/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3274/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3275/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3276/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3277/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3278/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3279/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3280/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3281/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3282/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3283/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3284/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3285/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3286/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8693 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3287/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3288/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3289/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3290/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3291/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3292/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3293/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3294/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8693 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3295/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3296/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3297/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8693 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3298/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8693 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 3299/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3300/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3301/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3302/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3303/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3304/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3305/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3306/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3307/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3308/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3309/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3310/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3311/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0973\n",
      "Epoch 3312/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3313/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3314/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3315/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3316/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3317/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3318/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3319/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3320/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3321/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3322/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3323/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3324/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3325/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3326/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3327/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3328/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 3329/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3330/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3331/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3332/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3333/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3334/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3335/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3336/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3337/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3338/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3339/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3340/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3341/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3342/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3343/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3344/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3345/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3346/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3347/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3348/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3349/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3350/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3351/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3352/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3353/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3354/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3355/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3356/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3357/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3358/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3359/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3360/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3361/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3362/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3363/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3364/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3365/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3366/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3367/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3368/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3369/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3370/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3371/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3372/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3373/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3374/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3375/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3376/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3377/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3378/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3379/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3380/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3381/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3382/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3383/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3384/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3385/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3386/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3387/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3388/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3389/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3390/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3391/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.8687 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3392/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3393/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3394/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3395/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3396/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3397/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3398/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3399/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8704 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3400/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3401/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3402/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3403/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3404/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3405/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3406/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8693 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3407/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3408/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3409/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3410/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8687 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3411/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3412/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0972\n",
      "Epoch 3413/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8710 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3414/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3415/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.8693 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3416/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3417/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3418/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3419/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3420/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3421/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3422/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3423/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3424/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3425/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3426/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8710 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3427/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3428/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3429/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3430/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3431/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0972\n",
      "Epoch 3432/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3433/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3434/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3435/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3436/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3437/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3438/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3439/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3440/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3441/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3442/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3443/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3444/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3445/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3446/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3447/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3448/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3449/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3450/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3451/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3452/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3453/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3454/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3455/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3456/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3457/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3458/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3459/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3460/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3461/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3462/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3463/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3464/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3465/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3466/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3467/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3468/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3469/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3470/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8693 - loss: 0.3212 - mse: 0.0972\n",
      "Epoch 3471/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3472/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3473/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3474/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3475/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3476/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3477/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3478/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8693 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3479/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3480/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3481/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3482/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3483/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3484/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3485/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0971\n",
      "Epoch 3486/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3487/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3488/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3489/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8693 - loss: 0.3214 - mse: 0.0971\n",
      "Epoch 3490/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3491/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3492/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3493/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8687 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3494/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3495/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3496/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3497/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3498/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8687 - loss: 0.3216 - mse: 0.0973\n",
      "Epoch 3499/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3500/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3501/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3502/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3503/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3504/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3505/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3506/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8687 - loss: 0.3217 - mse: 0.0972\n",
      "Epoch 3507/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.8693 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 3508/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3509/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8693 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3510/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3214 - mse: 0.0971\n",
      "Epoch 3511/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3512/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3513/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3514/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8687 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3515/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8704 - loss: 0.3214 - mse: 0.0971\n",
      "Epoch 3516/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8710 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3517/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0972\n",
      "Epoch 3518/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8687 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3519/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3520/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3521/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3522/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8687 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3523/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3524/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3525/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8687 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3526/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3527/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3528/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3529/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8687 - loss: 0.3214 - mse: 0.0971\n",
      "Epoch 3530/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3531/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3532/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3533/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3534/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3535/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3536/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3537/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 3538/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8693 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3539/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3540/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3541/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8687 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 3542/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8687 - loss: 0.3215 - mse: 0.0971\n",
      "Epoch 3543/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3544/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3545/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3546/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8687 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3547/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3548/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3549/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3550/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8699 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3551/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8693 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 3552/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3553/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3554/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8693 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3555/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3556/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3557/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3558/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8693 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3559/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3560/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3561/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3562/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8693 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3563/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3564/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3565/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8693 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3566/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3567/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3568/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3569/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3570/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8687 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3571/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3572/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3573/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3574/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3575/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8687 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 3576/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3577/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3578/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3579/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3580/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8693 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3581/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3582/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3583/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8693 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3584/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3585/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8693 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3586/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3587/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3588/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3589/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3590/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3591/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0971\n",
      "Epoch 3592/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0971\n",
      "Epoch 3593/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3594/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3595/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3596/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3597/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3598/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3599/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3600/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3601/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3602/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3603/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3604/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3605/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3606/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3607/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3608/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3609/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8699 - loss: 0.3211 - mse: 0.0970\n",
      "Epoch 3610/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3611/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3612/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3613/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3614/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3615/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3616/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3617/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3618/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3619/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3620/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0970\n",
      "Epoch 3621/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3622/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3623/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3624/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3625/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3626/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3627/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3628/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3629/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3630/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3631/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3632/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3633/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3634/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3635/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3636/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3637/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3638/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3639/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3640/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3641/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3642/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3643/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3644/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3645/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3646/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3647/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3648/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3649/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3650/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3651/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3652/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3653/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3654/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3655/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3656/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3657/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3658/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3659/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3660/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3661/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3662/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3663/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3664/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3665/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3666/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3667/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3668/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3669/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3670/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3671/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3672/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3673/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8687 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3674/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3675/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8687 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3676/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3677/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3678/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3679/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3680/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8693 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3681/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3682/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3683/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3684/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3685/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3686/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3687/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3688/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3689/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3690/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3691/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3692/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3693/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3694/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3695/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3696/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3697/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3698/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3699/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3700/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3701/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3702/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3703/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3704/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3705/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3706/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3707/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3708/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3709/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3710/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3711/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3712/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3713/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3714/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3715/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3716/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3717/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3718/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3719/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3720/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3721/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3722/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3723/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3724/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3725/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3726/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3727/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3728/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3729/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3730/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3731/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3732/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3733/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3734/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3735/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3736/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3737/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3738/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3739/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3740/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3741/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3742/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3743/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3744/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3745/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3746/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3747/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3748/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3749/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3750/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3751/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3752/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3753/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8687 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 3754/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3755/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3756/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3757/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3758/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3759/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3760/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0970\n",
      "Epoch 3761/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3762/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3763/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3764/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3765/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8693 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3766/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3767/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3768/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3769/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3770/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3771/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3772/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8687 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3773/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3774/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3775/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3776/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3777/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.8687 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3778/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 3779/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8693 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3780/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3781/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3782/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3783/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3784/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3785/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3786/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8687 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 3787/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0969\n",
      "Epoch 3788/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8687 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 3789/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8704 - loss: 0.3208 - mse: 0.0969\n",
      "Epoch 3790/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3791/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3792/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3793/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3794/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3795/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8693 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3796/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3797/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3798/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3799/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3800/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3801/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3802/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8693 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3803/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3804/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3805/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3806/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3807/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3808/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3809/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3810/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3811/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3812/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3813/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3814/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3815/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3816/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3817/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3818/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8693 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3819/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3820/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3821/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8693 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3822/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3823/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3824/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3825/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3826/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3827/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3828/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3829/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3830/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3831/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3832/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3833/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3834/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3835/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3836/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3837/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3838/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3839/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3840/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3841/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3842/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.8693 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3843/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3844/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3845/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3846/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3847/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3848/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3849/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3850/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3851/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3852/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3853/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3854/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3855/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8687 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3856/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3857/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3858/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3859/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3860/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3861/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3862/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3863/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3864/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8687 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3865/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3866/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3867/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3868/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 3869/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8693 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3870/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8693 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3871/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8693 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3872/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8704 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 3873/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3874/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3875/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8687 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3876/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3877/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3878/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3879/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3880/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8687 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3881/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3882/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3883/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3884/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3885/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3886/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3887/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3888/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3889/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 3890/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3891/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8687 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3892/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3893/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3894/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3895/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3896/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8687 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3897/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 3898/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3899/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3900/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3901/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3902/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3903/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3904/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3905/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3906/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3907/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3908/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3909/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3910/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3911/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3912/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3913/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3914/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3915/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 3916/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3917/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3918/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3919/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3920/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3921/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3922/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3923/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3924/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3925/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3926/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3927/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3928/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3929/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3930/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3931/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3932/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3933/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3934/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3935/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3936/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3937/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3938/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3939/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3940/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 3941/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3942/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3943/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3944/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3945/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3946/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3947/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3948/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3949/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3950/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3951/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8687 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3952/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3953/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3954/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3955/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3956/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3957/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3958/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3959/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3960/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3961/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3962/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3963/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3964/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3965/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3966/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3967/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3968/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3969/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3970/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 3971/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 3972/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8693 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 3973/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 3974/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3975/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3976/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3977/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 3978/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3979/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3980/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3981/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3982/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3983/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 3984/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3985/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3986/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3987/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3988/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3989/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3990/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3991/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3992/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3993/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 3994/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3995/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3996/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 3997/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 3998/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8687 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 3999/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4000/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 4001/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0969\n",
      "Epoch 4002/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4003/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4004/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4005/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4006/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4007/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4008/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4009/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4010/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4011/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4012/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 4013/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4014/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4015/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4016/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4017/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8687 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4018/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4019/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4020/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4021/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4022/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8704 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4023/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4024/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4025/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8687 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4026/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4027/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8687 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4028/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4029/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4030/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4031/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4032/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4033/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4034/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4035/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4036/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4037/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4038/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8687 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4039/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4040/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4041/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4042/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4043/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4044/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4045/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4046/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4047/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4048/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4049/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4050/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4051/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4052/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4053/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8687 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4054/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4055/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4056/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4057/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4058/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0967\n",
      "Epoch 4059/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4060/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4061/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4062/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4063/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0967\n",
      "Epoch 4064/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4065/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4066/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4067/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0967\n",
      "Epoch 4068/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0967\n",
      "Epoch 4069/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8693 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4070/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4071/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8687 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4072/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4073/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4074/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4075/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8687 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4076/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4077/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4078/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4079/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4080/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4081/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4082/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4083/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4084/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4085/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4086/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4087/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8687 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 4088/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4089/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4090/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4091/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4092/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4093/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4094/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4095/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8687 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 4096/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4097/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4098/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4099/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4100/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4101/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4102/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4103/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4104/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 4105/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4106/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4107/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 4108/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4109/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8704 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 4110/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4111/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4112/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4113/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4114/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4115/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8687 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 4116/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 4117/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8710 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 4118/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 4119/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4120/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4121/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4122/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4123/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4124/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4125/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4126/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4127/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4128/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4129/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4130/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4131/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4132/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4133/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4134/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4135/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8687 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4136/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4137/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4138/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4139/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4140/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4141/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4142/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4143/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4144/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4145/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0967\n",
      "Epoch 4146/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8687 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4147/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0967\n",
      "Epoch 4148/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0967\n",
      "Epoch 4149/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4150/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4151/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4152/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4153/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4154/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4155/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4156/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4157/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8687 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4158/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4159/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4160/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4161/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4162/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4163/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4164/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4165/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4166/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4167/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4168/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4169/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4170/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4171/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4172/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4173/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4174/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4175/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4176/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4177/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4178/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4179/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4180/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4181/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4182/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8687 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4183/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4184/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4185/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4186/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4187/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4188/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4189/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4190/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4191/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4192/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4193/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4194/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4195/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4196/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4197/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4198/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4199/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4200/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4201/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4202/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4203/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0967\n",
      "Epoch 4204/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4205/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4206/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4207/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4208/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4209/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4210/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4211/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4212/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4213/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4214/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4215/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4216/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4217/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4218/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4219/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4220/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4221/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4222/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4223/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4224/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4225/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4226/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4227/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4228/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8693 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4229/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4230/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4231/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4232/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4233/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4234/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8687 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4235/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4236/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4237/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4238/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4239/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4240/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4241/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4242/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4243/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4244/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4245/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4246/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4247/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4248/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4249/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4250/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4251/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4252/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4253/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8693 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4254/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4255/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4256/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4257/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4258/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4259/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4260/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4261/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4262/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4263/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8687 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4264/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4265/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4266/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8710 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4267/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8710 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4268/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4269/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4270/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4271/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4272/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4273/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4274/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4275/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4276/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4277/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4278/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4279/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4280/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4281/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4282/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4283/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8693 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4284/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4285/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8710 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 4286/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8693 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 4287/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8710 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4288/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8710 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4289/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4290/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4291/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4292/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4293/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4294/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4295/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4296/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4297/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8710 - loss: 0.3200 - mse: 0.0968\n",
      "Epoch 4298/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4299/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4300/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4301/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4302/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4303/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4304/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4305/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4306/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4307/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4308/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4309/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4310/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4311/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4312/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4313/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4314/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4315/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4316/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4317/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4318/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4319/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4320/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4321/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4322/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4323/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4324/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4325/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4326/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8693 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4327/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4328/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4329/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4330/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4331/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4332/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4333/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4334/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4335/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4336/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4337/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4338/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8693 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4339/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4340/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4341/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4342/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4343/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4344/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4345/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4346/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4347/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8693 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4348/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4349/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4350/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4351/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8687 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4352/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4353/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0967\n",
      "Epoch 4354/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8710 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4355/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4356/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4357/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4358/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4359/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4360/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4361/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8693 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4362/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8710 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4363/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8687 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4364/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4365/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4366/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4367/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4368/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4369/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8687 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4370/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4371/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8687 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4372/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4373/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4374/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4375/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8693 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4376/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4377/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4378/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4379/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4380/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4381/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8693 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4382/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4383/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8693 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4384/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4385/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8693 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4386/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4387/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8710 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4388/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4389/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8710 - loss: 0.3208 - mse: 0.0969\n",
      "Epoch 4390/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0969\n",
      "Epoch 4391/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8693 - loss: 0.3211 - mse: 0.0970\n",
      "Epoch 4392/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8704 - loss: 0.3209 - mse: 0.0969\n",
      "Epoch 4393/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8693 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 4394/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8710 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 4395/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8710 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4396/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4397/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4398/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4399/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8687 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4400/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4401/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8693 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4402/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8693 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4403/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4404/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4405/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4406/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8710 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4407/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4408/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8710 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4409/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8710 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4410/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8710 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4411/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8710 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4412/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4413/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4414/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4415/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4416/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4417/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4418/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4419/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8687 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4420/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4421/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4422/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4423/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4424/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4425/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4426/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4427/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4428/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4429/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.8693 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4430/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4431/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8687 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4432/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4433/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8687 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4434/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4435/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8687 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4436/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8693 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4437/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4438/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4439/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4440/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8693 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4441/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4442/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4443/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4444/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.8710 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4445/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8710 - loss: 0.3208 - mse: 0.0969\n",
      "Epoch 4446/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8693 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 4447/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8710 - loss: 0.3215 - mse: 0.0971\n",
      "Epoch 4448/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3221 - mse: 0.0973\n",
      "Epoch 4449/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0972\n",
      "Epoch 4450/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8693 - loss: 0.3220 - mse: 0.0972\n",
      "Epoch 4451/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8704 - loss: 0.3213 - mse: 0.0970\n",
      "Epoch 4452/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8710 - loss: 0.3207 - mse: 0.0968\n",
      "Epoch 4453/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4454/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8687 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4455/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4456/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4457/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4458/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4459/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.8710 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4460/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4461/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8710 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4462/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4463/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4464/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4465/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4466/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4467/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4468/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8693 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4469/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8716 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4470/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8693 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4471/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4472/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4473/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8710 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4474/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4475/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4476/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8687 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4477/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4478/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4479/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4480/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4481/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8687 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4482/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4483/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4484/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8710 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4485/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4486/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4487/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4488/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4489/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4490/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4491/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8710 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4492/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4493/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8710 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4494/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8710 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4495/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8710 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4496/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8710 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4497/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4498/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8710 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4499/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8710 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4500/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8710 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4501/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8710 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4502/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4503/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4504/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4505/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4506/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4507/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4508/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4509/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4510/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8687 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4511/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4512/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4513/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4514/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4515/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4516/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8693 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4517/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4518/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4519/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4520/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4521/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.8687 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4522/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8693 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4523/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8687 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4524/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4525/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4526/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4527/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4528/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4529/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8687 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4530/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4531/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8693 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4532/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4533/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8687 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4534/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4535/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4536/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4537/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8710 - loss: 0.3203 - mse: 0.0967\n",
      "Epoch 4538/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8710 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4539/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8693 - loss: 0.3209 - mse: 0.0969\n",
      "Epoch 4540/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8710 - loss: 0.3211 - mse: 0.0970\n",
      "Epoch 4541/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 4542/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 4543/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8693 - loss: 0.3227 - mse: 0.0974\n",
      "Epoch 4544/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8716 - loss: 0.3228 - mse: 0.0975\n",
      "Epoch 4545/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8710 - loss: 0.3233 - mse: 0.0976\n",
      "Epoch 4546/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8716 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 4547/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.8687 - loss: 0.3225 - mse: 0.0974\n",
      "Epoch 4548/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 4549/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4550/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.8693 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4551/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8687 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4552/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0965\n",
      "Epoch 4553/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8710 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4554/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4555/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4556/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8710 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4557/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8704 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4558/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8710 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4559/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4560/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0965\n",
      "Epoch 4561/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4562/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4563/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4564/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4565/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.8687 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4566/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4567/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4568/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4569/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8710 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4570/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4571/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4572/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4573/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4574/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8716 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4575/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4576/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4577/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8687 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4578/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4579/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4580/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8693 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4581/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4582/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.8687 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4583/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8716 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4584/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4585/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4586/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.8710 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4587/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4588/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.8710 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4589/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8704 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4590/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4591/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4592/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4593/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4594/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8687 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4595/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0965\n",
      "Epoch 4596/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8687 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4597/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4598/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8687 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4599/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8710 - loss: 0.3190 - mse: 0.0965\n",
      "Epoch 4600/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8687 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4601/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8693 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4602/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4603/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8716 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4604/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4605/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8716 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4606/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4607/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4608/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4609/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8716 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4610/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4611/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8716 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4612/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4613/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4614/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4615/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8699 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4616/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8710 - loss: 0.3200 - mse: 0.0966\n",
      "Epoch 4617/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3201 - mse: 0.0967\n",
      "Epoch 4618/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8704 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4619/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8716 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4620/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4621/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 0.8716 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4622/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.8693 - loss: 0.3211 - mse: 0.0970\n",
      "Epoch 4623/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 4624/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0971\n",
      "Epoch 4625/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8693 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 4626/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8699 - loss: 0.3220 - mse: 0.0973\n",
      "Epoch 4627/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0972\n",
      "Epoch 4628/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3219 - mse: 0.0972\n",
      "Epoch 4629/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.8704 - loss: 0.3212 - mse: 0.0971\n",
      "Epoch 4630/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0968\n",
      "Epoch 4631/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4632/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4633/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8704 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4634/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4635/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8693 - loss: 0.3194 - mse: 0.0964\n",
      "Epoch 4636/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4637/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4638/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 0.8710 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4639/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8710 - loss: 0.3198 - mse: 0.0966\n",
      "Epoch 4640/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4641/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8710 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4642/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.8710 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4643/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8710 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4644/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4645/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4646/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4647/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459ms/step - accuracy: 0.8687 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4648/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4649/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4650/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8693 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4651/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4652/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4653/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4654/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8687 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4655/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8716 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4656/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4657/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4658/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4659/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4660/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8710 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4661/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4662/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3202 - mse: 0.0967\n",
      "Epoch 4663/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8721 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4664/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4665/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8716 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4666/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8693 - loss: 0.3212 - mse: 0.0970\n",
      "Epoch 4667/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0970\n",
      "Epoch 4668/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0971\n",
      "Epoch 4669/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0970\n",
      "Epoch 4670/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8699 - loss: 0.3213 - mse: 0.0970\n",
      "Epoch 4671/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8710 - loss: 0.3207 - mse: 0.0969\n",
      "Epoch 4672/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4673/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8710 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4674/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8710 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4675/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4676/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4677/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8716 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4678/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8693 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4679/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8693 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4680/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4681/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4682/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.8693 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4683/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8693 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4684/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4685/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.8693 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4686/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8710 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4687/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8693 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4688/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4689/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4690/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4691/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4692/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4693/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4694/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4695/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4696/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4697/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4698/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4699/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4700/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4701/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4702/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4703/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8687 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4704/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4705/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8693 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4706/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8716 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4707/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8687 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4708/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.8716 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4709/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4710/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4711/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8710 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4712/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8716 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4713/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4714/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8716 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4715/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4716/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8710 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4717/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 4718/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8704 - loss: 0.3216 - mse: 0.0972\n",
      "Epoch 4719/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8704 - loss: 0.3227 - mse: 0.0976\n",
      "Epoch 4720/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8721 - loss: 0.3226 - mse: 0.0975\n",
      "Epoch 4721/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8704 - loss: 0.3236 - mse: 0.0977\n",
      "Epoch 4722/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8699 - loss: 0.3231 - mse: 0.0977\n",
      "Epoch 4723/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.8710 - loss: 0.3234 - mse: 0.0976\n",
      "Epoch 4724/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8716 - loss: 0.3220 - mse: 0.0974\n",
      "Epoch 4725/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0971\n",
      "Epoch 4726/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8716 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4727/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4728/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4729/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8693 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4730/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4731/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4732/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4733/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8716 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4734/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0967\n",
      "Epoch 4735/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8716 - loss: 0.3206 - mse: 0.0968\n",
      "Epoch 4736/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3207 - mse: 0.0970\n",
      "Epoch 4737/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - accuracy: 0.8716 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4738/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 0.8699 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4739/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8721 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4740/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8710 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4741/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8716 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4742/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4743/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8704 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4744/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4745/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4746/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4747/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4748/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8716 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4749/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8687 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4750/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8716 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4751/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4752/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4753/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4754/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4755/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8710 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4756/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4757/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4758/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4759/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8710 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4760/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8704 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4761/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8710 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4762/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4763/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4764/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8716 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4765/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4766/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.8710 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4767/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4768/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8710 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4769/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4770/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4771/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4772/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4773/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4774/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4775/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4776/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4777/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4778/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4779/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8687 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4780/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8710 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4781/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8704 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4782/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4783/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4784/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8721 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4785/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8699 - loss: 0.3205 - mse: 0.0968\n",
      "Epoch 4786/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 4787/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8710 - loss: 0.3223 - mse: 0.0974\n",
      "Epoch 4788/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8710 - loss: 0.3228 - mse: 0.0976\n",
      "Epoch 4789/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3249 - mse: 0.0981\n",
      "Epoch 4790/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8704 - loss: 0.3251 - mse: 0.0983\n",
      "Epoch 4791/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8687 - loss: 0.3279 - mse: 0.0990\n",
      "Epoch 4792/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3268 - mse: 0.0988\n",
      "Epoch 4793/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8682 - loss: 0.3282 - mse: 0.0991\n",
      "Epoch 4794/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8704 - loss: 0.3246 - mse: 0.0981\n",
      "Epoch 4795/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.8710 - loss: 0.3228 - mse: 0.0974\n",
      "Epoch 4796/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8721 - loss: 0.3201 - mse: 0.0968\n",
      "Epoch 4797/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4798/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4799/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8716 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4800/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8693 - loss: 0.3216 - mse: 0.0971\n",
      "Epoch 4801/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8716 - loss: 0.3224 - mse: 0.0974\n",
      "Epoch 4802/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8699 - loss: 0.3231 - mse: 0.0976\n",
      "Epoch 4803/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8710 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 4804/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8693 - loss: 0.3221 - mse: 0.0973\n",
      "Epoch 4805/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8704 - loss: 0.3206 - mse: 0.0969\n",
      "Epoch 4806/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3196 - mse: 0.0965\n",
      "Epoch 4807/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4808/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8710 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4809/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0966\n",
      "Epoch 4810/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8716 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4811/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0968\n",
      "Epoch 4812/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8716 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4813/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8704 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4814/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4815/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4816/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4817/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8699 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4818/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8693 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4819/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8710 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4820/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4821/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4822/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4823/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4824/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8710 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4825/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8704 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4826/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4827/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8699 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4828/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4829/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4830/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4831/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4832/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8699 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4833/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8693 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4834/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8699 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4835/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8687 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4836/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4837/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4838/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4839/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4840/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4841/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8693 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4842/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4843/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8693 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4844/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8699 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4845/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4846/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4847/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4848/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8693 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4849/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8699 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4850/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4851/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.8699 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4852/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4853/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0963\n",
      "Epoch 4854/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4855/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8704 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4856/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4857/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4858/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8710 - loss: 0.3193 - mse: 0.0964\n",
      "Epoch 4859/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0964\n",
      "Epoch 4860/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3194 - mse: 0.0965\n",
      "Epoch 4861/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0964\n",
      "Epoch 4862/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4863/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8699 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4864/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8710 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4865/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4866/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8710 - loss: 0.3193 - mse: 0.0964\n",
      "Epoch 4867/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4868/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8710 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4869/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8710 - loss: 0.3189 - mse: 0.0963\n",
      "Epoch 4870/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8699 - loss: 0.3189 - mse: 0.0963\n",
      "Epoch 4871/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8704 - loss: 0.3189 - mse: 0.0963\n",
      "Epoch 4872/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8693 - loss: 0.3189 - mse: 0.0963\n",
      "Epoch 4873/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4874/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8693 - loss: 0.3188 - mse: 0.0963\n",
      "Epoch 4875/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8693 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4876/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4877/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8704 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4878/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8716 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4879/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8699 - loss: 0.3192 - mse: 0.0964\n",
      "Epoch 4880/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4881/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4882/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8704 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4883/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4884/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4885/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4886/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4887/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8716 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4888/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4889/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4890/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8716 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4891/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0965\n",
      "Epoch 4892/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8721 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4893/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8704 - loss: 0.3202 - mse: 0.0968\n",
      "Epoch 4894/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.8716 - loss: 0.3205 - mse: 0.0969\n",
      "Epoch 4895/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8693 - loss: 0.3213 - mse: 0.0971\n",
      "Epoch 4896/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8716 - loss: 0.3215 - mse: 0.0972\n",
      "Epoch 4897/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8704 - loss: 0.3225 - mse: 0.0973\n",
      "Epoch 4898/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8721 - loss: 0.3224 - mse: 0.0975\n",
      "Epoch 4899/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8687 - loss: 0.3236 - mse: 0.0978\n",
      "Epoch 4900/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8710 - loss: 0.3235 - mse: 0.0977\n",
      "Epoch 4901/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8693 - loss: 0.3246 - mse: 0.0981\n",
      "Epoch 4902/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8710 - loss: 0.3234 - mse: 0.0977\n",
      "Epoch 4903/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8710 - loss: 0.3234 - mse: 0.0976\n",
      "Epoch 4904/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8727 - loss: 0.3217 - mse: 0.0973\n",
      "Epoch 4905/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8699 - loss: 0.3206 - mse: 0.0968\n",
      "Epoch 4906/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8721 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4907/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8699 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4908/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.8699 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4909/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8704 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4910/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8710 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4911/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8727 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4912/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3200 - mse: 0.0967\n",
      "Epoch 4913/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8710 - loss: 0.3204 - mse: 0.0969\n",
      "Epoch 4914/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8699 - loss: 0.3209 - mse: 0.0969\n",
      "Epoch 4915/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8699 - loss: 0.3208 - mse: 0.0970\n",
      "Epoch 4916/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8693 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 4917/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8716 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4918/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4919/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8721 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4920/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8699 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4921/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8699 - loss: 0.3184 - mse: 0.0963\n",
      "Epoch 4922/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8710 - loss: 0.3184 - mse: 0.0963\n",
      "Epoch 4923/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4924/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8710 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4925/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8716 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4926/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8710 - loss: 0.3190 - mse: 0.0965\n",
      "Epoch 4927/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4928/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8727 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4929/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8699 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4930/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8721 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4931/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4932/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8716 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4933/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8699 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4934/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8716 - loss: 0.3197 - mse: 0.0967\n",
      "Epoch 4935/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4936/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8716 - loss: 0.3196 - mse: 0.0966\n",
      "Epoch 4937/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.8699 - loss: 0.3197 - mse: 0.0966\n",
      "Epoch 4938/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8721 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4939/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8699 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4940/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8721 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4941/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8704 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4942/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8710 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4943/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0964\n",
      "Epoch 4944/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8710 - loss: 0.3190 - mse: 0.0965\n",
      "Epoch 4945/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4946/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.8699 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4947/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8710 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4948/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8704 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4949/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8704 - loss: 0.3189 - mse: 0.0964\n",
      "Epoch 4950/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8699 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4951/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8710 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4952/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8704 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4953/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8704 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4954/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8704 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4955/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8710 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4956/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8710 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4957/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8710 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4958/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8710 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4959/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8704 - loss: 0.3187 - mse: 0.0963\n",
      "Epoch 4960/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8710 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4961/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8704 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4962/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8704 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4963/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3188 - mse: 0.0964\n",
      "Epoch 4964/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8704 - loss: 0.3190 - mse: 0.0964\n",
      "Epoch 4965/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8704 - loss: 0.3192 - mse: 0.0965\n",
      "Epoch 4966/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.8721 - loss: 0.3193 - mse: 0.0965\n",
      "Epoch 4967/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8699 - loss: 0.3195 - mse: 0.0966\n",
      "Epoch 4968/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8716 - loss: 0.3198 - mse: 0.0967\n",
      "Epoch 4969/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8699 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4970/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8721 - loss: 0.3204 - mse: 0.0968\n",
      "Epoch 4971/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8693 - loss: 0.3209 - mse: 0.0970\n",
      "Epoch 4972/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 4973/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8704 - loss: 0.3218 - mse: 0.0972\n",
      "Epoch 4974/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8716 - loss: 0.3214 - mse: 0.0972\n",
      "Epoch 4975/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8699 - loss: 0.3217 - mse: 0.0972\n",
      "Epoch 4976/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8704 - loss: 0.3211 - mse: 0.0971\n",
      "Epoch 4977/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8704 - loss: 0.3210 - mse: 0.0970\n",
      "Epoch 4978/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8716 - loss: 0.3203 - mse: 0.0968\n",
      "Epoch 4979/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8704 - loss: 0.3199 - mse: 0.0967\n",
      "Epoch 4980/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8721 - loss: 0.3194 - mse: 0.0966\n",
      "Epoch 4981/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8704 - loss: 0.3191 - mse: 0.0965\n",
      "Epoch 4982/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.8704 - loss: 0.3187 - mse: 0.0964\n",
      "Epoch 4983/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8704 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4984/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8704 - loss: 0.3184 - mse: 0.0963\n",
      "Epoch 4985/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8693 - loss: 0.3184 - mse: 0.0963\n",
      "Epoch 4986/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8699 - loss: 0.3183 - mse: 0.0962\n",
      "Epoch 4987/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8693 - loss: 0.3183 - mse: 0.0962\n",
      "Epoch 4988/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.8693 - loss: 0.3185 - mse: 0.0962\n",
      "Epoch 4989/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8704 - loss: 0.3183 - mse: 0.0962\n",
      "Epoch 4990/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8693 - loss: 0.3184 - mse: 0.0962\n",
      "Epoch 4991/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8704 - loss: 0.3185 - mse: 0.0963\n",
      "Epoch 4992/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0963\n",
      "Epoch 4993/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8704 - loss: 0.3186 - mse: 0.0962\n",
      "Epoch 4994/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8693 - loss: 0.3186 - mse: 0.0962\n",
      "Epoch 4995/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8699 - loss: 0.3185 - mse: 0.0962\n",
      "Epoch 4996/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8693 - loss: 0.3185 - mse: 0.0962\n",
      "Epoch 4997/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8699 - loss: 0.3184 - mse: 0.0962\n",
      "Epoch 4998/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8693 - loss: 0.3183 - mse: 0.0962\n",
      "Epoch 4999/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8699 - loss: 0.3183 - mse: 0.0962\n",
      "Epoch 5000/5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8693 - loss: 0.3183 - mse: 0.0962\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=5000,\n",
    "    batch_size=10000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.87120068e-01],\n",
       "       [8.35524797e-01],\n",
       "       [6.56111121e-01],\n",
       "       [7.78556049e-01],\n",
       "       [9.35129821e-01],\n",
       "       [9.74528432e-01],\n",
       "       [9.67536747e-01],\n",
       "       [9.34848607e-01],\n",
       "       [6.72456741e-01],\n",
       "       [9.48001146e-01],\n",
       "       [9.76721466e-01],\n",
       "       [9.13934231e-01],\n",
       "       [7.39187837e-01],\n",
       "       [1.48508132e-01],\n",
       "       [2.87606090e-01],\n",
       "       [7.18085885e-01],\n",
       "       [9.74399626e-01],\n",
       "       [7.59977102e-01],\n",
       "       [6.85819626e-01],\n",
       "       [9.50436741e-02],\n",
       "       [3.92251819e-01],\n",
       "       [9.75918233e-01],\n",
       "       [2.10635304e-01],\n",
       "       [8.94816577e-01],\n",
       "       [3.44967246e-01],\n",
       "       [9.72844660e-01],\n",
       "       [8.65124524e-01],\n",
       "       [8.80235314e-01],\n",
       "       [8.14961672e-01],\n",
       "       [9.72065806e-01],\n",
       "       [9.96801257e-01],\n",
       "       [9.32195723e-01],\n",
       "       [8.97046804e-01],\n",
       "       [1.61761969e-01],\n",
       "       [2.05690935e-01],\n",
       "       [9.82744217e-01],\n",
       "       [9.52085257e-01],\n",
       "       [9.72363472e-01],\n",
       "       [6.41205668e-01],\n",
       "       [3.87352049e-01],\n",
       "       [8.14014435e-01],\n",
       "       [8.19232762e-01],\n",
       "       [9.62504387e-01],\n",
       "       [9.65429485e-01],\n",
       "       [9.61541951e-01],\n",
       "       [9.50120330e-01],\n",
       "       [9.04317796e-01],\n",
       "       [9.80471075e-01],\n",
       "       [2.94032425e-01],\n",
       "       [9.30992424e-01],\n",
       "       [9.73648548e-01],\n",
       "       [2.51826316e-01],\n",
       "       [9.25082803e-01],\n",
       "       [9.68244135e-01],\n",
       "       [7.59334385e-01],\n",
       "       [7.67302155e-01],\n",
       "       [9.66223776e-01],\n",
       "       [9.68178213e-01],\n",
       "       [9.58062887e-01],\n",
       "       [8.80532801e-01],\n",
       "       [3.72942001e-01],\n",
       "       [9.60157514e-01],\n",
       "       [9.17039275e-01],\n",
       "       [9.85501111e-01],\n",
       "       [9.21735048e-01],\n",
       "       [8.73229563e-01],\n",
       "       [8.68581057e-01],\n",
       "       [9.46317375e-01],\n",
       "       [9.51716721e-01],\n",
       "       [7.91995764e-01],\n",
       "       [9.12401080e-01],\n",
       "       [9.59068477e-01],\n",
       "       [9.02157128e-01],\n",
       "       [9.54356313e-01],\n",
       "       [2.69087464e-01],\n",
       "       [9.22253132e-01],\n",
       "       [3.66402626e-01],\n",
       "       [9.73340750e-01],\n",
       "       [8.98273230e-01],\n",
       "       [2.55759537e-01],\n",
       "       [8.47072124e-01],\n",
       "       [8.96005452e-01],\n",
       "       [3.07905346e-01],\n",
       "       [9.43632126e-01],\n",
       "       [9.43586409e-01],\n",
       "       [9.66770411e-01],\n",
       "       [9.39442396e-01],\n",
       "       [5.39139867e-01],\n",
       "       [9.71781790e-01],\n",
       "       [6.87102199e-01],\n",
       "       [9.23194885e-01],\n",
       "       [8.82355392e-01],\n",
       "       [2.66056746e-01],\n",
       "       [3.80298756e-02],\n",
       "       [8.70303392e-01],\n",
       "       [9.05866444e-01],\n",
       "       [9.99054193e-01],\n",
       "       [9.52274084e-01],\n",
       "       [2.02922046e-01],\n",
       "       [9.23844635e-01],\n",
       "       [9.15085554e-01],\n",
       "       [2.98072964e-01],\n",
       "       [4.21722531e-01],\n",
       "       [9.46061969e-01],\n",
       "       [2.93611825e-01],\n",
       "       [7.85575151e-01],\n",
       "       [9.25952792e-01],\n",
       "       [9.65173721e-01],\n",
       "       [9.17512596e-01],\n",
       "       [9.69162166e-01],\n",
       "       [3.08977067e-02],\n",
       "       [9.68812108e-01],\n",
       "       [9.98329103e-01],\n",
       "       [6.87662244e-01],\n",
       "       [8.60772252e-01],\n",
       "       [9.55863595e-01],\n",
       "       [8.76493037e-01],\n",
       "       [9.42137837e-01],\n",
       "       [9.28401649e-01],\n",
       "       [9.99984920e-01],\n",
       "       [9.34208214e-01],\n",
       "       [9.72415149e-01],\n",
       "       [9.74700391e-01],\n",
       "       [8.61799538e-01],\n",
       "       [7.78718233e-01],\n",
       "       [9.80437160e-01],\n",
       "       [2.92811781e-01],\n",
       "       [9.44204390e-01],\n",
       "       [9.74987447e-01],\n",
       "       [9.96909440e-01],\n",
       "       [5.76462269e-01],\n",
       "       [8.26293468e-01],\n",
       "       [9.56747174e-01],\n",
       "       [9.43781316e-01],\n",
       "       [9.58651483e-01],\n",
       "       [9.48549986e-01],\n",
       "       [9.50703800e-01],\n",
       "       [3.00190926e-01],\n",
       "       [9.01199102e-01],\n",
       "       [9.68908370e-01],\n",
       "       [1.20001666e-01],\n",
       "       [3.63177359e-01],\n",
       "       [9.96915638e-01],\n",
       "       [9.59420741e-01],\n",
       "       [2.09119648e-01],\n",
       "       [9.55257952e-01],\n",
       "       [9.58384752e-01],\n",
       "       [9.53787804e-01],\n",
       "       [9.40695524e-01],\n",
       "       [9.43555951e-01],\n",
       "       [7.91655421e-01],\n",
       "       [9.99934375e-01],\n",
       "       [8.95938993e-01],\n",
       "       [3.38170499e-01],\n",
       "       [7.99640656e-01],\n",
       "       [6.34605408e-01],\n",
       "       [9.66894090e-01],\n",
       "       [9.98751223e-01],\n",
       "       [9.54425931e-01],\n",
       "       [3.48132670e-01],\n",
       "       [9.49750662e-01],\n",
       "       [9.57694650e-01],\n",
       "       [3.70501131e-01],\n",
       "       [6.70622289e-01],\n",
       "       [9.71409440e-01],\n",
       "       [9.99996722e-01],\n",
       "       [9.51682568e-01],\n",
       "       [9.31088686e-01],\n",
       "       [4.74756271e-01],\n",
       "       [7.83355713e-01],\n",
       "       [9.38812196e-01],\n",
       "       [7.92718232e-01],\n",
       "       [9.99907553e-01],\n",
       "       [4.25875127e-01],\n",
       "       [1.26598939e-01],\n",
       "       [8.60403001e-01],\n",
       "       [9.68963921e-01],\n",
       "       [9.91077900e-01],\n",
       "       [7.33092427e-01],\n",
       "       [9.78927791e-01],\n",
       "       [9.49039936e-01],\n",
       "       [9.75380659e-01],\n",
       "       [9.67702508e-01],\n",
       "       [9.75302875e-01],\n",
       "       [9.49878454e-01],\n",
       "       [7.18069375e-01],\n",
       "       [1.43784121e-01],\n",
       "       [7.66756892e-01],\n",
       "       [7.65440226e-01],\n",
       "       [9.06599820e-01],\n",
       "       [2.01506019e-01],\n",
       "       [9.66814935e-01],\n",
       "       [9.19234514e-01],\n",
       "       [9.37653422e-01],\n",
       "       [7.62025774e-01],\n",
       "       [1.09338664e-01],\n",
       "       [3.18673556e-03],\n",
       "       [3.00244004e-01],\n",
       "       [9.92039919e-01],\n",
       "       [3.71475667e-01],\n",
       "       [7.91695595e-01],\n",
       "       [3.72194290e-01],\n",
       "       [9.58051562e-01],\n",
       "       [7.61068463e-01],\n",
       "       [3.69865149e-01],\n",
       "       [2.87787080e-01],\n",
       "       [7.78246701e-01],\n",
       "       [9.69232976e-01],\n",
       "       [9.69723821e-01],\n",
       "       [9.04066503e-01],\n",
       "       [9.27113891e-01],\n",
       "       [9.61800933e-01],\n",
       "       [5.44554472e-01],\n",
       "       [9.99999821e-01],\n",
       "       [9.93442476e-01],\n",
       "       [7.07936764e-01],\n",
       "       [9.68970299e-01],\n",
       "       [9.56787288e-01],\n",
       "       [9.76357281e-01],\n",
       "       [9.71510410e-01],\n",
       "       [6.92570746e-01],\n",
       "       [9.78045881e-01],\n",
       "       [9.78810310e-01],\n",
       "       [9.70407307e-01],\n",
       "       [1.05832396e-02],\n",
       "       [9.37526405e-01],\n",
       "       [9.58298206e-01],\n",
       "       [7.00350761e-01],\n",
       "       [9.40247416e-01],\n",
       "       [1.03664897e-01],\n",
       "       [9.65048909e-01],\n",
       "       [7.52466857e-01],\n",
       "       [1.76698372e-01],\n",
       "       [9.91146982e-01],\n",
       "       [2.07313597e-01],\n",
       "       [2.19718297e-03],\n",
       "       [3.58138829e-01],\n",
       "       [9.63166535e-01],\n",
       "       [9.01468396e-01],\n",
       "       [9.42288399e-01],\n",
       "       [9.79409516e-01],\n",
       "       [8.44215989e-01],\n",
       "       [9.80067670e-01],\n",
       "       [2.28854045e-01],\n",
       "       [7.35729218e-01],\n",
       "       [1.27468333e-01],\n",
       "       [7.63735831e-01],\n",
       "       [2.50055820e-01],\n",
       "       [3.06840926e-01],\n",
       "       [9.58462179e-01],\n",
       "       [9.52969313e-01],\n",
       "       [8.82493377e-01],\n",
       "       [9.59974289e-01],\n",
       "       [9.20154333e-01],\n",
       "       [6.36522532e-01],\n",
       "       [3.82302791e-01],\n",
       "       [9.54163074e-01],\n",
       "       [8.73470724e-01],\n",
       "       [9.50619876e-01],\n",
       "       [9.36793685e-01],\n",
       "       [4.47886974e-01],\n",
       "       [9.47036862e-01],\n",
       "       [9.08090711e-01],\n",
       "       [9.48444977e-02],\n",
       "       [2.74316579e-01],\n",
       "       [9.55560148e-01],\n",
       "       [9.16708708e-01],\n",
       "       [7.90739655e-02],\n",
       "       [9.43800092e-01],\n",
       "       [9.30307686e-01],\n",
       "       [9.45485473e-01],\n",
       "       [9.47166562e-01],\n",
       "       [6.95340812e-01],\n",
       "       [4.00421113e-01],\n",
       "       [5.76687932e-01],\n",
       "       [9.07316089e-01],\n",
       "       [1.23106472e-01],\n",
       "       [9.08714950e-01],\n",
       "       [8.36376548e-01],\n",
       "       [9.56662238e-01],\n",
       "       [9.65547562e-01],\n",
       "       [8.98510814e-01],\n",
       "       [9.64367688e-01],\n",
       "       [9.66446042e-01],\n",
       "       [9.54567134e-01],\n",
       "       [9.64847744e-01],\n",
       "       [9.63213861e-01],\n",
       "       [9.61325645e-01],\n",
       "       [2.27603599e-01],\n",
       "       [9.53667045e-01],\n",
       "       [7.22489953e-01],\n",
       "       [9.57702875e-01],\n",
       "       [9.97400105e-01],\n",
       "       [8.76227260e-01],\n",
       "       [9.77712333e-01],\n",
       "       [7.55201340e-01],\n",
       "       [9.14702535e-01],\n",
       "       [9.35426712e-01],\n",
       "       [9.60412741e-01],\n",
       "       [9.25424933e-01],\n",
       "       [2.50792474e-01],\n",
       "       [7.54212439e-01],\n",
       "       [9.70393956e-01],\n",
       "       [8.61947954e-01],\n",
       "       [6.96483016e-01],\n",
       "       [9.99306679e-01],\n",
       "       [9.39145088e-01],\n",
       "       [9.50525820e-01],\n",
       "       [9.77645397e-01],\n",
       "       [2.82936573e-01],\n",
       "       [8.00577879e-01],\n",
       "       [9.47585225e-01],\n",
       "       [9.48604167e-01],\n",
       "       [8.72928739e-01],\n",
       "       [9.16700840e-01],\n",
       "       [7.19549477e-01],\n",
       "       [2.63547152e-01],\n",
       "       [9.39712703e-01],\n",
       "       [9.72943366e-01],\n",
       "       [9.48239446e-01],\n",
       "       [1.32737666e-01],\n",
       "       [9.55699921e-01],\n",
       "       [8.26395035e-01],\n",
       "       [8.38748872e-01],\n",
       "       [5.60182333e-01],\n",
       "       [2.10616753e-01],\n",
       "       [9.52623188e-01],\n",
       "       [4.05419350e-01],\n",
       "       [9.30724204e-01],\n",
       "       [1.59500420e-01],\n",
       "       [8.86503398e-01],\n",
       "       [8.19934249e-01],\n",
       "       [6.80404544e-01],\n",
       "       [9.37187135e-01],\n",
       "       [9.55778539e-01],\n",
       "       [9.71182704e-01],\n",
       "       [9.53423381e-01],\n",
       "       [8.81156087e-01],\n",
       "       [9.36766386e-01],\n",
       "       [2.21720085e-01],\n",
       "       [8.83476794e-01],\n",
       "       [9.59058225e-01],\n",
       "       [9.72289264e-01],\n",
       "       [9.99996781e-01],\n",
       "       [9.81634021e-01],\n",
       "       [2.47717604e-01],\n",
       "       [9.71882164e-01],\n",
       "       [2.49527723e-01],\n",
       "       [9.43981227e-04],\n",
       "       [8.92364681e-01],\n",
       "       [9.61302757e-01],\n",
       "       [9.53960896e-01],\n",
       "       [9.60294366e-01],\n",
       "       [9.76561308e-01],\n",
       "       [9.66458023e-01],\n",
       "       [2.29824498e-01],\n",
       "       [4.41608548e-01],\n",
       "       [7.16659069e-01],\n",
       "       [9.75829184e-01],\n",
       "       [3.20080429e-01],\n",
       "       [9.36259627e-01],\n",
       "       [2.12879539e-01],\n",
       "       [9.65637028e-01],\n",
       "       [9.24617767e-01],\n",
       "       [4.07304078e-01],\n",
       "       [9.04720783e-01],\n",
       "       [9.70147073e-01],\n",
       "       [9.47289646e-01],\n",
       "       [8.50165844e-01],\n",
       "       [9.28204775e-01],\n",
       "       [9.45381999e-01],\n",
       "       [9.67231214e-01],\n",
       "       [3.09161782e-01],\n",
       "       [9.67187285e-01],\n",
       "       [9.39799428e-01],\n",
       "       [9.25773025e-01],\n",
       "       [8.23214412e-01],\n",
       "       [2.66100198e-01],\n",
       "       [4.17270452e-01],\n",
       "       [9.58411932e-01],\n",
       "       [3.32673937e-01],\n",
       "       [3.26836050e-01],\n",
       "       [2.88220048e-01],\n",
       "       [9.56963301e-01],\n",
       "       [9.71373439e-01],\n",
       "       [9.16143358e-01],\n",
       "       [7.05121517e-01],\n",
       "       [7.16804639e-02],\n",
       "       [9.70818937e-01],\n",
       "       [9.99603510e-01],\n",
       "       [9.12037134e-01],\n",
       "       [9.67326880e-01],\n",
       "       [4.73994762e-01],\n",
       "       [9.67858970e-01],\n",
       "       [9.62812841e-01],\n",
       "       [4.11874980e-01],\n",
       "       [1.02428272e-01],\n",
       "       [9.20777857e-01],\n",
       "       [8.36203873e-01],\n",
       "       [9.99845803e-01],\n",
       "       [9.73318577e-01],\n",
       "       [8.00534606e-01],\n",
       "       [8.52322578e-01],\n",
       "       [9.15428102e-01],\n",
       "       [9.47068751e-01],\n",
       "       [9.57092121e-02],\n",
       "       [9.66419935e-01],\n",
       "       [9.78575647e-01],\n",
       "       [2.31303692e-01],\n",
       "       [9.63405192e-01],\n",
       "       [9.66203690e-01],\n",
       "       [9.52052116e-01],\n",
       "       [2.04266176e-01],\n",
       "       [9.87615168e-01],\n",
       "       [2.82669157e-01],\n",
       "       [9.76594687e-01],\n",
       "       [3.47612292e-01],\n",
       "       [9.75680709e-01],\n",
       "       [9.49050426e-01],\n",
       "       [9.05882180e-01],\n",
       "       [7.11738393e-02],\n",
       "       [9.79569852e-01],\n",
       "       [8.93575372e-04],\n",
       "       [9.56920326e-01],\n",
       "       [1.16968885e-01],\n",
       "       [2.14900851e-01],\n",
       "       [6.27365559e-02],\n",
       "       [9.31798697e-01],\n",
       "       [9.14168358e-01],\n",
       "       [8.71149480e-01],\n",
       "       [9.67076123e-01],\n",
       "       [8.98408532e-01],\n",
       "       [9.87326980e-01],\n",
       "       [3.31907123e-01],\n",
       "       [9.76690412e-01],\n",
       "       [9.46487427e-01],\n",
       "       [8.68016601e-01],\n",
       "       [9.70416963e-01]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make the predictions using ANN\n",
    "ann_predictions=model.predict(X_test)\n",
    "ann_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Rainfall is happend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#1021.8|   21.3|       18.4|   15.2|     9.6|    52.0| 45.0|     3.6|         40.0|     24.8\n",
    "sample_data=[1017.4,21.2,20.6,19.9,19.4,87.0,88.0,1.1,60.0,17.2]\n",
    "sample_data_array=np.array(sample_data).reshape(1, -1)  \n",
    "ann_model_predictions=model.predict(sample_data_array)\n",
    "if ann_model_predictions[0]<=0.5:\n",
    "    print(\"Rainfall is not happend\")\n",
    "else:\n",
    "    print(\"Rainfall is happend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the model using new fresh test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+\n",
      "|  id|day|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|\n",
      "+----+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+\n",
      "|2190|  1|  1019.5|   17.5|       15.8|   12.7|    14.9|    96.0| 99.0|     0.0|         50.0|     24.3|\n",
      "|2191|  2|  1016.5|   17.5|       16.5|   15.8|    15.1|    97.0| 99.0|     0.0|         50.0|     35.3|\n",
      "|2192|  3|  1023.9|   11.2|       10.4|    9.4|     8.9|    86.0| 96.0|     0.0|         40.0|     16.9|\n",
      "|2193|  4|  1022.9|   20.6|       17.3|   15.2|     9.5|    75.0| 45.0|     7.1|         20.0|     50.6|\n",
      "|2194|  5|  1022.2|   16.1|       13.8|    6.4|     4.3|    68.0| 49.0|     9.2|         20.0|     19.4|\n",
      "|2195|  6|  1027.1|   15.6|       12.6|   11.5|     9.0|    76.0| 94.0|     0.0|         20.0|     41.4|\n",
      "|2196|  7|  1022.6|   15.5|       13.7|   10.7|    11.8|    79.0| 95.0|     0.0|         20.0|     43.1|\n",
      "|2197|  8|  1013.5|   20.5|       16.2|   15.2|    13.1|    94.0| 93.0|     0.2|         70.0|     41.3|\n",
      "|2198|  9|  1021.3|   16.3|       13.2|   11.3|    10.8|    85.0| 99.0|     0.1|         20.0|     34.0|\n",
      "|2199| 10|  1026.1|   10.4|        8.5|    7.0|     3.1|    69.0| 88.0|     0.0|         20.0|     26.4|\n",
      "|2200| 11|  1017.5|   17.6|       16.6|   15.5|    15.8|    95.0|100.0|     0.0|         50.0|     29.0|\n",
      "|2201| 12|  1021.5|   16.5|       13.3|   11.3|     9.8|    70.0| 49.0|     9.7|         20.0|     33.4|\n",
      "|2202| 13|  1019.0|   17.6|       15.3|   14.3|    12.9|    95.0|100.0|     0.0|         40.0|     41.3|\n",
      "|2203| 14|  1012.5|   16.2|       15.2|   14.5|    14.6|    97.0| 86.0|     0.0|         50.0|     24.4|\n",
      "|2204| 15|  1023.4|    7.4|        5.9|    4.2|     3.5|    71.0| 70.0|     5.1|         10.0|     11.0|\n",
      "|2205| 16|  1022.7|   15.3|       12.0|   12.4|     6.8|    59.0| 19.0|     9.9|         40.0|     29.8|\n",
      "|2206| 17|  1025.2|   14.7|       13.0|   11.4|     0.3|    90.0| 91.0|     0.6|         60.0|     30.2|\n",
      "|2207| 18|  1024.1|   14.9|       12.4|   11.1|     3.1|    77.0| 82.0|     0.0|         20.0|     16.7|\n",
      "|2208| 19|  1025.4|   17.6|       15.3|   12.6|     9.8|    70.0| 49.0|    10.2|         30.0|     28.6|\n",
      "|2209| 20|  1024.1|   15.8|       11.8|   10.0|     6.4|    52.0| 19.0|     9.1|         20.0|     32.2|\n",
      "+----+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set=spark.read.csv(\"test.csv\", header=True,inferSchema=True)\n",
    "train_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the Ann architecture model using  test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+\n",
      "|  id|day|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|\n",
      "+----+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+\n",
      "|2190|  1|  1019.5|   17.5|       15.8|   12.7|    14.9|    96.0| 99.0|     0.0|         50.0|     24.3|\n",
      "|2191|  2|  1016.5|   17.5|       16.5|   15.8|    15.1|    97.0| 99.0|     0.0|         50.0|     35.3|\n",
      "|2192|  3|  1023.9|   11.2|       10.4|    9.4|     8.9|    86.0| 96.0|     0.0|         40.0|     16.9|\n",
      "|2193|  4|  1022.9|   20.6|       17.3|   15.2|     9.5|    75.0| 45.0|     7.1|         20.0|     50.6|\n",
      "|2194|  5|  1022.2|   16.1|       13.8|    6.4|     4.3|    68.0| 49.0|     9.2|         20.0|     19.4|\n",
      "|2195|  6|  1027.1|   15.6|       12.6|   11.5|     9.0|    76.0| 94.0|     0.0|         20.0|     41.4|\n",
      "|2196|  7|  1022.6|   15.5|       13.7|   10.7|    11.8|    79.0| 95.0|     0.0|         20.0|     43.1|\n",
      "|2197|  8|  1013.5|   20.5|       16.2|   15.2|    13.1|    94.0| 93.0|     0.2|         70.0|     41.3|\n",
      "|2198|  9|  1021.3|   16.3|       13.2|   11.3|    10.8|    85.0| 99.0|     0.1|         20.0|     34.0|\n",
      "|2199| 10|  1026.1|   10.4|        8.5|    7.0|     3.1|    69.0| 88.0|     0.0|         20.0|     26.4|\n",
      "|2200| 11|  1017.5|   17.6|       16.6|   15.5|    15.8|    95.0|100.0|     0.0|         50.0|     29.0|\n",
      "|2201| 12|  1021.5|   16.5|       13.3|   11.3|     9.8|    70.0| 49.0|     9.7|         20.0|     33.4|\n",
      "|2202| 13|  1019.0|   17.6|       15.3|   14.3|    12.9|    95.0|100.0|     0.0|         40.0|     41.3|\n",
      "|2203| 14|  1012.5|   16.2|       15.2|   14.5|    14.6|    97.0| 86.0|     0.0|         50.0|     24.4|\n",
      "|2204| 15|  1023.4|    7.4|        5.9|    4.2|     3.5|    71.0| 70.0|     5.1|         10.0|     11.0|\n",
      "|2205| 16|  1022.7|   15.3|       12.0|   12.4|     6.8|    59.0| 19.0|     9.9|         40.0|     29.8|\n",
      "|2206| 17|  1025.2|   14.7|       13.0|   11.4|     0.3|    90.0| 91.0|     0.6|         60.0|     30.2|\n",
      "|2207| 18|  1024.1|   14.9|       12.4|   11.1|     3.1|    77.0| 82.0|     0.0|         20.0|     16.7|\n",
      "|2208| 19|  1025.4|   17.6|       15.3|   12.6|     9.8|    70.0| 49.0|    10.2|         30.0|     28.6|\n",
      "|2209| 20|  1024.1|   15.8|       11.8|   10.0|     6.4|    52.0| 19.0|     9.1|         20.0|     32.2|\n",
      "+----+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set=spark.read.csv(\"test.csv\",header=True, inferSchema=True)\n",
    "test_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the data types of each column of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- maxtemp: double (nullable = true)\n",
      " |-- temparature: double (nullable = true)\n",
      " |-- mintemp: double (nullable = true)\n",
      " |-- dewpoint: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- cloud: double (nullable = true)\n",
      " |-- sunshine: double (nullable = true)\n",
      " |-- winddirection: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
