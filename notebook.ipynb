{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Rainfall-predictions-ML-project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x19c74768710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import the  pyspark libraries adn create the pyspark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Rainfall-predictions-ML-project').getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "| id|day|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|rainfall|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "|  0|  1|  1017.4|   21.2|       20.6|   19.9|    19.4|    87.0| 88.0|     1.1|         60.0|     17.2|       1|\n",
      "|  1|  2|  1019.5|   16.2|       16.9|   15.8|    15.4|    95.0| 91.0|     0.0|         50.0|     21.9|       1|\n",
      "|  2|  3|  1024.1|   19.4|       16.1|   14.6|     9.3|    75.0| 47.0|     8.3|         70.0|     18.1|       1|\n",
      "|  3|  4|  1013.4|   18.1|       17.8|   16.9|    16.8|    95.0| 95.0|     0.0|         60.0|     35.6|       1|\n",
      "|  4|  5|  1021.8|   21.3|       18.4|   15.2|     9.6|    52.0| 45.0|     3.6|         40.0|     24.8|       0|\n",
      "|  5|  6|  1022.7|   20.6|       18.6|   16.5|    12.5|    79.0| 81.0|     0.0|         20.0|     15.7|       1|\n",
      "|  6|  7|  1022.8|   19.5|       18.4|   15.3|    11.3|    56.0| 46.0|     7.6|         20.0|     28.4|       0|\n",
      "|  7|  8|  1019.7|   15.8|       13.6|   12.7|    11.8|    96.0|100.0|     0.0|         50.0|     52.8|       1|\n",
      "|  8|  9|  1017.4|   17.6|       16.5|   15.6|    12.5|    86.0|100.0|     0.0|         50.0|     37.5|       1|\n",
      "|  9| 10|  1025.4|   16.5|       14.4|   12.0|     8.6|    77.0| 84.0|     1.0|         50.0|     38.3|       0|\n",
      "| 10| 11|  1016.8|   16.3|       15.3|   14.1|    14.0|    97.0|100.0|     0.0|         40.0|     24.4|       1|\n",
      "| 11| 12|  1012.5|   16.2|       15.2|   14.0|    12.4|    98.0|100.0|     0.0|         50.0|     23.5|       1|\n",
      "| 12| 13|  1020.4|   15.0|       15.5|   13.2|    12.0|    77.0| 86.0|     0.0|         50.0|     32.4|       1|\n",
      "| 13| 14|  1012.5|   13.5|       12.9|   11.6|    11.8|    87.0| 92.0|     0.0|         50.0|     37.0|       1|\n",
      "| 14| 15|  1018.4|   17.8|       16.5|   15.1|    12.1|    77.0| 85.0|     1.2|         40.0|     20.9|       1|\n",
      "| 15| 16|  1024.3|   15.3|       12.9|   10.0|    11.2|    79.0| 91.0|     0.3|         20.0|     20.0|       1|\n",
      "| 16| 17|  1022.5|   16.3|       13.1|   11.4|     2.0|    79.0| 70.0|     6.8|         20.0|     30.3|       1|\n",
      "| 17| 18|  1034.6|   17.5|       16.2|   14.1|    11.8|    68.0| 60.0|     2.5|         50.0|     13.4|       0|\n",
      "| 18| 19|  1024.1|   16.8|       15.8|   14.0|    12.9|    76.0| 77.0|     0.0|         20.0|     20.5|       1|\n",
      "| 19| 20|  1020.2|   16.4|       14.2|   12.7|    11.6|    75.0| 93.0|     0.1|         40.0|     21.9|       1|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read the train set\n",
    "df_train_set=spark.read.csv('train.csv',header=True,inferSchema=True)\n",
    "df_train_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- maxtemp: double (nullable = true)\n",
      " |-- temparature: double (nullable = true)\n",
      " |-- mintemp: double (nullable = true)\n",
      " |-- dewpoint: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- cloud: double (nullable = true)\n",
      " |-- sunshine: double (nullable = true)\n",
      " |-- winddirection: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- rainfall: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check the data types of each column in dataset\n",
    "df_train_set.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'day',\n",
       " 'pressure',\n",
       " 'maxtemp',\n",
       " 'temparature',\n",
       " 'mintemp',\n",
       " 'dewpoint',\n",
       " 'humidity',\n",
       " 'cloud',\n",
       " 'sunshine',\n",
       " 'winddirection',\n",
       " 'windspeed',\n",
       " 'rainfall']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the column names\n",
    "df_train_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "| id|day|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|rainfall|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "|  0|  0|       0|      0|          0|      0|       0|       0|    0|       0|            0|        0|       0|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## chech the null values\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "df_train_set.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_train_set.columns]\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperate the independent features and target lables\n",
    "#independent_features=df_train_set.select(['id','day','pressure','maxtemp','temparature','mintemp','dewpoint','humidity','cloud','sunshine','winddirection','windspeed'])\n",
    "#target_feature=df_train_set.select(['rainfall'])\n",
    "#independent_features.show(),target_feature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+--------------------+\n",
      "| id|day|pressure|maxtemp|temparature|mintemp|dewpoint|humidity|cloud|sunshine|winddirection|windspeed|rainfall|independent_features|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+--------------------+\n",
      "|  0|  1|  1017.4|   21.2|       20.6|   19.9|    19.4|    87.0| 88.0|     1.1|         60.0|     17.2|       1|[1017.4,21.2,20.6...|\n",
      "|  1|  2|  1019.5|   16.2|       16.9|   15.8|    15.4|    95.0| 91.0|     0.0|         50.0|     21.9|       1|[1019.5,16.2,16.9...|\n",
      "|  2|  3|  1024.1|   19.4|       16.1|   14.6|     9.3|    75.0| 47.0|     8.3|         70.0|     18.1|       1|[1024.1,19.4,16.1...|\n",
      "|  3|  4|  1013.4|   18.1|       17.8|   16.9|    16.8|    95.0| 95.0|     0.0|         60.0|     35.6|       1|[1013.4,18.1,17.8...|\n",
      "|  4|  5|  1021.8|   21.3|       18.4|   15.2|     9.6|    52.0| 45.0|     3.6|         40.0|     24.8|       0|[1021.8,21.3,18.4...|\n",
      "|  5|  6|  1022.7|   20.6|       18.6|   16.5|    12.5|    79.0| 81.0|     0.0|         20.0|     15.7|       1|[1022.7,20.6,18.6...|\n",
      "|  6|  7|  1022.8|   19.5|       18.4|   15.3|    11.3|    56.0| 46.0|     7.6|         20.0|     28.4|       0|[1022.8,19.5,18.4...|\n",
      "|  7|  8|  1019.7|   15.8|       13.6|   12.7|    11.8|    96.0|100.0|     0.0|         50.0|     52.8|       1|[1019.7,15.8,13.6...|\n",
      "|  8|  9|  1017.4|   17.6|       16.5|   15.6|    12.5|    86.0|100.0|     0.0|         50.0|     37.5|       1|[1017.4,17.6,16.5...|\n",
      "|  9| 10|  1025.4|   16.5|       14.4|   12.0|     8.6|    77.0| 84.0|     1.0|         50.0|     38.3|       0|[1025.4,16.5,14.4...|\n",
      "| 10| 11|  1016.8|   16.3|       15.3|   14.1|    14.0|    97.0|100.0|     0.0|         40.0|     24.4|       1|[1016.8,16.3,15.3...|\n",
      "| 11| 12|  1012.5|   16.2|       15.2|   14.0|    12.4|    98.0|100.0|     0.0|         50.0|     23.5|       1|[1012.5,16.2,15.2...|\n",
      "| 12| 13|  1020.4|   15.0|       15.5|   13.2|    12.0|    77.0| 86.0|     0.0|         50.0|     32.4|       1|[1020.4,15.0,15.5...|\n",
      "| 13| 14|  1012.5|   13.5|       12.9|   11.6|    11.8|    87.0| 92.0|     0.0|         50.0|     37.0|       1|[1012.5,13.5,12.9...|\n",
      "| 14| 15|  1018.4|   17.8|       16.5|   15.1|    12.1|    77.0| 85.0|     1.2|         40.0|     20.9|       1|[1018.4,17.8,16.5...|\n",
      "| 15| 16|  1024.3|   15.3|       12.9|   10.0|    11.2|    79.0| 91.0|     0.3|         20.0|     20.0|       1|[1024.3,15.3,12.9...|\n",
      "| 16| 17|  1022.5|   16.3|       13.1|   11.4|     2.0|    79.0| 70.0|     6.8|         20.0|     30.3|       1|[1022.5,16.3,13.1...|\n",
      "| 17| 18|  1034.6|   17.5|       16.2|   14.1|    11.8|    68.0| 60.0|     2.5|         50.0|     13.4|       0|[1034.6,17.5,16.2...|\n",
      "| 18| 19|  1024.1|   16.8|       15.8|   14.0|    12.9|    76.0| 77.0|     0.0|         20.0|     20.5|       1|[1024.1,16.8,15.8...|\n",
      "| 19| 20|  1020.2|   16.4|       14.2|   12.7|    11.6|    75.0| 93.0|     0.1|         40.0|     21.9|       1|[1020.2,16.4,14.2...|\n",
      "+---+---+--------+-------+-----------+-------+--------+--------+-----+--------+-------------+---------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_assembler = VectorAssembler(inputCols=['pressure','maxtemp','temparature','mintemp','dewpoint','humidity','cloud','sunshine','winddirection','windspeed'],outputCol='independent_features')\n",
    "output_features =feature_assembler.transform(df_train_set)\n",
    "output_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|independent_features|\n",
      "+--------------------+\n",
      "|[1017.4,21.2,20.6...|\n",
      "|[1019.5,16.2,16.9...|\n",
      "|[1024.1,19.4,16.1...|\n",
      "|[1013.4,18.1,17.8...|\n",
      "|[1021.8,21.3,18.4...|\n",
      "|[1022.7,20.6,18.6...|\n",
      "|[1022.8,19.5,18.4...|\n",
      "|[1019.7,15.8,13.6...|\n",
      "|[1017.4,17.6,16.5...|\n",
      "|[1025.4,16.5,14.4...|\n",
      "|[1016.8,16.3,15.3...|\n",
      "|[1012.5,16.2,15.2...|\n",
      "|[1020.4,15.0,15.5...|\n",
      "|[1012.5,13.5,12.9...|\n",
      "|[1018.4,17.8,16.5...|\n",
      "|[1024.3,15.3,12.9...|\n",
      "|[1022.5,16.3,13.1...|\n",
      "|[1034.6,17.5,16.2...|\n",
      "|[1024.1,16.8,15.8...|\n",
      "|[1020.2,16.4,14.2...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## select the  indepedent features from dataset\n",
    "output_independent_features=output_features.select('independent_features')\n",
    "output_independent_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|independent_features|     scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[1017.4,21.2,20.6...|[179.899948764456...|\n",
      "|[1019.5,16.2,16.9...|[180.271277536233...|\n",
      "|[1024.1,19.4,16.1...|[181.084664369648...|\n",
      "|[1013.4,18.1,17.8...|[179.192655865834...|\n",
      "|[1021.8,21.3,18.4...|[180.677970952940...|\n",
      "|[1022.7,20.6,18.6...|[180.837111855130...|\n",
      "|[1022.8,19.5,18.4...|[180.854794177596...|\n",
      "|[1019.7,15.8,13.6...|[180.306642181164...|\n",
      "|[1017.4,17.6,16.5...|[179.899948764456...|\n",
      "|[1025.4,16.5,14.4...|[181.314534561700...|\n",
      "|[1016.8,16.3,15.3...|[179.793854829663...|\n",
      "|[1012.5,16.2,15.2...|[179.033514963644...|\n",
      "|[1020.4,15.0,15.5...|[180.430418438423...|\n",
      "|[1012.5,13.5,12.9...|[179.033514963644...|\n",
      "|[1018.4,17.8,16.5...|[180.076771989112...|\n",
      "|[1024.3,15.3,12.9...|[181.120029014579...|\n",
      "|[1022.5,16.3,13.1...|[180.801747210199...|\n",
      "|[1034.6,17.5,16.2...|[182.941308228530...|\n",
      "|[1024.1,16.8,15.8...|[181.084664369648...|\n",
      "|[1020.2,16.4,14.2...|[180.395053793491...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## scale the dataset usinf standard scaling\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler=StandardScaler(inputCol='independent_features', outputCol='scaled_features',withStd=True, withMean=False)\n",
    "scaler_model=scaler.fit(output_independent_features)\n",
    "scaled_train_data = scaler_model.transform(output_independent_features)\n",
    "scaled_train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     scaled_features|\n",
      "+--------------------+\n",
      "|[179.899948764456...|\n",
      "|[180.271277536233...|\n",
      "|[181.084664369648...|\n",
      "|[179.192655865834...|\n",
      "|[180.677970952940...|\n",
      "|[180.837111855130...|\n",
      "|[180.854794177596...|\n",
      "|[180.306642181164...|\n",
      "|[179.899948764456...|\n",
      "|[181.314534561700...|\n",
      "|[179.793854829663...|\n",
      "|[179.033514963644...|\n",
      "|[180.430418438423...|\n",
      "|[179.033514963644...|\n",
      "|[180.076771989112...|\n",
      "|[181.120029014579...|\n",
      "|[180.801747210199...|\n",
      "|[182.941308228530...|\n",
      "|[181.084664369648...|\n",
      "|[180.395053793491...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_train_preprocessed_data=scaled_train_data.select('scaled_features')\n",
    "scaled_train_preprocessed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explotary Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
